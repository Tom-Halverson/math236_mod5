[["index.html", "MATH 236: Linear Algebra Preface", " MATH 236: Linear Algebra Preface This is the class handbook for Math 236 Linear Algebra at Macalester College. The content here was made by Andrew Beveridge and Tom Halverson and other faculty in the Department of Mathematics, Statistics and Computer Science at Macalester College. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["problem-set-1-a.html", "Section 1 Problem Set 1-A 1.1 Characterize the Solution Set 1.2 Find the General Solution 1.3 Elementary row operations are reversible 1.4 Designer Parabolas 1.5 Traffic Flow", " Section 1 Problem Set 1-A Due: Tuesday January 26 by noon CST. Week 1 and Week 8 are half weeks, so those two assignments will be split in two and called Problem Sets 1A and 1B. Upload your solutions to problems 1–4 by writing them out by hand, scanning them to pdf using a scanning software such as AdobeScan, assembling them into a single PDF, and uploading it to Moodle. Problem 1.5 is to be done using RStudio. To solve it, create an Rmarkdown file, knit it to .html, and upload the .html on Moodle along with the PDF for questions 1-4. 1.1 Characterize the Solution Set The following augmented matrices are in row echelon form. Decide whether the set of solutions is a point, line, plane, or the empty set in 3-space. Briefly justify your answer. \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 4 \\\\ 0 &amp; 1 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; -1 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 0 &amp; 1 &amp; 0 &amp; 6 \\\\ 0 &amp; 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.2 Find the General Solution Each of the following matrices is the reduced row echelon form of the augmented matrix of a system of linear equations. Give the general solution to each system. \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 3 &amp; 0 &amp; -2 &amp; 5\\\\ 0 &amp; 0 &amp; 1 &amp; 4 &amp; -2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; 4 &amp; 0 &amp; 3 &amp; 6\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -2&amp; -8 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 3 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 4 &amp; 0 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7 &amp; 6\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.3 Elementary row operations are reversible In each case below, an elementary row operation turns the matrix \\(A\\) into the matrix \\(B\\). For each of them, Describe the row operation that turns \\(A\\) into \\(B\\), and Describe the row operation that turns \\(B\\) into \\(A\\). Give your answers in the form: “scale \\(R_2\\) by 3” or “swap \\(R_1\\) and \\(R_4\\)” or “replace \\(R_3\\) with \\(R_3 + \\frac{1}{5} R_1\\).” \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 0 &amp; 7 &amp; 0 &amp; -4 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 4 &amp; 1 &amp; -2 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] 1.4 Designer Parabolas In each part below, set up and solve a linear system of equations to find all possible parabolas of the form \\[ f(x) = a + b x + c x^2 \\] that satisfy the given conditions. For full credit, please solve these by hand, doing all row reductions that bring the system of equations to Reduced Row Echelon Form. On future assignments, you can solve problems like this using either RStudio or WolframAlpha. You are welcome (and, in fact, encouraged) to check your answers using software. \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(2,4)\\). \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(3,10)\\). \\(f(x)\\) passes through the two points: \\((1,3)\\) and \\((3,11)\\). 1.5 Traffic Flow Below you find a section of one-way streets in downtown St Paul, where the arrows indicate traffic direction. The traffic control center has installed electronic sensors that count the numbers of vehicles passing through the 6 streets that lead into and out of this area. Assume that the total flow that enters each intersection equals the the total flow that leaves each intersection (we will ignore parking and staying). Create a system of linear equations to find the possible flow values for the inner streets \\(x_1, x_2, x_3, x_4\\). Using RStudio, enter the augmented matrix of this system, and solve it using the rref command. Type out the general solution to this system of equations. Your answer to part b should be an infinite solution set. Give two distinct solutions that are realistic in terms of traffic flow. Is it possible to close down the street labeled by \\(x_2\\) for road construction? That is, is it possible to have \\(x_2 = 0\\) and to meet the other conditions? "],["problem-set-2.html", "Section 2 Problem Set 2 2.1 Parametric Vector Form 2.2 RREF for a linear system 2.3 RREF for a set of vectors 2.4 Removing free variable columns from a matrix 2.5 A square matrix 2.6 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) 2.7 A Balanced Diet 2.8 Missing Column 2.9 Linear System", " Section 2 Problem Set 2 Due: Tuesday February 2 by 10:00am CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. In problems where you use RStudio for row reduction and are not asked to turn in an R markdown file, you can write something like this: ## Loading required package: pracma You can download the Rmd source file for this problem set. The Problem Set covers sections 1.3, 1.4, 1.5, and 1.7. 2.1 Parametric Vector Form Here is the augmented matrix for a system of linear equations \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\), and its RREF. Give the general solution to this system in parametric vector form. \\[ \\left[ \\begin{array}{ccccc|c} 1 &amp; 1 &amp; -1 &amp; -1 &amp; 2 &amp; 1 \\\\ 1 &amp; 0 &amp; -2 &amp; 1 &amp; 1 &amp; 3 \\\\ -2 &amp; 1 &amp; 5 &amp; 1 &amp; -6 &amp; 2 \\\\ -3 &amp; 0 &amp; 6 &amp; 2 &amp; -8 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; -3 &amp; 6 \\\\ 1 &amp; 0 &amp; -2 &amp; -1 &amp; 3 &amp; -1 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; -2 &amp; 0 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] 2.2 RREF for a linear system Here is the reduced row echelon form of a matrix \\(\\mathsf{A}\\) (you are not given the matrix \\(\\mathsf{A}\\)). \\[ \\mathsf{A} \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 0 &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; -5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] Give the parametric equations of the general solution to the homogenous equation \\(\\mathsf{A} \\mathsf{x} = {\\bf 0}\\). Describe the geometric form of your answer to part (a). For example, you answer should be something like: “it is a plane in \\(\\mathbb{R}^3\\)” or “it is a line in \\(\\mathbb{R}^7\\)” or “it is a point in \\(\\mathbb{R}^4\\).” Suppose that we also know that \\(\\mathsf{A}\\begin{bmatrix} 4 \\\\ 1 \\\\ -3 \\\\ 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 22 \\\\ -13 \\\\ 7 \\\\ \\end{bmatrix}\\). Then give the general solution to \\(\\mathsf{A} \\mathsf{x}= \\begin{bmatrix} 22 \\\\ -13\\\\ 7 \\\\ \\end{bmatrix}\\) in parametric form. 2.3 RREF for a set of vectors Suppose that we have five vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4,\\mathsf{v}_5\\) in \\(\\mathbb{R}^4\\) and that the matrix \\[ A = \\left[ \\begin{array}{ccc} \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\\\ \\mathsf{v}_1 &amp; \\mathsf{v}_2 &amp; \\mathsf{v}_3 &amp;\\mathsf{v}_4 &amp;\\mathsf{v}_5 \\\\ \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\end{array} \\right] \\] has reduced row echelon form \\[ \\begin{bmatrix} 1 &amp; 0 &amp; -3 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 4 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}. \\] Do the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\) span \\(\\mathbb{R}^4\\)? Justify your answer. Is the vector \\(\\mathsf{v}_3\\) in \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2)\\)? Justify your answer. Pick any \\(\\mathsf{b}\\) in \\(\\mathrm{span}(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5)\\). Is there always a unique way to write \\(\\mathsf{b}\\) as a linear combination of \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\)? Justify your answer. 2.4 Removing free variable columns from a matrix Consider the matrix \\[ A =\\left[ \\begin{array}{cccccc} 6 &amp; 5 &amp; -3 &amp; 4 &amp; 2 &amp; -9 \\\\ -7 &amp; -6 &amp; 4 &amp; -5 &amp; -7 &amp; 16 \\\\ -4 &amp; -3 &amp; -1 &amp; 0 &amp; -8 &amp; 9 \\\\ 8 &amp; 7 &amp; -5 &amp; 6 &amp; 1 &amp; -12 \\end{array} \\right]. \\] Use RStudio to show that the columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^4\\). You don’t need to turn in an R file here, just report the reduced row echelon form that you get. Write down the new matrix \\(\\mathsf{A}&#39;\\) gotten by removing the free variable columns from \\(\\mathsf{A}\\). Without using additional calculations on RStudio, explain why the new system \\(\\mathsf{A}&#39; \\mathsf{x} = \\mathsf{b}\\) is consistent and has a unique solution for every choice of \\(\\mathsf{b} \\in \\mathbb{R}^4\\). 2.5 A square matrix Suppose that \\(A\\) is a \\(5\\times 5\\) matrix and \\(\\mathsf{b}\\) is a vector in \\(\\mathbb{R}^5\\) with the property that \\(A\\mathsf{x}=\\mathsf{b}\\) has a unique solution. Explain why the columns of \\(A\\) must span \\(\\mathbb{R}^5\\). Use the reduced row echelon form of \\(A\\) in your explanation. 2.6 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) Suppose that \\(\\mathsf{x}_1\\) and \\(\\mathsf{x}_2\\) are solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) (where \\(\\mathsf{b} \\not= \\mathsf{0}\\)). Decide if any of the following are also solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\). \\(\\mathsf{x}_1+ \\mathsf{x}_2\\) \\(\\mathsf{x}_1 - \\mathsf{x}_2\\) \\(\\frac{1}{2} ( \\mathsf{x}_1 + \\mathsf{x}_2)\\) \\(\\frac{5}{2} \\mathsf{x}_1 - \\frac{3}{2} \\mathsf{x}_2\\). Under what conditions on \\(c\\) and \\(d\\) is \\(\\mathsf{x} = c \\mathsf{x}_1 + d \\mathsf{x}_2\\) a solution to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\)? Justify your answer. Let \\(\\mathsf{u}\\) be the vector that points to \\(1/3\\) of the way from the tip of \\(\\mathsf{v}\\) to the tip of \\(\\mathsf{w}\\) as depicted below. Write \\(\\mathsf{u}\\) as a linear combination of \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) (hint: think about \\(\\mathsf{w} - \\mathsf{v}\\)) If \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) are solutions to \\(A x = \\mathsf{b}\\) then show that \\(\\mathsf{u}\\) is also a solution to \\(A \\mathsf{x} = \\mathsf{b}\\). 2.7 A Balanced Diet An athlete wants to consume a daily diet of 200 grams of carbohydrates, 60 grams of fats and 160 grams of proteins. Here are some of their favorite foods. Table 2.1: Food Carb/Fat/Protein (grams) food carbs fats proteins almonds 3 8 5 avocado 15 31 4 beans 20 1 8 bread 12 1 2 cheese 1 5 3 chicken 0 13 50 egg 1 5 6 milk 12 8 8 zucchini 6 0 2 Answer the following questions, using RStudio for your calculations. Each response must use two or more of the following terms: linear combination, span, linearly dependent, linearly independent. Explain why they cannot achieve their daily goal by eating only almonds, milk and zucchini. Explain why they cannot achieve their daily goal by eating only almonds, beans and cheese. Find a valid one-day diet consisting of almonds, chicken, and zucchini. 2.8 Missing Column The matrices below are supposed to be \\(3 \\times 3\\) but in each case the third column was accdentally deleted. In each case, add a third column, that has no 0s in it and is different from either the first or second column, so that the columns of \\(A\\) are linearly dependent and so that the columns of \\(B\\) are linearly independent. Briefly describe your strategy. \\[ A=\\left[ \\begin{matrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{matrix}\\right] \\qquad\\qquad B=\\left[ \\begin{matrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{matrix}\\right] \\] 2.9 Linear System Use R to solve this problem. Do your computations in an R markdown file. Knit the file to HTML and include it with your homework. Here you can download a template for doing this problem (including the matrix typed out for you!). \\[ A =\\left[ \\begin{array}{cccccc} 12 &amp; 10 &amp; -6 &amp; 8 &amp; 4 &amp; -18 \\\\ -7 &amp; -6 &amp; 4 &amp; -5 &amp; -7 &amp; 16 \\\\ 9 &amp; 9 &amp; -9 &amp; 9 &amp; 9 &amp; -27 \\\\ -4 &amp; -3 &amp; -1 &amp; 0 &amp; -8 &amp; 9 \\\\ 8 &amp; 7 &amp; -5 &amp; 6 &amp; 1 &amp; -12 \\\\ \\end{array} \\right] \\quad b = \\begin{bmatrix} 14 \\\\ -12 \\\\ 9\\\\ -15 \\\\6 \\end{bmatrix} \\] Show that the columns of \\(A\\) are linearly dependent by finding two different dependency relations among them. You can write your answer in a form like \\(5 a1+ 4 a2 + 3 a3 + 2 a4 + a5 = 0\\), where \\(a1, a2,\\) etc are the columns of \\(A\\). Augment \\(A\\) with \\(b\\) and show that \\(A x = b\\) is consistent and has infinitely many solutions. Remove the free-variable columns from \\(A\\) to get a new, smaller matrix \\(A&#39;\\). Show that \\(A&#39; x = b\\) has a unique solution and say what that solution is. "],["problem-set-3.html", "Section 3 Problem Set 3 3.1 Properties of Linear Transformations 3.2 Partial Information about a Linear Transformation 3.3 House Renovations 3.4 Matrix of a Nonlinear Transformation? 3.5 A Proof 3.6 Inner and Outer Products 3.7 Archaeological Seriation 3.8 Rental Car 3.9 Adjacency Matrix", " Section 3 Problem Set 3 Due: Tuesday February 9 by noon CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 1.8, 1.9, 2.1, 2.2. 3.1 Properties of Linear Transformations Here are the row reductions pf 4 matrices into reduced row echelon form. \\[ \\begin{array}{ll} A \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 5 &amp; -3 &amp; 0\\\\ 0 &amp; 1 &amp; -2 &amp; 8 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\qquad &amp; B \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\\\ \\\\ C \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} &amp; D \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{bmatrix} \\end{array} \\] In each case, if \\(T_M\\) is the linear transformation given by the matrix product \\(T_M(x) = M x\\), where \\(M\\) is the given matrix, then \\(T_M: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a transformation from domain \\(\\mathbb{R}^n\\) to codomain (aka target) \\(\\mathbb{R}^m\\). Determine the appropriate values for \\(n\\) and \\(m\\), and decide whether \\(T_M\\) is one-to-one and/or onto. Submit your answers in table form, as shown below. \\[ \\begin{array} {|c|c|c|c|c|} \\hline \\text{transformation} &amp; n &amp; m &amp; \\text{one-to-one?} &amp; \\text{onto?} \\\\ \\hline T_A &amp;\\phantom{\\Big\\vert XX}&amp;\\phantom{\\Big\\vert XX}&amp;&amp; \\\\ \\hline T_B &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline T_C &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline T_D &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline \\end{array} \\hskip5in \\] 3.2 Partial Information about a Linear Transformation We are given that \\(T: \\mathbb{R}^4 \\rightarrow \\mathbb{R}^3\\) is a linear transformation such that: \\[ T\\left(\\begin{bmatrix} 3 \\\\ ~2~ \\\\ 1 \\\\ 2 \\end{bmatrix} \\right)=\\begin{bmatrix} ~2~ \\\\ 3 \\\\ 6 \\end{bmatrix} \\qquad\\hbox{and}\\qquad T\\left(\\begin{bmatrix}~~2 \\\\ -1 \\\\ 0 \\\\ -1 \\end{bmatrix} \\right)=\\begin{bmatrix} 2 \\\\ ~0~ \\\\ 1 \\end{bmatrix}. \\] If that is all we know about \\(T\\), then do we have enough information to compute the value of \\(T\\) below? \\[T\\left(\\begin{bmatrix} 5 \\\\ 8 \\\\ ~3~ \\\\ 8 \\end{bmatrix} \\right) = \\hskip5in\\] If yes, then compute it (showing how you do so). If no, then explain why not. Hint: try to write the third input vector as a linear combination of the first two. 3.3 House Renovations Find the matrix of a linear transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) that performs the given transformation of my house. (Hint: use the base, the doorway and the peak of the roof as a guide.) Transformation #1 \\(\\qquad \\qquad\\) Transformation #2 \\(\\qquad \\qquad\\) 3.4 Matrix of a Nonlinear Transformation? This problem illustrates what happens if you try to make the matrix of a transformation that is not linear. Consider the transformation \\(T\\) defined by \\[ T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right) = \\begin{bmatrix} x_1 + x_2^2 + x_3 \\\\ 2 x_2 + x_1 x_3 + 1 \\\\ 2 x_1 + 3 x_2 + x_3 \\end{bmatrix} \\] This is not a linear transformation. Let’s see what happens if we compute its matrix anyway. Compute \\(T(\\mathbf{e}_1)\\), \\(T(\\mathbf{e}_2)\\), and \\(T(\\mathbf{e}_3)\\), and put the vectors you get in the columns of a matrix \\(A\\). Then compute the product below: \\[ \\underbrace{\\begin{bmatrix} \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\end{bmatrix}}_{A} \\begin{bmatrix} x_1 \\\\ x_ 2 \\\\ x_3 \\end{bmatrix} = \\] Explain how the result of this computation demonstrates that \\(T\\) is not linear. 3.5 A Proof Let \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) be a linear transformation. Suppose that \\(\\{v_1, v_2, v_3, v_4\\}\\) is a linearly independent set of vectors in \\(\\mathbb{R}^n\\) but the set of images \\(\\{T(v_1), T(v_2), T(v_3), T(v_4)\\}\\) is a linearly dependent set in \\(\\mathbb{R}^m\\). In the following steps, you will prove that \\(T\\) is not one-to-one. Write out clearly, using the definition, what it means for \\(\\{v_1, v_2, v_3, v_4\\}\\) to be linearly independent. Write out clearly, using the definition, what it means for \\(\\{T(v_1), T(v_2), T(v_3), T(v_4)\\}\\) to be linearly dependent. Use the definition of linear transformation and parts (a) and (b) above to argue that \\(T(x) = \\vec{0}\\) for some nonzero vector \\(x \\in \\mathbb{R}^n\\). Explain why this tells us that \\(T\\) is not one-to-one. 3.6 Inner and Outer Products We can also think of vectors as matrix. A column vector is an \\(n \\times 1\\) matrix and a row vector is a \\(1 \\times n\\) matrix. Compute the following products. These matrix products are called inner products (or dot products) of the vectors. \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\1 \\\\1 \\\\\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 2 \\\\ 5 \\\\ 0 \\\\ -1 \\\\\\end{bmatrix} = \\hskip3in \\] b. Now compute the following products. These are called outer products. \\[ \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} \\begin{bmatrix} 1 &amp; -5 &amp; 2 &amp; 3\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1\\end{bmatrix} =\\hskip3in \\] Row reduce both of the matrices that you get in part b (this should be easy to do by hand,but you can use R if you want to). How many pivots do you get? Explain why you always get this number of pivots when you row reduce an outer product. 3.7 Archaeological Seriation The matrix \\(A\\) below is used in archaeological dating. Its rows correspond to four different grave sites \\(G_1, G_2, G_3, G_4\\) and its columns correspond to five types of pottery\\(P_1, P_2, P_3, P_4, P_5\\). There is a 1 in position \\(i\\)-\\(j\\) if pottery type \\(P_j\\) is found in grave \\(G_i\\) (and a 0 otherwise). \\[ A=\\begin{array}{c|ccccc} &amp; P_1 &amp; P_2 &amp; P_3 &amp; P_4 &amp; P_5 \\\\ \\hline G_1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ G_2 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ G_3 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ G_4 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ \\end{array} \\] Compute the matrix \\(\\mathbf{G} = A A^T\\), where \\(A^T\\) is the transpose of \\(A\\), meaning that the rows and columns have been interchanged. Give the meaning of the \\(i\\)-\\(j\\) entry of \\(\\mathbf{G}\\) (the entry in row \\(i\\) and column \\(j\\)). State clearly the meaning of this entry using complete sentences (or sentence) and explain why it has this meaning. 3.8 Rental Car Solve this problem using R and turn in a markdown file knitted to .html. A group of Macalester alumni open a rental car company specializing in renting electric cars. As a start, they have opened offices in St. Paul, Rochester, and Duluth. Through market research they find that of the cars rented in St. Paul, 85% will get returned in St. Paul, 9% will get returned in Rochester, and 6% will get returned in Duluth. Of the cars rented in Rochester, 30% will get returned in St. Paul, 60% will get returned in Rochester, and 10% in Duluth. Of the cars rented in Duluth, 35% will get returned in St. Paul, 5% in Rochester, and 60% in Duluth. This information is represented in the matrix below. StP = c(.85,.09,.06) Roch = c(.30,.60,.10) Dul = c(.35,.05,.60) M = cbind(StP,Roch,Dul) M ## StP Roch Dul ## [1,] 0.85 0.3 0.35 ## [2,] 0.09 0.6 0.05 ## [3,] 0.06 0.1 0.60 Such a matrix is called a probability matrix or a stochastic matrix because it contains numbers between 0 and 1 and each of its columns sum to 1. The owners are trying to use this data to estimate how much of their fleet will be at each location on average in the long run. Assume that initially they locate 20 cars in each city. This can be recorded by the vector v0 = c(20,20,20). Apply, M to v0, call this vector v1, and explain, using how the matrix-vector product works, why v1 represents the number of cars at each location one day later (for simplicity, we assume that each rental is for 1 day). Now apply M to v1 and call it v2. This should represent the number of cars at each location 2 days later. Also compute the square of the matrix M and call it M2. Confirm that M2 times v0 is the same as M times v1. Write a for loop that applies M over and over again to see what happens to the distribution of cars in the long-run (we will learn how to do this in class but you can also probably just google it). Does this sequence stabilize or does it keep changing after each application? If it does stabilize, how long does it take to stabilize (to within 0.1 cars at each location). Does the starting distribution matter? Try 4 different starting distributions (with a total of 60 cars) and see what the final distribution looks like in each case. For one of your 4 starting distributions, try all 60 cars at one of the locations. 3.9 Adjacency Matrix You can do this problem in R or by hand. Consider the matrix \\(A\\) defined here A = rbind(c( 0 , 1 , 0 , 1 , 1 ,0), c(1 , 0 , 1 , 1 , 0, 0 ),c( 0 , 1 , 0 , 1 , 1, 0 ), c( 1 , 1 , 1 , 0 , 1, 0 ),c( 1 , 0 , 1 , 1 , 0, 1 ), c(0, 0, 0, 0, 1, 0)) A ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0 1 0 1 1 0 ## [2,] 1 0 1 1 0 0 ## [3,] 0 1 0 1 1 0 ## [4,] 1 1 1 0 1 0 ## [5,] 1 0 1 1 0 1 ## [6,] 0 0 0 0 1 0 This matrix represents the connections in the network diagram below. There is a 1 in position \\((i,j)\\) of the matrix if there is a connection (an edge) between vertex \\(i\\) and vertex \\(j\\) and there is a 0 if there is not. Compute \\(A v\\) where \\(v\\) is the vector of all 1’s. Explain what this new vector tells us about the graph. Compute \\(A^2 = A A\\), the square of the matrix \\(A\\). Look at the \\((2,5)\\) entry of \\(A^2\\). Explain what this entry says about connections in the network. Do the same for the \\((2,3)\\) and the \\((2,6)\\) entry of \\(A^2\\). "],["problem-set-4.html", "Section 4 Problem Set 4 4.1 Rainy Day in LA 4.2 Fibonacci Vectors 4.3 Vectors Rescaled 4.4 Polynomial Vector Spaces 4.5 Column and Null Space 4.6 Extend to a basis 4.7 Getting Into a Subspace 4.8 A Vector in Both Col(A) and Nul(A)", " Section 4 Problem Set 4 Due: Wednesday February 17 by noon CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 2.2-2.3 on Matrix Inverses and 4.1-4.3 on Subspaces and Bases. 4.1 Rainy Day in LA In Los Angeles if it rains today, there is a 50% chance it will rain tomorrow, but it if is sunny today, there is a 90% chance it will be sunny tomorrow. This is modeled in the rain-sunshine probability matrix P. \\[ P = \\begin{array}{c|cc|} &amp;\\text{rain}&amp;\\text{sun}\\\\ \\hline \\text{rain}&amp;1/2&amp;1/10\\\\ \\text{sun}&amp;1/2&amp;9/10\\\\ \\hline \\end{array} \\] This matrix works as follows: if the rain-sunshine probability today is (40, 60) (that is, 40% chance rain and 60% chance sunshine), then the rain-sunshine probability tomorrow is (26, 74) as seen by the calculation below. \\[ \\begin{bmatrix} 1/2 &amp; 1/10 \\\\ 1/2 &amp; 9/10 \\\\ \\end{bmatrix} \\begin{bmatrix} 40 \\\\ 60 \\end{bmatrix} = \\begin{bmatrix} 26 \\\\ 74 \\end{bmatrix} \\] Find the rain-sunshine probability the day after tomorrow. Compute \\(P^2\\) and explain the meaning of each of the four entries in the matrix. Find \\(P^{-1}\\) and and use it find the rain-sunshine probability yesterday if the rain-sunshine probability today is (40, 60). 4.2 Fibonacci Vectors The Fibonacci vectors \\(F\\) in \\(\\mathbb{R}^5\\) are defined below: \\[ F = \\left\\{ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} ~\\Bigg\\vert~ \\ x_3 = x_1 + x_2, x_4 = x_2 + x_3, x_5 = x_3 + x_4 \\right\\} \\subseteq \\mathbb{R}^5. \\hskip5in \\] Find a basis for \\(F\\). Be sure to show that your vectors span \\(F\\) and are linearly independent. 4.3 Vectors Rescaled If the function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^n\\) is a linear transformation, then show that the set below is a subspace of \\(\\mathbb{R}^n\\) \\[ E_2 = \\{\\ \\vec{x} \\in \\mathbb{R}^n \\mid T(\\vec{x}) = 2 \\vec{x} \\}. \\] Is there anything special about 2 in the definition? If it were replaced by another scalar, would it still be a subspace? 4.4 Polynomial Vector Spaces This problem refers to the information about the vector space of polynomials \\(\\mathcal{P}_n\\) found in the Day 13 class notes. Here are three subsets of \\(\\mathcal{P}_4\\). Decide if they are subspaces. In each case, if it is not a subspace, give examples using specific polynomials to show that one of the rules is broken, and if it is a subspace, show that the subspace rule holds for any two polynomials \\(p(x), q(x)\\) and any constant \\(c \\in \\mathbb{R}\\). \\(U = \\{ p(x) \\in \\mathcal{P}_4 \\mid p(1) = 0 \\}\\) \\(V = \\{ p(x) \\in \\mathcal{P}_4 \\mid p(0) = 1 \\}\\) \\(W= \\{ p(x) \\in \\mathcal{P}_4 \\mid p&#39;(1) = 0 \\}\\) 4.5 Column and Null Space Find a basis for the column space \\(Col(A)\\) and the null space \\(Nul(A)\\) of the following matrix \\(A\\) below (A = rbind(c(1, 2, 0, 2, 0, -1),c(1, 2, 1, 1, 0, -2), c(2, 4, -2, 6, 1, 2),c(1, 2, 0, 2, -1, -3 ))) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 0 2 0 -1 ## [2,] 1 2 1 1 0 -2 ## [3,] 2 4 -2 6 1 2 ## [4,] 1 2 0 2 -1 -3 4.6 Extend to a basis I am interested in the vectors below. I know that they do not span \\(\\mathbb{R}^5\\), because there are not enough of them, but I want to extend this set to a basis of \\(\\mathbb{R}^5\\) by adding some vectors to the set. \\[ \\begin{bmatrix} 5\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 4\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 1\\\\ 1\\\\ 1\\\\ 1\\\\ 1\\end{bmatrix}. \\] I searched online for ideas and one suggested that I make the matrix below and row reduce it. (A = cbind(c(5,4,3,1,2),c(4,4,3,1,2),c(1,1,1,1,1),diag(5))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 5 4 1 1 0 0 0 0 ## [2,] 4 4 1 0 1 0 0 0 ## [3,] 3 3 1 0 0 1 0 0 ## [4,] 1 1 1 0 0 0 1 0 ## [5,] 2 2 1 0 0 0 0 1 Row reduce this matrix. Use the result to come up with a basis for \\(\\mathbb{R}^5\\) that includes my original 3 vectors Explain why this method works. 4.7 Getting Into a Subspace Let \\(S \\subset \\mathbb{R}^n\\) be a subspace and let \\(\\mathsf{v}, \\mathsf{w} \\in \\mathbb{R}^n\\). For each of the following statements, either give a specific example or explain why it cannot happen. If \\(\\mathsf{v}\\) is in \\(S\\) but \\(\\mathsf{w}\\) is not in \\(S\\), can \\(\\mathsf{v} + \\mathsf{w}\\) be in \\(S\\)? If \\(\\mathsf{v}\\) is not in \\(S\\) and \\(\\mathsf{w}\\) is not in \\(S\\), can \\(\\mathsf{v} + \\mathsf{w}\\) be in \\(S\\)? If \\(\\mathsf{v}\\) is not in \\(S\\) and \\(c\\) is a nonzero constant, can \\(c\\mathsf{v}\\) be in \\(S\\)? 4.8 A Vector in Both Col(A) and Nul(A) Give a \\(3 \\times 3\\) matrix \\(A\\) for which the vector \\(\\mathsf{v} = \\begin{bmatrix}3 \\\\ -2 \\\\ 5 \\end{bmatrix}\\) is in both \\(\\mathrm{Col}(A)\\) and \\(\\mathrm{Nul}(A)\\). Be sure to demonstrate that \\(\\mathsf{v} \\in \\mathrm{Col}(A)\\) and \\(\\mathsf{v} \\in \\mathrm{Nul}(A)\\). "],["problem-set-5.html", "Section 5 Problem Set 5 5.1 A Tale of Two Bases 5.2 Dimension 5.3 Determinant Properties 5.4 Matrix Rank 5.5 A Tetrahedral Basis 5.6 Eigenbasis 5.7 House Party", " Section 5 Problem Set 5 Due: Tuesday February 23 by 11:59am CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 4.4, 4.5, 3.1, 5.1 5.1 A Tale of Two Bases I recommend using R on this problem. Consider the subspace \\(S\\) of \\(\\mathbb{R}^5\\) below. \\[ S = \\textsf{span}\\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 2 \\\\ \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ -1 \\\\ \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 1 \\\\ 3 \\\\ 0 \\\\ -2 \\\\ \\end{bmatrix}, \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 3 \\\\ \\end{bmatrix} \\right) \\] A = cbind(c(1,1,1,1,2),c(1, 2, 3, 0, -1),c(0, 0, 0, 1, 2), c(-1, 1, 3, 0, -2),c(2, 1, 0, 1, 3)) Give a basis of \\(S\\) consisting of some or all of the vectors used to define \\(S\\) above. Give a basis of \\(S\\) that has the nice standard basis property (i.e., the 0s and 1s property). For the two vectors below, decide if they are in \\(S\\). If the vector is in \\(S\\) then give its coordinates in each of your bases from parts (a) and (b). If you can do one of these “by hand” then explain how. \\[ \\mathbf{w} = \\begin{bmatrix} 8 \\\\ 11 \\\\ 14 \\\\ 7 \\\\ 11 \\end{bmatrix}, \\qquad \\mathbf{v} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\\\ 1 \\\\ 1 \\end{bmatrix}. \\] 5.2 Dimension Find the dimension of the subspace \\(Z\\) of \\(\\mathbb{R}^5\\) of zero-sum vectors below \\[ Z = \\left\\{ \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} \\ \\Bigg\\vert\\ \\ x_1 + x_2 + x_3 + x_4 + x_5 = 0\\ \\right\\}. \\] 5.3 Determinant Properties Turn in an R-markdown file with your solution to this problem. You can compute determinants in R using the det command. Here you will explore some properties of determinants. (A = rbind(c(3, 0, -1, 1, 2), c(1, 1, -1, 1, 1), c(-2, -3, -2, 3, 1), c(1, 3, 1, 3, 0), c(1, 3, 0, -2, 0))) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 3 0 -1 1 2 ## [2,] 1 1 -1 1 1 ## [3,] -2 -3 -2 3 1 ## [4,] 1 3 1 3 0 ## [5,] 1 3 0 -2 0 Compute the determinant of \\(A\\). Compute the determinant of \\(A^2\\). How does it compare to \\(det(A)\\)? Compute the determinant of \\(A^{-1}\\). How does it compare to \\(det(A)\\)? Swap two rows of \\(A\\) and then compute the determinant of the matrix that you get. Multiply the 4th row of the original matrix \\(A\\) by 7 and then compute the determinant of the matrix you get. How does it compare to \\(det(A)\\)? Compute the determinant of \\(7A\\). How does it compare to \\(det(A)\\)? Let \\(B\\) be the matrix below, and compute \\(det(B)\\), \\(det(A B)\\), and \\(det(A) det(B)\\). (B = rbind(c(1, 2,1, 1, 1), c(1, 2, 0, 1, -1), c(-2, -1, -2,0, 1), c(1, 0, 1, 3, 0), c(1, 0, 0, 1,1))) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 1 1 1 ## [2,] 1 2 0 1 -1 ## [3,] -2 -1 -2 0 1 ## [4,] 1 0 1 3 0 ## [5,] 1 0 0 1 1 5.4 Matrix Rank Fill in the entries of the table with T = true or F = false or I = not enough information to know. (Hint: draw a “picture” of the RREF of \\(\\mathsf{A}\\) in each case). .col2 { columns: 2 200px; /* number of columns and width in pixels*/ -webkit-columns: 2 200px; /* chrome, safari */ -moz-columns: 2 200px; /* firefox */ } .col3 { columns: 3 100px; -webkit-columns: 3 100px; -moz-columns: 3 100px; } (a) \\(\\mathsf{A}\\) is invertible (b) \\(\\mathsf{rref}(\\mathsf{A}) = I\\) (c) \\(\\mathsf{A}\\) has 8 pivots (d) \\(\\mathsf{A} \\mathbf{0} = \\mathbf{0}\\) (e) \\(\\mathsf{A} \\mathsf{x} = \\mathbf{0}\\) has more than one solution. (f) \\(T\\) is one-to-one (g) \\(T\\) is onto (h) \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has at least one solution for all \\(\\mathsf{b} \\in \\mathbb{R}^8\\). (i) The columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^8\\). (j) There is a vector \\(\\mathsf{b} \\in \\mathbb{R}^8\\) such that \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has no solutions. (k) There is a vector \\(\\mathsf{b} \\in \\mathbb{R}^8\\) such that \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has infinitely many solutions. (l) There is a vector \\(\\mathsf{b} \\in \\mathbb{R}^8\\) such that \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has exactly 17 solutions. (m) There is a vector \\(b \\in \\mathbb{R}^8\\) that can be written as a linear combination of the columns of \\(\\mathsf{A}\\) in more than one way. (n) The rows of \\(\\mathsf{A}\\) span a 7 dimensional subspace of \\(\\mathbb{R}^8\\). (o) The columns of \\(\\mathsf{A}\\) are linearly independent. (p) The rows of \\(\\mathsf{A}\\) are linearly independent \\[ \\begin{array}{|c|c|c|c|c|} \\hline &amp; T: \\mathbb{R}^8 \\to \\mathbb{R}^8 &amp; T: \\mathbb{R}^8 \\to \\mathbb{R}^8 &amp; T: \\mathbb{R}^7 \\to \\mathbb{R}^8 &amp; T: \\mathbb{R}^9 \\to \\mathbb{R}^8 \\\\ &amp; \\text{$\\mathsf{A}$ has rank 7} &amp; \\text{$\\mathsf{A}$ has rank 8} &amp; \\text{$\\mathsf{A}$ has rank 7} &amp; \\text{$\\mathsf{A}$ has rank 8} \\\\ \\hline (a) &amp; &amp; &amp; &amp; \\\\ \\hline (b) &amp; &amp; &amp; &amp; \\\\ \\hline (c) &amp; &amp; &amp; &amp; \\\\ \\hline (d) &amp; &amp; &amp; &amp; \\\\ \\hline (e) &amp; &amp; &amp; &amp; \\\\ \\hline (f) &amp; &amp; &amp; &amp; \\\\ \\hline (g) &amp; &amp; &amp; &amp; \\\\ \\hline (h) &amp; &amp; &amp; &amp; \\\\ \\hline (i) &amp; &amp; &amp; &amp; \\\\ \\hline (j) &amp; &amp; &amp; &amp; \\\\ \\hline (k) &amp; &amp; &amp; &amp; \\\\ \\hline (l) &amp; &amp; &amp; &amp; \\\\ \\hline (m) &amp; &amp; &amp; &amp; \\\\ \\hline (n) &amp; &amp; &amp; &amp; \\\\ \\hline (o) &amp; &amp; &amp; &amp; \\\\ \\hline (p) &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} \\] 5.5 A Tetrahedral Basis In practice, we change bases because problems are computationally easier in another coordinate system or because we learn something by looking at a problem from the point of view of a different coordinate system. The following example illustrates this with ideas that arises both in chemistry and computer graphics. Below is the tetrahedral molecule methane, \\(\\mathsf{CH}_4\\). Its coordinates can be described in 3-dimensional space by the vectors below. \\[ \\mathsf{C}=\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{H}_1=\\begin{bmatrix} 0 \\\\ 0 \\\\ \\frac{3}{2\\sqrt{6}} \\end{bmatrix}, \\mathsf{H}_2=\\begin{bmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ -\\frac{1}{2} \\\\ -\\frac{1}{2 \\sqrt{6}} \\end{bmatrix}, \\mathsf{H}_3=\\begin{bmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{2} \\\\ -\\frac{1}{2 \\sqrt{6}} \\end{bmatrix}, \\mathsf{H}_4=\\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\0 \\\\-\\frac{1}{2 \\sqrt{6}} \\end{bmatrix} \\] Let \\(\\mathcal{M} = \\{ \\mathsf{H}_1, \\mathsf{H}_2, \\mathsf{H_3} \\}\\). Then \\(\\mathcal{M}\\) is a basis of \\(\\mathbb{R}^3\\), which we will call the tetrahedral basis. You can see from the plot that these vectors are linearly independent (not all on the same plane) Express \\(\\mathsf{H}_4\\) in the tetrahedral basis. Hint: first compute \\(\\mathsf{H}_1+\\mathsf{H}_2+\\mathsf{H}_3+\\mathsf{H}_4\\) using the coordinates above. You do not need to do any row reductions for this question. Give the change of basis matrix \\(T\\) that converts from the tetrahedral basis \\(\\mathcal{M}\\) to the standard basis \\(\\mathcal{S}\\) and compute its inverse that converts from the standard basis back to \\(\\mathcal{M}\\). Chemists are interested in symmetry operations. These are linear transformations such that the atom looks the same after the transformation as it did before. For example one such operation is rotation \\(r_4\\) by 120\\(^o\\) around the \\(\\mathsf{H}_4\\) axis. This rotation sends \\(\\mathsf{H}_1\\) to \\(\\mathsf{H}_3\\), \\(\\mathsf{H}_3\\) to \\(\\mathsf{H}_2\\), and \\(\\mathsf{H}_2\\) to \\(\\mathsf{H}_1\\). Give the matrix of \\(r_4\\) in the \\(\\mathcal{M}\\) basis. r4.M = cbind(c(0,0,0),c(0,0,0),c(0,0,0)) rownames(r4.M) &lt;- c(&quot;H1&quot;,&quot;H2&quot;,&quot;H3&quot;) colnames(r4.M) &lt;- c(&quot;H1&quot;,&quot;H2&quot;,&quot;H3&quot;) r4.M ## H1 H2 H3 ## H1 0 0 0 ## H2 0 0 0 ## H3 0 0 0 It is a pain to describe these transformations in the standard basis, but it is easy and elegant to do so in the methane basis. We can now use the change-of-basis matrix to get the matrix in the standard basis. Compute the rotation in the standard basis by multiplying out these matrices in R. Use your matrices from parts b and c. Give the matrix in the \\(\\mathcal{M}\\)-basis for the following symmetry transformations: (ii) The rotation \\(r_2\\) around the \\(\\mathsf{H}_2\\) axis sending \\(\\mathsf{H}_1\\) to \\(\\mathsf{H}_3\\), \\(\\mathsf{H}_3\\) to \\(\\mathsf{H}_4\\), and \\(\\mathsf{H}_4\\) to \\(\\mathsf{H}_1\\). (iii) The reflection \\(\\sigma_{1,2}\\) across the plane containing \\(\\mathsf{H}_1, \\mathsf{H}_2,\\) and \\(\\mathsf{C}\\). You do not need to give these in standard coordinates. 5.6 Eigenbasis Consider the matrix \\(A\\) below \\[ A = \\frac{1}{3} \\begin{bmatrix} -14 &amp; 13 &amp; -2 \\\\ -20 &amp; 19 &amp; -2 \\\\ -23 &amp; 19 &amp; 1 \\\\ \\end{bmatrix} \\] and consider the following set basis of \\(\\mathbb{R}^3\\) (you don’t have to check that it is a basis). \\[ \\mathcal{B} = \\left\\{ v_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, v_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, v_3 = \\begin{bmatrix} -1 \\\\ -1 \\\\ 2 \\end{bmatrix} \\right\\} \\] By hand, compute \\(A v_1\\), \\(A v_2\\), and \\(A v_3\\) and see that each of these vectors is an eigenvector. From this, find the corresponding eigenvalues \\(\\lambda_1, \\lambda_2, \\lambda_3\\). A basis consisting of eigenvectors of \\(A\\) is called an eigenbasis. We will see that they often, but not always, exist. Let \\(B\\) be the change of basis matrix from \\(\\mathcal{B}\\) to \\(\\mathcal{S}\\) (the standard basis), and compute the product \\(B^{-1} A B\\). You can use R. Just report the matrix \\(B\\) and the matrix \\(B^{-1} A B\\). If you look closely at your answer, you should see something nice. Report what you see. 5.7 House Party Here is a plot of the grey house and four other houses, colored cyan, red, gold, and purple Reproduce this image using homogeneous coordinates. See Homogeneous Coordinates. ############# # your code for 3x3 matrices that create the transformed houses goes here A.red = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.purple = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.gold = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.cyan = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) #################### # you do not need to change this code house = cbind(c(0,0,1), c(0,3/4,1), c(2/4,3/4,1), c(2/4,0,1), c(4/4,0,1), c(4/4,4/4,1), c(5/4,4/4,1), c(0,8/4,1), c(-5/4,4/4,1), c(-4/4,4/4,1), c(-4/4,0,1), c(0,0,1)); plot(house[1,], house[2,], type = &quot;n&quot;, xlim=c(-2.5,2.5),ylim=c(-2.0,3.0),,xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-4:4, v=-4:4, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house.gold = A.gold %*% house polygon(house.gold[1,], house.gold[2,], col = &quot;gold&quot;, border = &quot;blue&quot;) house.cyan = A.cyan %*% house polygon(house.cyan[1,], house.cyan[2,], col = &quot;cyan&quot;, border = &quot;blue&quot;) house.red = A.red %*% house polygon(house.red[1,], house.red[2,], col = &quot;red&quot;, border = &quot;blue&quot;) house.purple= A.purple %*% house polygon(house.purple[1,], house.purple[2,], col = &quot;purple&quot;, border = &quot;blue&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) "],["problem-set-6.html", "Section 6 Problem Set 6 6.1 Rain and Sunshine Revisited 6.2 The Square Root of a Matrix? 6.3 Matrix Reconstruction 6.4 Coyotes and Roadrunners 6.5 Same Eigenvectors 6.6 Hunt Creek 6.7 Glucose-Insulin", " Section 6 Problem Set 6 Due: Wednesday March 03 by 11:59am CST (since Monday is a wellness day). Upload your solutions to Moodle in a PDF. You can download the Rmd source file for this problem set. The Problem Set covers sections 5.1, 5.2, 5.3, 5.6. 6.1 Rain and Sunshine Revisited On PS4, we encountered the rain-sunshine matrix \\(A\\) below \\[ A = \\begin{bmatrix} 1/2 &amp; 1/10 \\\\ 1/2 &amp; 9/10 \\\\ \\end{bmatrix}. \\] Perform the following calculations by hand and show your work. Find the characteristic polynomial of \\(A\\) and find its eigenvalues. Find an eigenvector for each eigenvalue and describe the eigenspaces. Diagonalize \\(A\\). Use your answer to (c) to give a formula for \\(A^n\\) and use this formula to compute \\(\\displaystyle{\\lim_{n\\to \\infty}} A^n\\). Write a loop in R that starts with the vector v = c(1,0) (i.e., a rainy day vector) and applies the matrix A = cbind(c(1/2,1/2),c(1/10,9/10)) over and over again (100 times). Explain how your answer compares to the answer to the previous problem. 6.2 The Square Root of a Matrix? The matrix \\(A =\\begin{bmatrix} 7 &amp; 2 \\\\ -4 &amp; 1 \\end{bmatrix}\\) has characteristic polynomial \\(c(\\lambda) = \\lambda^2 - 8 \\lambda + 15 = (\\lambda -3)(\\lambda - 5).\\) Describe the eigenspaces of \\(A\\). Diagonalize \\(A\\). Find a matrix that makes sense to call \\(\\sqrt{A}\\). Then show that when you square this matrix, you really do get matrix \\(A\\). 6.3 Matrix Reconstruction An unknown \\(3 \\times 3\\) matrix \\(M\\) has eigenvectors and corresponding eigenvalues: \\[ \\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\ \\lambda_1 = 1; \\qquad \\mathsf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix},\\ \\lambda_2 = \\frac{9}{10}; \\qquad \\mathsf{v}_3 = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix},\\ \\lambda_3 = 0. \\] Without using the matrix \\(M\\), compute \\(M^{10} \\mathsf{v}\\) where \\(\\mathsf{v} = \\begin{bmatrix}7\\\\3\\\\4\\end{bmatrix}\\). (That is, use only the eigen-information.) Describe all vectors \\(\\mathsf{v}\\), if there are any, such that \\(M^{n} \\mathsf{v} \\to {\\bf 0}\\) as \\(n \\to \\infty\\). Is it possible to reconstruct \\(M\\) from the evidence given? If so, then do it! If not, explain what further information is needed. 6.4 Coyotes and Roadrunners This summer, Macalester’s Ordway Natural History Study Area will be stocked with a population of coyotes and roadrunners so that Math 236 students can study real-life predator-prey dynamics. From similar experiments, we expect the predator-prey dynamics to be governed by linear model below. The eigenvalues of the matrix are also given. \\[ \\begin{bmatrix} \\phantom{\\Big\\vert} r_{t+1}\\phantom{\\Big\\vert} \\\\ \\phantom{\\Big\\vert} c_{t+1}\\phantom{\\Big\\vert} \\phantom{\\Big\\vert} \\end{bmatrix} =\\left[ \\begin{array}{cc} \\phantom{\\Big\\vert} \\frac{57}{50} &amp; -\\frac{6}{50} \\\\ \\phantom{\\Big\\vert} \\frac{4}{50} &amp; \\frac{43}{50} \\\\ \\end{array} \\right] \\begin{bmatrix} \\phantom{\\Big\\vert} r_t \\phantom{\\Big\\vert} \\\\ \\phantom{\\Big\\vert} c_t \\phantom{\\Big\\vert} \\end{bmatrix} = \\begin{bmatrix} \\phantom{\\Big\\vert} \\frac{57}{50} r_t - \\frac{6}{50} c_t \\\\ \\phantom{\\Big\\vert} \\frac{4}{50} r_t + \\frac{43}{50} c_t \\end{bmatrix}, \\] The eigenvalues and eigenvectors of this matrix are: \\[ \\begin{array}{lcl} \\lambda_1 = \\frac{11}{10} = 1.1, &amp; \\qquad &amp; \\lambda_2 = \\frac{9}{10} = 0.9 \\\\ \\mathsf{v}_1 = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} &amp;&amp; \\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\end{array} \\] A = cbind(c(57/50,4/50),c(-6/50,43/50)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] 0.9486833 0.4472136 ## [2,] 0.3162278 0.8944272 If \\(r_0 = 10\\) roadrunners and \\(c_0 = 15\\) coyotes are introduced to the area, then give for the population of coyotes \\(c_t\\) and roadrunners \\(r_t\\) after \\(t\\) years. In the long-term, in this model, what is the ratio of roadrunners to coyotes? When another college tried the same experiment in their Arboretum, they introduced \\(r_0 = 5\\) roadrunners and \\(c_0 = 10\\) coyotes and both populations died off (as is verified in the computation below). Explain why this happens using the eigenvalues and eigenvectors. A = cbind(c(57/50,4/50),c(-6/50,43/50)) v = c(5,10) for (i in 1:100) {v = A %*% v} v ## [,1] ## [1,] 0.0001328069 ## [2,] 0.0002656140 6.5 Same Eigenvectors Here are two matrices (A = cbind(c(-8, 3, 29), c(-40, 24, 46), c(10, 3, 11))) ## [,1] [,2] [,3] ## [1,] -8 -40 10 ## [2,] 3 24 3 ## [3,] 29 46 11 (B = cbind(c(4, 3, 35), c(-49, 42, 55), c(13, 3, 26))) ## [,1] [,2] [,3] ## [1,] 4 -49 13 ## [2,] 3 42 3 ## [3,] 35 55 26 Use R to show that they have the same eigenvectors but different eigenvalues. Show that \\(A B = B A\\) (even though we know that, in general, matrices do not commute). Now let \\(A\\) and \\(B\\) be any \\(n \\times n\\) matrices which have the same eigenvectors. Prove that \\(AB = BA\\). Hint: use the diagonalization of these two matrices. 6.6 Hunt Creek Age-structured population models like we saw in the Spotted Owl Example are often called Leslie Matrices, named after the British ecologist P.H. Leslie. Here is the Leslie Matrix of a population of brook trout in Hunt Creek in Michigan. The population is categorized into 5 age categories: fingerlings (0,1), yearlings (1-2), young adults (2-3), adults (3-4), and adults (4-5). Right now the population is seen to be dying off. The vector \\(p(t)\\) denotes the population at year \\(t\\) broken into the 5 age categories: \\[p(t) = (f (t), y(t), ya(t), a_1(t), a_2(t))^T\\] and the matrix \\(L\\) gives next year’s population from this year’s population: \\(p_{t+1} = L p_t\\). Below is the Leslie matrix for this example. \\[ \\begin{bmatrix} f (t+1) \\\\ y(t+1) \\\\ ya(t+1) \\\\ a_1(t+1) \\\\ a_2(t+1) \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 37 &amp; 64 &amp; 82 \\\\ 0.06 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0&amp;0.28 &amp; 0 &amp; 0 &amp; 0 \\\\ 0&amp;0&amp;0.16&amp; 0 &amp; 0 \\\\ 0&amp;0&amp;0&amp;0.08&amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} f (t) \\\\ y(t) \\\\ ya(t) \\\\ a_1(t) \\\\ a_2(t) \\end{bmatrix} = \\begin{bmatrix} 37 ya(t) + 64 a_1(t) + 82 a_2(t) \\\\ 0.06 f(t) \\\\ 0.28y(t) \\\\ 0.16 ya(t) \\\\ 0.08 a_1(t) \\end{bmatrix} \\] L = cbind(c(0,.06,0,0,0),c(0,0,.28,0,0),c(37,0,0,.16,0),c(64,0,0,0,.08),c(82,0,0,0,0.00)) The trout population in the creek is known to be dying off largely due to poisoning by the insecticide rotenone. The model demonstrates this behavior here, as can be seen in the folowing plot, which starts with 200 trout in each age group. You shouldn’t need to edit this plot. start = c(200,200,200,200,200) # the starting distribution N = 35 # N is the number of iterations X = matrix(0,nrow=5,ncol=N) # Store the results in a 3 x N matrix called X X[,1] = start # put start in the first column of X # loop N times and put your results in X for (i in 2:N) {X[,i] = L %*% X[,i-1]} # Then plot the results t = seq(1,N) # time plot(t,X[1,],type=&#39;l&#39;,col=1,ylim=c(0,8000),ylab=&quot;population&quot;,xlab=&quot;time (year)&quot;, main=&quot;Population in Age Group&quot;) for (i in 1:5) { lines(t,X[i,],col=i) points(t,X[i,],col=i,pch=20,cex=.8)} legend(22, 7600, legend=c(&quot;Fingerlings (0-1)&quot;, &quot;Yearlings (1-2)&quot;, &quot;Young Adults (2-3)&quot;,&quot;Adults (3-4)&quot;,&quot;Adults (4-5)&quot;), col=1:5, lty=1) Give the meaning of the values 37, 64, 82, 0.06, 0.28, 0.16, 0.08 that appear in this matrix. Compute the eigenvectors and eigenvalues of \\(L\\) and relate what you find to population dynamics. In particular, use the eigen-information to Give the overall population growth rate. Give the limiting age distribution: that is, the long-run distribution of the population into the different age categories. Give your answer as proportions which sum to 1. You are seeking funding from the Michigan DNR to support a cleanup effort. As part of your proposal, you argue that you believe that such a cleanup will most impact the youngest fish and will improve the survival rate of fingerlings to yearlings. Figure out (by trial and error) how high this survival rate will need to grow in order for the population to stop dying off. Justify your answer with eigenvalues and a plot. You should just be able to duplicate the code for the plot above (after changing the matrix). 6.7 Glucose-Insulin The hormone insulin helps regulate glucose metabolism in your blood. The presene of insulin helps your body absorb excess glucose. Here \\(G_t\\) (glucose) and \\(H_t\\) (insulin) are measued as excess values (in mg per 100 ml of blood) above the steady state. \\[ \\begin{bmatrix} G_{t+1} \\\\ H_{t+1} \\end{bmatrix} = \\begin{bmatrix} 0.9 &amp; -0.4 \\\\ 0.1 &amp; 0.9 \\\\ \\end{bmatrix} \\begin{bmatrix} G_t \\\\ H_t \\end{bmatrix}= \\begin{bmatrix} 0.9 G_t - 0.4 H_t \\\\ 0.1 G_t + 0.9 H_t \\end{bmatrix} \\] Here is what happens if we start at \\((1,0)\\) and iterate. That is we start with 1 unit excess glucose. Observe that the system spirals back to the steady state of \\((0,0)\\). We can plot the indivdual glucose and insulin coordinates over time. These are the x and y coordinates of the points in the above plot. You see the insulin responding to the excess glucose, and then the glucose being absorbed by the presence of insulin, and so on … The key point here is that the spiraling in the (x,y) plane or oscillating in the (x,t) plane corresponds to the presence of complex eigenvalues. Your job is to perform an eigen-analysis of this problem: Give a trajectory plot of this matrix using the trajectory_plot command from Dynamical Systems in 2D. Note: you need to copy the trajectory_plot code to your homework markdown file. When you do so, open the R chunk that contains it with ```{r,echo=FALSE}. Then when you knit, it won’t include all the code in the output. Use R to find the eigenvalues and eigenvectors. Write out the eigenvalues in the form \\(\\lambda = a \\pm b i\\) and the eigenvectors in the form \\(\\vec{v} = \\vec{u} \\pm \\vec{w} i.\\) Use this information to find the scaling factor \\(|\\lambda|\\) for this matrix and the angle of rotation \\(\\arctan(b/a)\\). Give your answer in degrees. Compare your answers from part (d) to the plots above to confirm that the system is doing what the eigenvalues predict. "],["problem-set-7.html", "Section 7 Problem Set 7 7.1 The Rise of Moscow 7.2 Orthogonal Complements 7.3 Orthogonal Diagonalization 7.4 Cosine Similarity 7.5 Fibonacci Orthogonality 7.6 Least Squares Solution to \\(\\mathsf{A} x = \\mathsf{b}\\)", " Section 7 Problem Set 7 Due: Tuesday March 09 by 11:59am CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all calculations, including row reduction, matrix multiplication, eigenvector calculation and inverse matrices. You can download the Rmd source file for this problem set. This problem set covers Network Centralities and Sections 6.1, 6.2, and 6.3 on Orthogonal Projections. 7.1 The Rise of Moscow Here is a video I made in Module 1 explaining the Airline Network example. If you collaborated on this one with classmates (and I hope you did), please include the names of the students that you worked with. Read Network Centralities and analyze the network of trade routes in medieval Russia given to you there. Create a vector containing the normalized Degree Centralities. See Section 24.2 for help. Create a vector containing the Gould Index values. See Section 24.3.5 for help. Plot the network where the size of the vertices is determined by Gould’s Index and the size of the label is determined by degree centrality. Create a data frame that contains Gould’s Index and Degree Centralities. The rows should be labeled with the city names and the columns should be named by the centrality measures. Sort according to Gould’s Index. Use Gould’s Index to decide whether Moscow’s dominance was solely due to its geographic location. Compare the Gould’s Index and Degree Centrality rankings and note any interesting findings. See Section 24.3.4 for help. 7.2 Orthogonal Complements Here are two subspaces of \\(\\mathbb{R}^5\\) that we have seen before. (See PS4.2 and PS5.2) \\[ \\begin{align} \\mathsf{Z} &amp; = \\left\\{ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} \\ \\bigg\\vert \\ x_1 + x_2 + x_3 + x_4 + x_5 = 0 \\right\\}. \\\\ \\mathsf{F} &amp; = \\left\\{ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} \\ \\bigg\\vert \\ x_3 = x_1 + x_2, x_4 = x_2 + x_3, x_5 = x_3 + x_4 \\right\\}. \\end{align} \\] Find the orthogonal complement of each subspace in \\(\\mathbb{R}^5\\). For each example, compute \\(\\dim(W) + \\dim(W^\\perp)\\). 7.3 Orthogonal Diagonalization Recall that a square \\(n \\times n\\) matrix is symmetric when \\(A^{\\top} = A\\). We learned that the eigenvectors of a symmetric matrix form an orthogonal basis of \\(\\mathbb{R}^n\\). In this problem, you will confirm that this holds for the following symmetric matrix \\[ A = \\begin{bmatrix} 0 &amp; 8 &amp; 10 &amp; -4 \\\\ 8 &amp; 4 &amp; 28 &amp; 6 \\\\ 10 &amp; 28 &amp; 3 &amp; -4 \\\\ -4 &amp; 6 &amp; -4 &amp; -7 \\end{bmatrix}. \\] Find the eigenvalues and eigenvectors of \\(A\\). Confirm that the eigenvectors returned by R are an orthonormal set (do this in a single calculation). Express the vector \\(\\mathsf{v} = \\begin{bmatrix} 2 &amp; -4 &amp; -9 &amp; -2 \\end{bmatrix}^{\\top}\\) as a linear combination of the eigenvectors. Use the fact that the eigenvectors are orthonormal. (Don’t augment and row reduce.) Let \\(P\\) be the matrix of these normalized, orthogonal eigenvectors. Diagonalize \\(A\\) using \\(P\\). Just write out \\(A = P D P^{-1}\\). Congratulations: you have orthogonally diagonalized the symmetric matrix \\(A\\)! Turn in: Your R code and the output for each part. For parts (c) and (d), you need to make it clear what your final answers are. 7.4 Cosine Similarity In high dimensional space \\(\\mathbb{R}^n\\) a common measure of similarity between two vectors is cosine similarity: the cosine of the angle \\(\\theta\\) between the vectors. We calculate this value as follows: \\[ \\cos(\\theta) = \\frac{ \\mathsf{u} \\cdot \\mathsf{v}} {\\| \\mathsf{u}\\| \\, \\|\\mathsf{v}\\|} = \\frac{ \\mathsf{u} \\cdot \\mathsf{v}} {\\sqrt{\\mathsf{u} \\cdot \\mathsf{u}} \\sqrt{\\mathsf{v} \\cdot \\mathsf{v}}}. \\] This measure has the following nice properties: \\(-1 \\le \\cos(\\theta) \\le 1\\), \\(\\cos(\\theta)\\) is close to 1 if \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are closely aligned, \\(\\cos(\\theta)\\) is close to 0 if \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are are orthogonal, \\(\\cos(\\theta)\\) is close to \\(-1\\) if \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are polar opposites. \\(\\cos(\\theta)\\) is positive if \\(\\theta\\) is acute (less than \\(90^o\\)). \\(\\cos(\\theta)\\) is negative if \\(\\theta\\) is obtuse (greater than \\(90^o\\)). Write a function cosine_similarity that takes as input two vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) and returns the value of \\(\\cos(\\theta)\\) for the angle \\(\\theta\\) between \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\). Below is a shell of the code that you need. Right now it always just returns 1.You need to fix that up. Demonstrate that your code works on some vectors in \\(\\mathbb{R}^5\\). Use vectors that are orthogonal, closely aligned, and close to polar opposites. cosine_similarity &lt;- function(u, v) { # your code goes here! # find the cosine of the angle between u and v cosine = 1 return (cosine) } In the file US_Senate_s21.Rmd you will find vectors of the voting record of the Senate in the 109th US Congress (2007-2008). You will see how to take the dot product of the voting record of two different senators. The dot product is always an integer. Explain what it counts. It is the number of something or possibly the difference of two things. Use your cosine_similary function to find the cosine similarity between every pair of the following senators: Hilary Clinton (D, NY), presidential candidate 2016 John McCain (R, AZ), presidential candidate 2008 Barack Obama (D, IL), president 2008-2016 Susan Collins (R, ME), moderate Republican Does the cosine similarity pick up on the fact that Senator Collins is a “moderate Republican”? The senate majority leader of the 109th Congress was Bill Frist (R, TN), and the senate minority leader was Harry Reid (D, NV). Create a function classify_senator(senator) that returns “R” or “D” depending on the cosine similarity of senator to frist and to reid. You will have to write an “if … else statement” (here is the syntax). There is a chunk of code in the R file I’ve given you that gets you started. Then run the my_classification code that we have given. I dentify any senators that have been misclassified using this method, meaning that their votes are more similar to the leader of the opposing party. Jim Jeffords (I, VT) was a Republican who became and Independent in 2001 and then caucused with the Democrats. How does your classifier handle Jeffords? 7.5 Fibonacci Orthogonality In problem 1, you saw that the vector space of Fibonacci vectors \\(\\mathsf{F} \\subseteq \\mathbb{R}^5\\) is a 2-dimensional subspace of \\(\\mathbb{R}^5\\). Below is an orthogonal basis of \\(\\mathsf{F}\\). Check that the basis vectors \\(\\{\\mathsf{f}_1, \\mathsf{f}_2\\}\\) are in \\(\\mathsf{F}\\) and are orthogonal. \\[ \\mathsf{F} = \\mathsf{span} \\left\\{ \\mathsf{f}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\mathsf{f}_2 = \\begin{bmatrix} -9 \\\\ 7 \\\\ -2 \\\\ 5 \\\\ 3\\end{bmatrix} \\right\\}, \\hskip.5in \\mathsf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 5 \\\\ 7\\end{bmatrix}.\\hskip.5in \\] The vector \\(\\mathsf{v}\\) above is not in \\(\\mathsf{F}\\) (check that!). Its projection \\(\\mathsf{p}\\) onto \\(\\mathsf{F}\\) is given by the formula below. Compute \\(\\mathsf{p}\\). This is closest approximation of \\(\\mathsf{f}\\) with a Fibonacci vector. \\[ \\mathsf{p} = \\frac{ (\\mathsf{v} \\cdot \\mathsf{f}_1)}{ ( \\mathsf{f}_1 \\cdot \\mathsf{f}_1)} \\mathsf{f}_1 + \\frac{ (\\mathsf{v} \\cdot \\mathsf{f}_2)}{ ( \\mathsf{f}_2 \\cdot \\mathsf{f}_2)} \\mathsf{f}_2. \\] This formula requires that the basis be orthogonal. The residual vector is the vector \\(\\mathsf{r} = \\mathsf{v} - \\mathsf{p}\\). Compute \\(\\mathsf{r}\\). Show that \\(\\mathsf{r}\\) is orthogonal to \\(\\mathsf{f}_1\\) and \\(\\mathsf{f}_2\\). Compute \\(||\\mathsf{r}||\\). This is the distance from \\(\\mathsf{v}\\) to \\(\\mathsf{F}\\) (i.e., how far \\(\\mathsf{v}\\) is from being Fibonacci). 7.6 Least Squares Solution to \\(\\mathsf{A} x = \\mathsf{b}\\) Find the least-squares solution to the following inconsistent matrix equation \\(\\mathsf{A} x = \\mathsf{b}\\). \\[ \\left[ \\begin{array}{ccc} 1 &amp; 1 &amp; 2 \\\\ 1 &amp; 0 &amp; 2 \\\\ -1 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ \\end{array} \\right] \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 11 \\\\ -3 \\\\ 10 \\\\ 7 \\end{bmatrix}. \\hskip5in \\] Here are the steps you should follow: Show that this system of equations is inconsistent. Make the normal equations: \\(\\mathsf{A}^T \\mathsf{A} x = \\mathsf{A}^T \\mathsf{b}\\). Solve the normal equations for \\(\\hat{x}\\) (by hand or software). Get the projection \\(\\hat{\\mathsf{b}}\\) of \\(\\mathsf{b}\\) onto \\(\\mathrm{Col}(\\mathsf{A})\\) by \\(\\hat{\\mathsf{b}} = \\mathsf{A} \\hat{x}\\). Get the residual \\(\\mathsf{r} = \\mathsf{b} - \\hat{\\mathsf{b}}\\) and compute its length. "],["problem-set-8.html", "Section 8 Problem Set 8 8.1 Subspace Projection 8.2 Least-Squares Polynomials 8.3 Fuel Efficiency 8.4 Fourier Analysis 8.5 Global Fossil Fuel Emissions", " Section 8 Problem Set 8 Download the Rmd source file for this problem set. Upload a completed, knitted .html version of this file on Moodle. If you have collaborated with others on this assignment (encouraged), please include their names here (no penalty). 8.1 Subspace Projection Last week we learned how to project onto a subspace, but our method required that we have an orthogonal basis. Here we will see that our least-squares method allows us to project onto a subspace with any (not-necessarily-orthgoonal) basis. Consider the following subspace of \\(\\mathbb{R}^4\\). We can turn it into a least-squares problem by making it the column space of a matrix \\(A\\). That is \\(W = Col(A)\\). \\[ W = span\\left\\{ \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\\\ -2 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\right\\}, \\hskip.6in b = \\begin{bmatrix} 9 \\\\ 5 \\\\ 5 \\\\ 8 \\end{bmatrix}. \\] ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 2 0 ## [3,] -1 3 1 ## [4,] -2 4 0 Perform a matrix computation on A to show that the basis is not orthogonal. Show that b is not in W. Find the least-squares projection of b onto W. Find both \\(\\hat x\\) and \\(\\hat b\\). Calculate the residual vector r, show that \\(r \\in W^\\perp\\), and find \\(||r||\\). Consider the following derivation from the normal equations: \\[ A^T A x = A^T b \\qquad \\Longrightarrow \\qquad \\hat x = (A^T A)^{-1} A^T b. \\] The pseudoinverse is the matrix \\[ A^+ = (A^T A)^{-1} A^T \\] From what we see above it gives the least-squares solution to \\(A x = b\\). Compute the matrix \\(A^+\\), multiply it by \\(b\\), and show that you get \\(\\hat x\\). Continuing this story, \\[ \\hat b = A \\hat x \\qquad \\Longrightarrow \\qquad \\hat b = A (A^T A)^{-1} A^T b. \\] The projection matrix onto the subspace \\(W\\) is the matrix \\[ P = A (A^T A)^{-1} A^T. \\] Compute the matrix \\(P\\), apply it to \\(b\\), and see that you get the projected value \\(\\hat b\\). Compute \\(P^2\\) and compare it to \\(P\\). Explain why this happens. Use it to project b2 = c(1,2,3,4) onto \\(W\\). Find the eigenvalues of \\(P\\). They are nice. Explain (briefly) where the eigenvectors of this matrix are in relation to \\(W\\). 8.2 Least-Squares Polynomials Here is the problem that we discussed in class with a quadratic fit to it. Make a cubic, quartic, and quintic fit to this data. Turn in a plot of each. Comupute the length of the residual in each case. Which do you think is the best model of the data? x = c(1,2,3,4,5,6) y = c(7,2,1,3,7,7) (A = cbind(x^0,x,x^2)) ## x ## [1,] 1 1 1 ## [2,] 1 2 4 ## [3,] 1 3 9 ## [4,] 1 4 16 ## [5,] 1 5 25 ## [6,] 1 6 36 xhat = solve(t(A)%*%A,t(A)%*%y) yhat = A %*% xhat r = y - yhat t(r) %*% r ## [,1] ## [1,] 11.26429 8.3 Fuel Efficiency Below is a classic data set of fuel efficiency in 38 different automobiles. ## lbs HP Cyl MPG ## BuickEstateWagon 3967.60 155 8 16.9 ## FordCountrySquireWagon 3689.14 142 8 15.5 ## ChevyMalibuWagon 3280.55 125 8 19.2 ## ChryslerLeBaronWagon 3585.40 150 8 18.5 ## Chevette 1961.05 68 4 30.0 ## ToyotaCorona 2329.60 95 4 27.5 ## Datsun510 2093.00 97 4 27.2 ## DodgeOmni 2029.30 75 4 30.9 ## Audi5000 2575.30 103 5 20.3 ## Volvo240GL 2857.40 125 6 17.0 ## Saab99GLE 2543.45 115 4 21.6 ## Peugeot694SL 3103.10 133 6 16.2 ## BuickCenturySpecial 3075.80 105 6 20.6 ## MercuryZephyr 2793.70 85 6 20.8 ## DodgeAspen 3294.20 110 6 18.6 ## AMCConcordD/L 3103.10 120 6 18.1 ## ChevyCapriceClassic 3494.40 130 8 17.0 ## FordLTD 3389.75 129 8 17.6 ## MercuryGrandMarquis 3599.05 138 8 16.5 ## DodgeStRegis 3485.30 135 8 18.2 ## FordMustang4 2352.35 88 4 26.5 ## FordMustangGhia 2648.10 109 6 21.9 ## MazdaGLC 1797.25 65 4 34.1 ## DodgeColt 1742.65 80 4 35.1 ## AMCSpirit 2429.70 80 4 27.4 ## VWScirocco 1810.90 71 4 31.5 ## HondaAccordLX 1942.85 68 4 29.5 ## BuickSkylark 2429.70 90 4 28.4 ## ChevyCitation 2361.45 115 6 28.8 ## OldsOmega 2457.00 115 6 26.8 ## PontiacPhoenix 2325.96 90 4 33.5 ## PlymouthHorizon 2002.00 70 4 34.2 ## Datsun210 1838.20 65 4 31.8 ## FiatStrada 1938.30 69 4 37.3 ## VWDasher 1992.90 78 4 30.5 ## Datsun810 2561.65 97 6 22.0 ## BMW320i 2366.00 110 4 21.5 ## VWRabbit 1925.00 71 4 31.9 Fit a linear model of the form \\[ mpg = a_0 + a_1 lbs + a_2 HP + a_3 Cyl. \\] Find the coefficients \\(a_0,a_1,a_2,a_3\\) and the length of the residual. If you have taken Stat 155, you can see that we are doing the exact same thing by comparing your results with ## ## Call: ## lm(formula = MPG ~ lbs + HP + Cyl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4669 -1.6011 -0.3246 1.0759 6.6231 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.644579 1.992433 24.917 &lt; 2e-16 *** ## lbs -0.008288 0.002316 -3.579 0.00106 ** ## HP -0.073961 0.043862 -1.686 0.10091 ## Cyl 0.791590 0.730326 1.084 0.28604 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.791 on 34 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.8183 ## F-statistic: 56.55 on 3 and 34 DF, p-value: 2.656e-13 Add the cars weight in tons to your model and solve \\(mpg = a_0 + a_1 (lbs) + a_2 (HP) + a_3 (Cyl) + a_4 (tons).\\) Compare the coefficients you get with those that you got in part a. Give a short explanation of what you see using some of the linear algebra language that we have learned in the course. The residual vector \\(\\mathsf{r}\\) measures the quality of fit of our model. But how do we turn this into a meaningful quantity? One method is to look at the coefficient of determination, which is more commonly refered to as the “\\(R^2\\) value.” You can see the \\(R^2\\) value of your fit in part (a) under the “Multiple R-squared” output in the linear model summary above. If \\(\\mathsf{y} = [ y_1, y_2, \\ldots, y_n ]^{\\top}\\) is our target vector with least-squares solution \\(\\hat{\\mathsf{y}} = A \\hat{\\mathsf{x}}\\) and residual vector is \\(\\mathsf{r} = \\mathsf{y} - \\hat{\\mathsf{y}}\\). Let \\[ a = \\frac{1}{n} ( y_1 + y_2 + \\cdots + y_n) \\] be the average or mean of the entries of target vector \\(\\mathsf{y}\\) and let \\(\\mathsf{y}^* = [a, a, \\ldots, a]\\). (We call this vector “y star”, so ystar would be a fine name in R.) The \\(R^2\\) value is \\[ R^2 = 1 - \\frac{\\| \\mathsf{y} - \\hat{\\mathsf{y}} \\|^2 }{\\| \\mathsf{y} - \\mathsf{y}^* \\|^2} = 1 - \\frac{\\| \\mathsf{r} \\|^2}{\\| \\mathsf{y} - \\mathsf{y}^* \\|^2}. \\] The \\(R^2\\) value is a number in \\([0,1]\\). The squared-length \\(|| \\mathsf{y} -\\mathsf{y}^*||^2\\) is the total variance: that is, how much the data varies from the mean, and \\(\\frac{\\| \\mathsf{r} \\|^2}{\\| \\mathsf{y} - \\mathsf{y}^* \\|^2}\\) tells us the fraction of the total variance that is explained by our model. Thus, if \\(R^2\\) is near 1, then our model does a good job at “explaining” the behavior of \\(\\mathsf{y}\\) via a linear combination of the columns of \\(A\\). To do: Find the \\(R^2\\) value for our least squares solution to the cars data in part (a). Here are some helpful functions: + mean(vec) returns the mean (average) of the entries of the vector vec + rep(a, n) creates a constant vector of length \\(n\\) where every entry is \\(a\\). + Norm(vec) from the pracma package returns the magnitude (Euclidean length) of the vector vec. To learn more, you should take STAT 155: Introduction to Statistical Modeling. 8.4 Fourier Analysis In Fourier analysis one uses trigonometric functions to model oscillatory behavior in data. These methods have important applications in the study of sound or video signals, financial data, medicine, and engineering (to mention just a few). For example, consider the following set of 200 data points. A first Fourier approximation would fit a model of the form \\[ f_1(t) = c_0 + c_1 \\sin(t) + c_2 \\cos(t). \\] Thus, we make the following matrix (we show here only the first 10 rows; there are 200 rows). ## [,1] [,2] [,3] ## [1,] 1 0.00000000 1.0000000 ## [2,] 1 0.09983342 0.9950042 ## [3,] 1 0.19866933 0.9800666 ## [4,] 1 0.29552021 0.9553365 ## [5,] 1 0.38941834 0.9210610 ## [6,] 1 0.47942554 0.8775826 ## [7,] 1 0.56464247 0.8253356 ## [8,] 1 0.64421769 0.7648422 ## [9,] 1 0.71735609 0.6967067 ## [10,] 1 0.78332691 0.6216100 Now we solve the normal equations ## [,1] ## [1,] 3.9971143 ## [2,] 1.0207277 ## [3,] -0.4486618 and plot the solution Your task: Update this to add the second Fourier coefficient terms by fitting the following function to the data. Plot your result. \\[ f_2(t) = c_0 + c_1 \\sin(t) + c_2 \\cos(t) + c_3 \\sin(2t) + c_4 \\cos(2t) \\] Compute the length of the residul vector for both the \\(f_1(t)\\) and the \\(f_2(t)\\) model. Which approximation looks better visually. That is, does the second approximation capture more of the shape of the data, or do you think that the first is a better model? 8.5 Global Fossil Fuel Emissions Below is a plot of global fossil fuel emmissions between 1751 and 1998 measured in megatons of carbon. year=c(1751:1998) emissions = c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 10, 9, 9, 9, 10, 10, 10, 10, 10, 11, 11, 11, 11, 12, 13, 14, 14, 14, 14, 14, 15, 16, 16, 17, 17, 18, 18, 18, 24, 23, 23, 24, 24, 25, 29, 29, 30, 31, 33, 34, 36, 37, 39, 43, 43, 46, 47, 50, 54, 54, 57, 59, 69, 71, 76, 77, 78, 83, 91, 95, 97, 104, 112, 119, 122, 130, 135, 142, 147, 156, 173, 184, 174, 188, 191, 194, 196, 210, 236, 243, 256, 272, 275, 277, 281, 295, 327, 327, 356, 372, 374, 370, 383, 406, 419, 440, 465, 507, 534, 552, 566, 617, 624, 663, 707, 784, 750, 785, 819, 836, 879, 943, 850, 838, 901, 955, 936, 806, 932, 803, 845, 970, 963, 975, 983, 1062, 1065, 1145, 1053, 940, 847, 893, 973, 1027, 1130, 1209, 1142, 1192, 1299, 1334, 1342, 1391, 1383, 1160, 1238, 1392, 1469, 1419, 1630, 1767, 1795, 1841, 1865, 2043, 2177, 2270, 2330, 2463, 2578, 2595, 2701, 2848, 3009, 3146, 3306, 3412, 3588, 3802, 4075, 4227, 4394, 4633, 4641, 4613, 4879, 5018, 5078, 5368, 5297, 5125, 5080, 5067, 5241, 5405, 5573, 5701, 5926, 6035, 6096, 6186, 6089, 6090, 6236, 6378, 6530, 6628, 6608) plot(year,emissions,pch=20,cex=.7,col=&quot;red&quot;) The data suggest that the fossil fuel emissions \\(f\\) follow an exponential model with respect to the year \\(y\\): \\[f = a e^{k(y-1750)},\\] where \\(a\\) and \\(k\\) are the unknown constants. This model is not linear in the unknowns \\(a\\) and \\(k\\), but (this is a great idea!) it becomes linear if we take the logarithm of both sides. Doing so yields the following linear system: \\[\\log(f)=\\log(a)+k(y-1750).\\] Note: This process works for any logarithm, but it is common to use the natural logarithm (use log() in R). Note: To simplify even further, we will define time=year-1750 so that time represents years after 1750. This results in the model \\[ \\log(f)=d+kt,\\] where \\(d=\\log(a)\\) and \\(t\\) is time (since 1750), Your task: Use least-squares projection to find the best fitting exponential function for this data. This will give you the values for \\(d\\) and \\(k\\), and once you know \\(d\\), you can find \\(a = \\exp(d)\\). We have started the code for you by defining x=year-1750 and y=log(emissions). ### your code goes here. # be sure to define d and k and A t=year-1750 y=log(emissions) A=cbind(1) # this isn&#39;t right, yet! d=1 k=0 ##### # your code above has found b and k (a = exp(d)) ## [1] 2.718282 k ## [1] 0 Run the code below to plot the original data along with your exponential model curve \\(f(t)\\). Note: This code assumes that you have already defined the values for k and a. Otherwise, it will not work! f=function(y){a * exp(k*(y-1750))} plot(year,f(year),type=&quot;l&quot;,lwd=3,ylim=c(0,10000),ylab=&quot;emissions&quot;, main=&quot;best fit exponential function&quot;) points(year,emissions,pch=20,cex=.7,col=&quot;red&quot;) "],["important-definitions.html", "Section 9 Important Definitions 9.1 Vector Spaces 9.2 Matrices 9.3 Orthogonality 9.4 Spectral Decompostion", " Section 9 Important Definitions 9.1 Vector Spaces span A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span a vector space \\(V\\) if for every \\(\\mathsf{v} \\in V\\) there exist a set of scalars (weights) \\(c_1, c_2, \\ldots, c_n \\in \\mathbb{R}\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, \\ldots, c_n]^{\\top}\\) is a solution to \\(A x = \\mathsf{v}\\). linear independence A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\ldots, \\mathsf{v}_n\\) are linearly independent if the only way to write \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] is with \\(c_1 = c_2 = \\cdots = c_n = 0\\). Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(A x = \\mathsf{0}\\) has only the trivial solution. linear dependence Conversely, a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly dependent if there exist scalars \\(c_1, c_2,\\ldots, c_n \\in \\mathbb{R}\\) that are not all equal to 0 such that \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] This is called a dependence relation among the vectors. Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, c_2, \\ldots, c_n]^{\\top}\\) is a nontrivial solution to \\(A \\mathsf{x} = \\mathsf{0}\\). linear transformation A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation when: \\(T(\\mathsf{u} + \\mathsf{v}) = T(\\mathsf{u}) + T(\\mathsf{v})\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\) (preserves addition) \\(T(c \\mathsf{u} ) = c T(\\mathsf{u})\\) for all \\(\\mathsf{u} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\) (preserves scalar multiplication). It follows from these that also \\(T(\\mathsf{0}) = \\mathsf{0}\\). one-to-one A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a one-to-one when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at most one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). onto A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a onto when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at least one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). subspace A subset \\(S \\subseteq \\mathbb{R}^n\\) is a subspace when: \\(\\mathsf{u} + \\mathsf{v} \\in S\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in S\\) (closed under addition) \\(c \\mathsf{u} \\in S\\) for all \\(\\mathsf{u}\\in S\\) and \\(c \\in \\mathbb{R}\\) (closed under scalar multiplication) It follows from these that also \\(\\mathsf{0} \\in S\\). basis A basis of a vector space (or subspace) \\(V\\) is a set of vectors \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) in \\(V\\) such that \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span \\(V\\) \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly independent Equivalently, one can say that \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) is a basis of \\(V\\) if for every vector \\(\\mathsf{v} \\in V\\) there is a unique set of scalars \\(c_1, \\ldots, c_n\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] (The fact that there is a set of vectors comes from the span; the fact that they are unique comes from linear independence). dimension The dimension of a subspace \\(W\\) is the number of vectors in any basis of \\(W\\). This is also the fewest number of vectors required to span the subspace. 9.2 Matrices invertible The square \\(n \\times n\\) matrix \\(A\\) is invertible when there exists an \\(n \\times n\\) matrix \\(A^{-1}\\) such that \\(A A^{-1} = I = A^{-1} A\\). The Invertible Matrix Theorem collects over two dozen equivalent conditions, each of which guarantees that \\(A\\) is invertible. null space The null space \\(\\mbox{Nul}(A) \\subset \\mathbb{R}^n\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of solutions to the homogeneous equation \\(A \\mathsf{x} = \\mathbf{0}\\)&gt; We also write this as \\[ \\mbox{Nul}(A) = \\{ \\mathsf{x} \\in \\mathbb{R}^n : A \\mathsf{x} = \\mathbf{0} \\} \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the kernel of \\(T\\) is the null space of matrix \\(A\\). column space The column space \\(\\mbox{Col}(A) \\subset \\mathbb{R}^m\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of all linear combinations of the columns of \\(A\\). For \\(A = \\begin{bmatrix} \\mathsf{a}_1 &amp; \\mathsf{a}_2 &amp; \\cdots &amp; \\mathsf{a}_n \\end{bmatrix}\\), we have \\[ \\mbox{Col}(A) = \\mbox{span} ( \\mathsf{a}_1, \\mathsf{a}_2, \\ldots , \\mathsf{a}_n ) \\] We also write this as \\[ \\mbox{Col}(A) = \\{ \\mathsf{b} \\in \\mathbb{R}^m : \\mathsf{b} = A \\mathsf{x} \\mbox{ for some } \\mathsf{x} \\in \\mathbb{R}^n \\}. \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the range (also called the image) of \\(T\\) is the column space of matrix \\(A\\). rank The rank of the \\(m \\times n\\) matrix \\(A\\) is the dimension of the column space of \\(A\\). This is also the number of pivot columns of the matrix. eigenvalue and eigenvector For a square \\(n \\times n\\) matrix \\(A\\), the scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue for \\(A\\) when there exists a nonzero vector \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(A \\mathsf{x} = \\lambda \\mathsf{x}\\). The nonzero vector \\(\\mathsf{x}\\) is the eigenvector for eigenvalue \\(\\lambda\\). The collection of all of these eigenvalues and eigenvectors is called the eigensystem of A. diagonalization A square \\(n \\times n\\) matrix is diagonalizable when \\(A = P D P^{-1}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an invertible matrix. In this case, the eigenvalues of \\(A\\) are the diagonal entries of \\(D\\) and their corresponding eigenvectors are the columns of \\(P\\). dominant eigenvalue The eigenvalue \\(\\lambda\\) of the square matrix \\(A\\) is the dominant eigenvalue when \\(| \\lambda | &gt; | \\mu |\\) where \\(\\mu\\) is any other eigenvalue of \\(A\\). The dominant eigenvalue determines the long-term behavior of \\(A^t\\) as \\(t \\rightarrow \\infty\\). 9.3 Orthogonality length The length of a vector \\(\\mathsf{v}\\) is \\[ \\| \\mathsf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}. \\] distance and angle The distance between vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) is \\[ \\mbox{dist}(\\mathsf{u},\\mathsf{v}) = \\| \\mathsf{u} - \\mathsf{v} \\|. \\] The angle \\(\\theta\\) between these vectors is determined by \\[ \\cos \\theta = \\frac{\\mathsf{u} \\cdot \\mathsf{v}}{ \\| \\mathsf{u} \\| \\, \\| \\mathsf{v} \\|}. \\] orthogonal The vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are orthogonal when \\(\\mathsf{u} \\cdot \\mathsf{v} = 0\\). This means that either one of them is the zero vector, or they are perpendicular to one another. orthogonal complement If \\(W \\subset \\mathbb{R}^n\\) is a subspace, then its orthogonal complement \\(W^{\\perp}\\) is the set of all vectors in \\(\\mathsf{R}^n\\) that are orthogonal to \\(W\\). We also write \\[ W^{\\perp} = \\{ \\mathsf{v} \\in \\mathbb{R}^n : \\mathsf{v} \\cdot \\mathsf{w} \\mbox{ for all } \\mathsf{w} \\in W \\}. \\] orthonormal set A collection of vectors \\(\\mathsf{u}_1, \\mathsf{u}_2, \\ldots, \\mathsf{u}_k\\) are an orthonormal set when every vector has length 1 and the vectors are pairwise orthogonal. orthogonal matrix orthogonal matrix A square \\(n \\times n\\) matrix \\(P\\) is an orthogonal matrix when its columns are an orthonormal set. As a result, we have \\(P^{-1} = P^{\\top}\\). projection and residual The orthogonal projection of vector \\(\\mathsf{y}\\) into a subspace \\(W\\) is the unique vector \\(\\hat{\\mathsf{y}} \\in W\\) such that \\(\\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}} \\in W^{\\perp}\\). The vector \\(\\mathsf{z}\\) is called the residual vector for the projection. 9.4 Spectral Decompostion orthogonal diagonalization Every symmetric \\(n \\times n\\) matrix is orthogonally diagonalizable, meaning that we have \\(A = P D P^{\\top}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix. The diagonal entries of \\(D\\) are the eigenvalues of \\(A\\) and the columns of \\(P\\) are the corresponding orthonormal eigenvectors. Furthermore, the eigenvalues of \\(A\\) are nonnegative. spectral decomposition A symmetric matrix \\(A\\) can be written as a linear combination of rank 1 matrices derived from the orthonormal eigensystem of \\(A\\). In particular, we have \\[ A = \\lambda_1 \\mathsf{u}_1 \\mathsf{u}_1^{\\top} + \\lambda_2 \\mathsf{u}_2 \\mathsf{u}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{u}_n \\mathsf{u}_n^{\\top}. \\] This linear combination of rank 1 vectors is called the spectral decomposition of \\(A\\). singular value decomposition (SVD) Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be factored into its singular value decomposition \\(U \\Sigma V^{\\top}\\) where \\(U\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\Sigma\\) is a matrix whose nonzero entries are the positive numbers \\(\\sigma_1, \\ldots , \\sigma_r\\), which appear in decreasing order on the diagonal, and \\(V\\) is an \\(n \\times n\\) orthogonal matrix. The nonzero entries of \\(\\Sigma\\) are called the singular values of \\(A\\). The columns of \\(U\\) are the left singular vectors and the rows of \\(V^{\\top}\\) are the right singular vectors. SVD spectral decomposition Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be written as a linear combination of rank 1 matrices derived from the singular value decomposition of \\(A\\). In particular, we have \\[ A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\sigma_r \\mathsf{u}_r \\mathsf{v}_r^{\\top}. \\] This linear combination of rank 1 vectors is called the (SVD) spectral decomposition of \\(A\\). "],["week-1-learning-goals.html", "Section 10 Week 1 Learning Goals 10.1 Solving Linear Equations 10.2 RStudio 10.3 Vocabulary 10.4 Conceptual Thinking", " Section 10 Week 1 Learning Goals Here are the knowledge and skills you should master by the end of this first, shorter week. 10.1 Solving Linear Equations I should be able to do the following tasks: Identify linear systems from nonlinear systems Create a linear system to solve a variety of applied scenarios Convert between a linear system and an augmented matrix Row reduce an augmented matrix into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Use REF to determine whether a linear system is consistent or inconsistent Use REF to determine whether a consistent system has a unique solution or an infinite number of solutions Use RREF to find explicit equations for the solution set of a consistent system 10.2 RStudio I should be able to do the following tasks: Log in to Macalester’s RStudio server Upload R Markdown files to RStudio Knit R Markdown to produce HTML Use RStudio to create vectors and matrices Use the rref command from pracma to solve a linear system 10.3 Vocabulary I should know and be able to use and explain the following terms: elementary row operation (and be able to state them) augmented matrix REF and RREF pivot position basic variable (pivot variable) free variable consistent system and inconsistent system 10.4 Conceptual Thinking I should understand and be able to perform the following conceptual tasks: Model 2-dimensional linear systems as the intersections of lines Model 3-dimensional linear systems as the intersections of planes "],["week-2-learning-goals.html", "Section 11 Week 2 Learning Goals 11.1 Solution Sets, Span and Linear Independence 11.2 Vocabulary 11.3 Conceptual Thinking", " Section 11 Week 2 Learning Goals Here are the knowledge and skills you should master by the end the second week. 11.1 Solution Sets, Span and Linear Independence I should be able to do the following tasks: Go back and forth between (i) systems of equations, (ii) vector equations, and (iii) the matrix equation \\(Ax = b\\). Compute and understand the matrix-vector product \\(A x\\) both as a linear combination of the columns of A and as the dot product of \\(x\\) with the rows of \\(A\\). Write the solution set to \\(Ax=b\\) as a parametric vector equation. Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) 11.2 Vocabulary I should know and be able to use and explain the following terms or properties. \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) homogeneous and nonhomogeneous equations parametric vector equations linear independence and linear dependence 11.3 Conceptual Thinking I should understand and be able to explain the following concepts: Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b \\in \\mathbb{R}^m\\), the system \\(A x = b\\) has at least one solution Each \\(b \\in \\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. Understand the relation between homogeneous solutions and nonhomogeneous solutions. Linear independence Span More than \\(n\\) vectors in \\(\\mathbb{R}^n\\) must be linearly dependent. "],["week-3-learning-goals.html", "Section 12 Week 3 Learning Goals 12.1 Linear Transformations and Matrix Inverses 12.2 Vocabulary 12.3 Conceptual Thinking", " Section 12 Week 3 Learning Goals Here are the knowledge and skills you should master by the end the third week. 12.1 Linear Transformations and Matrix Inverses I should be able to do the following tasks: Determine whether a mapping from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) is a linear transformation. Use the RREF of the corresponding matrix to determine whether \\(T(\\mathsf{x})\\) is one-to-one and/or onto. Describe 2D linear transformations as a mixture of geometric operations, including expansion, contraction, reflection, rotation, shearing and dimension reduction. Perform a 2D translation using 3D homogeneous coordinates. Multiply an \\(m \\times n\\) matrix with an \\(n \\times p\\) matrix to get an \\(m \\times p\\) matrix. Determine whether a \\(2 \\times 2\\) matrix is invertible. Find the inverse of a \\(2 \\times 2\\) matrix by hand. Use RStudio to check for invertiblity and to find the inverse of an \\(n \\times n\\) square matrix. Explain the connection between Gaussian Elimination, elementary matrices, and the matrix inverse. 12.2 Vocabulary I should know and be able to use and explain the following terms or properties. linear transformation: \\(T(a \\mathsf{u} + b \\mathsf{v}) = a T(\\mathsf{u}) + b T(\\mathsf{v})\\) domain, codomain (aka target) and range (aka image) \\(T\\) maps vector \\(\\mathsf{x}\\) to its image \\(T(\\mathsf{x})\\) one-to-one onto standard matrix for a linear transformation homogeneous coordinates transpose of a matix invertible matrix elementary matrices 12.3 Conceptual Thinking I should understand and be able to explain the following concepts: A linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) corresponds to multiplication by an \\(m \\times n\\) matrix \\(A\\). \\(T(\\mathsf{x})=\\mathsf{A} \\mathsf{x}\\) is a one-to-one linear transformations if and only \\(\\mathsf{A}\\) has linearly independent columns \\(T(\\mathsf{x})=\\mathsf{A} \\mathsf{x}\\) is an onto linear transformations if and only if the columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^m\\). The Invertible Matrix Theorem (Section 2.3, Theorem 8, page 112) is one of the highlights of the course! It gives 12 different conditions that all equivalent! You should think deeply about why everything comes together like this for square matrices. "],["week-4-learning-goals.html", "Section 13 Week 4 Learning Goals 13.1 Vector Spaces and the Determinant 13.2 Vocabulary 13.3 Conceptual Thinking", " Section 13 Week 4 Learning Goals Here are the knowledge and skills you should master by the end of the fourth week. 13.1 Vector Spaces and the Determinant I should be able to do the following tasks: Prove/disprove that a subset of a vector space is a subspace. Prove/disprove that a set of vectors is linearly dependent. Prove/disprove that a set of vectors span a vector space (or a subspace). Find the kernel and image of \\(T(\\mathsf{x}) = Ax\\). Determine whether a set of vectors is a basis. Find a basis for \\(\\mathrm{Nul}(A)\\) and a basis for \\(\\mathrm{Col}(A)\\). Find the change-of-coordinate matrix \\(P_{\\mathcal{B}}\\) from basis \\({\\mathcal{B}}\\) to the standard basis \\(\\mathcal{S}\\). Use matrix inverses (and RStudio) to find the change-of-coordinate matrix \\(P_{\\mathcal{B}}^{-1}\\) from basis \\({\\mathcal{S}}\\) to the standard basis \\(\\mathcal{B}\\). Find the coordinate vector with respect to a given basis. Find the dimension of a vector space (or subspace) by finding or verifying a basis. Find the determinant of a \\(2 \\times 2\\) matrix by hand. Find the determinant of a \\(3 \\times 3\\) matrix by using row operations/cofactor expansion/permutation method. Use RStudio to calculate the determinant of a square matrix. Use \\(\\det(A)\\) to decide whether the square matrix \\(A\\) is invertible. 13.2 Vocabulary I should know and be able to use and explain the following terms or properties. every one of these Important Definitions subspace null space and column space of a matrix kernel and image of a linear transformation basis coordinate vector with respect to a basis change-of-coordinates matrix the coordinate vector with respect to a basis the dimension of a vector space (or a subspace) determinant 13.3 Conceptual Thinking I should understand and be able to explain the following concepts: A vector space consists of a collection of vectors and all of their linear combinations. A subspace is a subset of a vector space that is also a vector space by itself (closed under linear combinations). The solutions to \\(A \\mathsf{x} = \\mathbb{0}\\) form a subspace. The span of the columns of \\(A\\) form a subspace. How the kernel and image of \\(T(\\mathsf{x}) = Ax\\) correspond to the nullspace and columnspace of \\(A\\). Every basis of a given vector space (or subspace) contains the same number of vectors. Why every vector in a vector space has a unique representation as a linear combination of a given basis \\({\\mathcal{B}}\\). How dimension relates to span and linear independence. Interpret \\(\\det(A)\\) as a measure the expansion/contraction of “volumes” in \\(\\mathbb{R}^n\\) under the linear transformation \\(T(\\mathsf{x})=A\\mathsf{x}\\). "],["week-5-6-learning-goals.html", "Section 14 Week 5-6 Learning Goals 14.1 Eigensystems 14.2 Vocabulary 14.3 Conceptual Thinking", " Section 14 Week 5-6 Learning Goals Here are the knowledge and skills you should master by the end of the fifth and sixth weeks. 14.1 Eigensystems I should be able to do the following tasks: Check whether a given vector \\(\\mathsf{v}\\) is an eigenvector for square matrix \\(A\\). Find the eigenvalues of a matrix \\(2 \\times 2\\) matrix by hand, using the characteristic equation Find the eigenvalues of a triangular matix by inspection. Given the eigenvalues of matrix \\(A\\), find the eigenvectors by solving \\((A - \\lambda I) = \\mathbf{0}\\). Find the eigenvalues and eigenvectors of an \\(n \\times n\\) matrix \\(A\\) using eigen(A) on RStudio. Determine whether a matrix is diagonalizable. Factor a diagonalizable \\(n \\times n\\) matrix as \\(A = PDP^{-1}\\) where \\(D\\) is a diagonal matrix of eigenvalues and \\(P\\) is the matrix whose columns are the corresponding eignvectors. Compute matrix powers using the diagonalization. Use RStudio to find complex eigenvalues and eigenvectors of a square matrix. Factor a \\(2 \\times 2\\) scaling-rotation matrix as \\(A = P C P^{-1}\\) where \\(C\\) is a scaling-rotation matrix \\(\\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix}\\) and \\(P = [ \\mathsf{w}, \\mathsf{u}]\\) where \\(\\mathsf{v} = \\mathsf{u} + i \\mathsf{w}\\) is the eigenvector for \\(\\lambda = a + b i\\). Find the angle of rotation and the scaling factor in a \\(2 \\times 2\\) matrix with complex eigenvalues. Use the dominant eigenvalue and dominant eigenvector to determine the long-term behavior of a dynamical system. Use eigenvalues to investigate a population modeled with a Leslie matrix. Give a close-formula for a dynamical system using the eigen decomposition of a matrix 14.2 Vocabulary I should know and be able to use and explain the following terms or properties. eigenvalue, eigenvector and eigenspace characteristic equation diagonalizable matrix similar matrices algebraic multiplicity of an eigenvalue geometric multiplicity of an eigenvalue scaling-rotation matrix discrete dynamical system trajectory dominant eigenvalue and dominant eigenvector population model Leslie matrix 14.3 Conceptual Thinking I should understand and be able to explain the following concepts: An eigenspace of \\(A\\) is a subspace that is fixed under the linear transformation \\(T(\\mathsf{x}) = A \\mathsf{x}\\). An eigenvalue \\(\\lambda\\) with \\(1 &lt;| \\lambda |\\) corresponds to expansion. An eigenvalue \\(\\lambda\\) with \\(0 &lt; | \\lambda | &lt; 1\\) corresponds to contraction. A complex eigenvalue corresponds to a rotation in a 2D subspace. The eigenspace for \\(\\lambda\\) is the subspace \\(E_\\lambda = \\mathrm{Nul}(A - \\lambda I)\\). A matrix is not diagonalizable when it has complex eigenvalues. A matrix is not diagonalizable when it has an eigenvalue whose algebraic mutiplicity is strictly larger than its geometrix multiplicity. The long-term behavior of a dynamical system is determined by its dominant eigenvalue and eigenvector. Population model predicts one of: long term growth, extinction, convergence to a stable population. "],["week-7-8-learning-goals.html", "Section 15 Week 7-8 Learning Goals 15.1 Orthogonality and SVD 15.2 Vocabulary 15.3 Conceptual Thinking", " Section 15 Week 7-8 Learning Goals Here are the knowledge and skills you should master by the end of the seventh and eighth weeks. 15.1 Orthogonality and SVD I should be able to do the following tasks: Find the length of a vector Find the distance between two vectors Normalize a vector Find the cosine of the angle between two vectors Find the orthogonal projection of one vector onto another Find the orthogonal projection of one vector onto a subspace (using an orthogonal basis) Find the orthogonal complement of a subspace Find the least squares approximation for an inconsistent system Formulate a curve fitting problem as an inconsistent linear system \\(A \\mathsf{x} = \\mathsf{b}\\) Orthogonally diagonalize a symmetric matrix as \\(A=PDP^{\\top}\\). Find the spectral decomposition \\(A = \\lambda_1 \\mathsf{v}_1 \\mathsf{v}_1^{\\top} + \\lambda_2 \\mathsf{v}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{v}_n \\mathsf{v}_n^{\\top}\\) of a symmetric matrix \\(A\\) Use an orthogonal diagonalization to find the best rank \\(k\\) approximation of symmetric matrix \\(A\\) 15.2 Vocabulary I should know and be able to use and explain the following terms or properties. dot product of two vectors \\(\\mathsf{v} \\cdot \\mathsf{w} = \\mathsf{v}^{\\top} \\mathsf{w}\\) (aka scalar product, inner product) length (magnitude) of a vector angle between vectors normalize unit vector orthogonal vectors orthogonal complement of a subspace orthogonal projection orthogonal basis orthonormal basis normal equations for a least squares approximation least squares solution residual vector symmetric matrix orthogonally diagonalizable outer product of two vectors \\(\\mathsf{v} \\, \\mathsf{w}^{\\top}\\) spectral decomposition of a symmetric matrix 15.3 Conceptual Thinking I should understand and be able to explain the following concepts: The dot product gives an algebraic encoding of the geometry (lengths and angles) of \\(\\mathbb{R}^n\\) If two vectors are orthogonal, then they are perpendicular, or one of them is the zero vector An orthogonal projection is a linear transformation The row space of a matrix is orthogonal to its nullspace The inverse of orthogonal matrix \\(A\\) is the transpose \\(A^{\\top}\\) Cosine similarity is a useful way to compare vectors, especially in high-dimensional vector spaces. The residual vector measures the quality of fit of a least squares solution The outer product \\(\\mathsf{v}\\, \\mathsf{w}^{\\top}\\) is a square matrix with rank 1 "],["linear-systems-in-r.html", "Section 16 Linear Systems in R 16.1 Getting started with R 16.2 Building Vectors and Matrices 16.3 Solving a Linear System 16.4 Solving another Linear System 16.5 Appendix: Dimensionless Vectors in R", " Section 16 Linear Systems in R 16.1 Getting started with R To use RStudio, you have two choices: Use the cloud version by logging in to Rstudio.macalester.edu. This is the easiest way to use RStudio and works great for our course. You can also download the free desktop version of RStudio. If you plan to go on to take more MSCS classes, especially in statistics and data science, you may want to use the desktop version. Download the desktop version following the instructions here: rstudio.com/products. Now, let’s learn how to use R to solve systems of linear equations! Download this Rmd file. First, we will create vectors and matrices Then we will see how to create an augmented matrix and then apply Gaussian Elimination to obtain is reduced row echelon form. Gaussian elimination is performed by the rref() command. However, this command is not loaded into R by default. So we have have to tell RStudio to use the practical math package, which is known as pracma. So we need to run the following command once at the beginning of our session. require(pracma) 16.2 Building Vectors and Matrices A vector in R is a list of data. The simplest way to create a vector is to use the c() command. The letter ‘c’ is short for ‘combine these values into a vector.’ For example, we can make a vector v for the numbers 1,2,3 as follows: v=c(1,2,3) v ## [1] 1 2 3 Note that we had to ask R to display the value of v. This is because the assignment of v doesn’t echo the value to the console. But can see the value of v in the Environment tab in the upper right panel of RStudio. For example, run this command and then check to see that the value of v gets updated in the environment. v=c(1,2,3,4,5,6) It is interesting to note that c() returns a dimensionless vector. So you can treat a vector c() as either a row or a column when you construct a matrix. For example, suppose that we want to make the matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] We could create this matrix by binding three row vectors: A = rbind(c(1,1,1), c(2,4,8), c(3,9,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 or we could bind three column vectors: A = cbind(c(1,2,3), c(1,4,9), c(1,8,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 16.3 Solving a Linear System Suppose that we want to solve the linear system \\[\\begin{aligned} x + y + z &amp;= 7 \\\\ 2x + 4y + 8z &amp;= 6 \\\\ 3x +9y+27z &amp;=12 \\end{aligned}\\] which has coefficient matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] and target (column) vector \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix}. \\] This is the same matrix A we defined above. Let’s define a vector b and use cbind() to create an augmented matrix which we will name Ab. (We could have just made the full augmented matrix from the start, but using cbind to add a column to a matrix is a skill we will use later in the course!) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 b = c(4,6,12) Ab = cbind(A,b) Ab ## b ## [1,] 1 1 1 4 ## [2,] 2 4 8 6 ## [3,] 3 9 27 12 Now we use the rref() command to apply Gaussian Elimination to produce the reduced row echelon form. (And remember: we had to load this function into R by using the require(pracma) command above.) rref(Ab) ## b ## [1,] 1 0 0 7 ## [2,] 0 1 0 -4 ## [3,] 0 0 1 1 We conclude that this is a consistent system no free variables. The unique solution is \\[\\begin{align} x&amp;=7\\\\ y&amp;=-4\\\\ z&amp;=1 \\end{align}\\] We can verify that our answer works by multiplying \\(A\\) by one of the solutions above. Matrix multiplication uses the funny operation %*%. A %*% c(7,-4,1) ## [,1] ## [1,] 4 ## [2,] 6 ## [3,] 12 #A %*% c(1,2,3,0,0) Which matches our target \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix} \\] just as we had hoped. 16.4 Solving another Linear System Now let’s find the solution set for the linear system \\[ \\begin{array}{rrrrrcr} x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ 2x_1 &amp; +x_2 &amp; +2x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; 4 \\\\ -x_1 &amp; +x_2 &amp; +x_3 &amp; &amp; &amp; = &amp; 10 \\\\ x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ \\end{array} \\] which corresponds to augmented matrix \\[ \\left[ \\begin{array}{rrrrr|r} 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ 2 &amp; +1 &amp; +2 &amp; -1 &amp; -1 &amp; 4 \\\\ -1 &amp; +1 &amp; +1 &amp; &amp; &amp; 10 \\\\ 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ \\end{array} \\right] \\] This time, let’s just construct the augmented matrix direclty. Then we define the coefficient matrix \\(A\\). Here we use cbind to combine the vectors into the columns of a matrix named \\(A\\). You can use rbind if you want to combine the vectors into the rows of a matrix. Ab = cbind(c(1,2,-1,1),c(0,1,1,0),c(-1,2,1,-1),c(-1,1,0,-1),c(-1,5,0,-1),c(-2,10,4,-2)) Ab ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 -1 -1 -1 -2 ## [2,] 2 1 2 1 5 10 ## [3,] -1 1 1 0 0 4 ## [4,] 1 0 -1 -1 -1 -2 And now let’s row reduce to get RREF. rref(Ab) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 1 1 ## [2,] 0 1 0 -1 -1 2 ## [3,] 0 0 1 1 2 3 ## [4,] 0 0 0 0 0 0 So the set of solutions in parametric form is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ 1 \\\\ -2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] and this is a “plane” in \\(\\mathbb{R}^5\\). It is in \\(\\mathbb{R}^5\\) because these vectors have 5 coordinates. It is a plane because it is spanned by two vectors that are not on the same line. 16.5 Appendix: Dimensionless Vectors in R Let’s revisit the vector constructed by cbind. Above we called this a “dimensionless” vector because it can be used as a column vector or a row vector. In general, R will do its best to make sense of a dimensionless vector. In other words, it will promote c() to make an expression valid. For example, let \\(A\\) be an \\(n \\times n\\) matrix, and let \\(b\\) be a vector. The expression \\(Av\\) is only defined when \\(v\\) is a \\(n \\times 1\\) column vector and that \\(wA\\) is only defined when \\(w\\) is a \\(1 \\times n\\) ** row vector**. But let’s look at what happens when we use a dimensionless vector instead. A = cbind(c(1,1,1),c(-1,0,1), c(0,1,-1)) A ## [,1] [,2] [,3] ## [1,] 1 -1 0 ## [2,] 1 0 1 ## [3,] 1 1 -1 b = c(2,5,11) b ## [1] 2 5 11 # A times b A %*% b ## [,1] ## [1,] -3 ## [2,] 13 ## [3,] -4 # b times A b %*% A ## [,1] [,2] [,3] ## [1,] 18 9 -6 Both of these multiplications worked! So R treated b as a column vector for the multiplicationA %*% b. And then R treated b as a row vector for the multiplication b %b% A. So how do you make a true column vector or a true row vector? The answer is to use cbind and rbind! Here are some examples: # dimensionless b = c(1,2,3,4) b ## [1] 1 2 3 4 # column vector b.col = cbind(b) b.col ## b ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 # row vector b.row = rbind(b) b.row ## [,1] [,2] [,3] [,4] ## b 1 2 3 4 "],["linear-dependence.html", "Section 17 Linear Dependence 17.1 Example 1: a 7x9 integer matrix 17.2 A 5 x 6 Numerical Matrix 17.3 Random Matrices", " Section 17 Linear Dependence In this activity, we will explore linear dependence and independence in the context of solving nonhomogeneous \\(A x = b\\) and homogeneous equations \\(A x = 0\\). Download this Rmd file. Remember that we will use the pracma package to get the rref function, so we first load it in: require(&quot;pracma&quot;) 17.1 Example 1: a 7x9 integer matrix Here is a 7 x 9 coeefficient matrix that we will use. These commands define it and echo it back. A = cbind( c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24)) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 And here is a vector b that we hope to use in solving A x = b. b = c(382, 51, -321, -314, -86, -170, -153) b ## [1] 382 51 -321 -314 -86 -170 -153 You can augment A with b, and call it Ab, using cbind: Ab = cbind(A,b) Ab ## b ## [1,] 3 5 3 4 0 -4 5 0 37 382 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 51 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 -321 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 -314 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 -86 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 -170 ## [7,] 1 4 5 -3 8 3 -1 0 -24 -153 And row reduce using rref rref(Ab) ## b ## [1,] 1 0 0 0 5 0 0 0 -2 8 ## [2,] 0 1 0 0 -2 0 0 0 2 10 ## [3,] 0 0 1 0 1 0 0 0 -3 -19 ## [4,] 0 0 0 1 -2 0 0 0 3 21 ## [5,] 0 0 0 0 0 1 0 0 0 6 ## [6,] 0 0 0 0 0 0 1 0 6 61 ## [7,] 0 0 0 0 0 0 0 1 0 8 17.1.1 Solution to the nonhomogeneous equations Ax = b Write out the solution to Ax=b in parametric form using the following formatting. You just need to fill in the correct values of the vectors: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 10 \\\\ -19 \\\\ 21 \\\\ 0 \\\\ 6 \\\\ 61 \\\\ 8 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} -5 \\\\ 2 \\\\ -1 \\\\ 2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} 2 \\\\ -2 \\\\ 3 \\\\ -3 \\\\ 0 \\\\ 0 \\\\ -6 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] Describe this solution space (by fixing up this sentence, which is incorrect right now): the set of solutions to A x= b is a plane in \\(\\mathbb{R}^9\\). 17.1.2 Solution to the nonhomogeneous equations Ax = 0 Now, describe the set of solutions to the homogeneous equations A x = 0. Again, you can just edit this: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} p1 \\\\ p2 \\\\ p3 \\\\ p4 \\\\ p5 \\\\ p6 \\\\ p7 \\\\ p8 \\\\ p9 \\end{bmatrix} + s \\begin{bmatrix} u1 \\\\ u2 \\\\ u3 \\\\ u4 \\\\ u5 \\\\ u6 \\\\ u7 \\\\ u8 \\\\ u9 \\end{bmatrix} + t \\begin{bmatrix} v1 \\\\ v2 \\\\ v3 \\\\ v4 \\\\ v5 \\\\ v6 \\\\ v7 \\\\ v8 \\\\ v9 \\end{bmatrix} \\] And describe, in words, the geometric relationship between the solutions to Ax=b and Ax=0. your answer here 17.1.3 Linearly dependent columns The columns of the matrix A are linearly dependent. You can see that in rref(A). rref(A) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Discuss in your group how you see it. Then write out a dependence relation among the columns by filling in numbers for the weights in this equation \\[ 0 = c_1 \\vec{a}_1 + c_2 \\vec{a}_2 + c_3 \\vec{a}_3 + c_4 \\vec{a}_4 + c_5 \\vec{a}_5 + c_6 \\vec{a}_6 + c_7 \\vec{a}_7 + c_8 \\vec{a}_8 + c_9 \\vec{a}_9. \\] Challenge: give a dependency relation that none of the other groups in the class have. This is telling us that there is some redundancy in the matrix A. Remove columns from A to get a new matrix M whose columns are linearly independent. You can do this by removing the appropriate columns from the code below: M = cbind( # you need to edit this matrix c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24) ) M ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 rref(M) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Your matrix should now be square (7x7) with linearly independent columns. R has a build in solve command, solve, that works for matrices of this form (i.e., square with linearly independent columns). You can try it here. First you need to un-comment-out the solve command. I have it commented out right now, because it does not work with the matrix M (above) until you remove its redundancies. # solve(M,b) Now, you should get a unique solution to the equation M x = b, since M has no free variables, and it should be one of the solutions to the original question A x = b. Which solution is it? That is, which of the many solutions to A x = b are you getting here (forw which values of the paramters?). Compare this with trying to use solve on the original equation A x = b with linearly dependent columns. The solve command in the next bit of code is commented out. Delete the comment command and try executing it. # solve(A,b) 17.2 A 5 x 6 Numerical Matrix So far, all of the matrices we’ve worked with in this class have integer values. This is only so that the calulations are nice to do by hand. All of our theory works over the real numbers. Here we will look at a real matrix with numerical values, something you might find when dealing with real-world data. B = cbind( c(0.717, -0.274, 0.365, 0.482, -0.362), c(0.587, -0.545, 0.5, -0.407, -0.597), c(-0.441, 0.886, 0.784, -0.831, -0.594), c(0.923, -0.466, 0.222, 0.867, 0.493), c(-0.42, -0.745, -0.02, -0.44, 0.209), c(0.621, 0.049, -0.134, -0.844, -0.31) ) B ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.717 0.587 -0.441 0.923 -0.420 0.621 ## [2,] -0.274 -0.545 0.886 -0.466 -0.745 0.049 ## [3,] 0.365 0.500 0.784 0.222 -0.020 -0.134 ## [4,] 0.482 -0.407 -0.831 0.867 -0.440 -0.844 ## [5,] -0.362 -0.597 -0.594 0.493 0.209 -0.310 and here is a vector d in \\(\\mathbb{R}^5\\). d = c(5.886, -4.001, 3.701, -6.621, -2.199) d ## [1] 5.886 -4.001 3.701 -6.621 -2.199 Try answering some of these questions: Are the columns of B linearly independent? Do the columns of B span \\(\\mathbb{R}^5\\)? Give the parametric solution to B x = d. What is the geometric form of this solution (e.g., a plane in \\(\\mathbb{R}^4\\))? Remove redundancies from the columns of B to get a new matrix B2 and use solve to solve the equation B2 x = d. Which of the parametric solutions to you get. 17.3 Random Matrices The following code generates a random 5 x 5 matrix. Every time you enter it, it will give you a new matrix. Use this to try to figure out how likely it is that a random square matrix has linearly dependent columns. R1 = matrix(runif(5*5), nrow = 5, ncol = 5) R1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.1184689 0.1239424 0.1420546 0.38033581 0.5928765 ## [2,] 0.4217238 0.8711342 0.8435237 0.17954405 0.7903798 ## [3,] 0.5855844 0.1123338 0.7855272 0.07088579 0.6929428 ## [4,] 0.5112875 0.8575215 0.3534619 0.52137452 0.1753942 ## [5,] 0.9409891 0.6477325 0.3017961 0.34152585 0.4227388 Try the same using the following code that generates a random 5 x 6 matrix. R2 = matrix(runif(5*6), nrow = 5, ncol = 6) R2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.47150661 0.5366943 0.4551525 0.5407866 0.2121572 0.2340912 ## [2,] 0.16764022 0.4415726 0.8494005 0.1755968 0.9941802 0.7120939 ## [3,] 0.48996204 0.4125602 0.6368018 0.7142944 0.3006777 0.5845530 ## [4,] 0.72257888 0.1700376 0.7924064 0.8796394 0.9375319 0.2329733 ## [5,] 0.06819129 0.5428337 0.3318470 0.8570837 0.2606611 0.6455901 Try the same using the following code that generates a random 5 x 4 matrix. R3 = matrix(runif(5*4), nrow = 5, ncol = 4) R3 ## [,1] [,2] [,3] [,4] ## [1,] 0.6621675 0.82418524 0.7650590 0.7465529 ## [2,] 0.6165469 0.16873313 0.8384848 0.3032072 ## [3,] 0.2872389 0.84969832 0.4445548 0.9522272 ## [4,] 0.1511305 0.05050793 0.1506577 0.9258081 ## [5,] 0.7931637 0.20442428 0.5570917 0.4452665 rref(R3) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## [5,] 0 0 0 0 In each of these cases, how likely is it that the columns of the matrix spans all of \\(\\mathbb{R}^4\\)? "],["matrix-multiplication.html", "Section 18 Matrix Multiplication", " Section 18 Matrix Multiplication Download this Rmd file from GitHub Here we will practice multiplying matrices in R. First, let’s define a few matrices.I’m using a trick here. By putting the assignment in parentheses, it assigns the matrix and displays it.s (A = cbind(c(1,2,3),c(4,5,6),c(1,1,-1))) ## [,1] [,2] [,3] ## [1,] 1 4 1 ## [2,] 2 5 1 ## [3,] 3 6 -1 (B = cbind(c(1,-1,1),c(1,1,1),c(0,2,1))) ## [,1] [,2] [,3] ## [1,] 1 1 0 ## [2,] -1 1 2 ## [3,] 1 1 1 (C = cbind(c(2,1,1),c(1,0,1),c(1,-3,1),c(3,2,1))) ## [,1] [,2] [,3] [,4] ## [1,] 2 1 1 3 ## [2,] 1 0 -3 2 ## [3,] 1 1 1 1 We multiply using the %*% command. As seen here: A %*% B ## [,1] [,2] [,3] ## [1,] -2 6 9 ## [2,] -2 8 11 ## [3,] -4 8 11 Note that B %*% A ## [,1] [,2] [,3] ## [1,] 3 9 2 ## [2,] 7 13 -2 ## [3,] 6 15 1 What do these last two multiplications say about the matrix product AB and BA? This is a very important property (or, perhaps, lack of property) of matrix multiplication. Try multiplying BC and CB. What happens? And why? The transpose of a matrix is computed by t(A). Compute the transpose of the matrices A, B, C and be sure that you all understand what it does. The command diag(n) gives the n x n identity matrix. This is denoted \\(I_n\\). For example, here is \\(I_3\\). diag(3) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Compute \\(I_2\\), \\(I_4\\), and \\(I_5\\) and be sure you all agree on what the identity matrix is. Multiply the matrices A and B by the appropriately-sized identity matrix. Multiply both ways, A I and I A, and agree upon what multiplying by the identity does. Multiply C by an identity matrix I C and C I. You might need a different size on the left and on the right. Our topic for Thursday (tomorrow) is the inverse of a matrix. You compute the inverse of the matrix A with solve(A). Try this solve(B) ## [,1] [,2] [,3] ## [1,] -0.5 -0.5 1 ## [2,] 1.5 0.5 -1 ## [3,] -1.0 0.0 1 B %*% solve(B) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Multiply A by its inverse and look closely at the answer you get. solve(A) ## [,1] [,2] [,3] ## [1,] -1.8333333 1.6666667 -0.1666667 ## [2,] 0.8333333 -0.6666667 0.1666667 ## [3,] -0.5000000 1.0000000 -0.5000000 A %*% solve(A) ## [,1] [,2] [,3] ## [1,] 1.000000e+00 0.000000e+00 0.000000e+00 ## [2,] 3.330669e-16 1.000000e+00 -1.110223e-16 ## [3,] 1.110223e-16 -2.220446e-16 1.000000e+00 Some matrices do not have inverses. Try computing the inverse of the following matrices. We will discuss this tomorrow! (M1 = cbind(c(3,5),c(-2,1))) ## [,1] [,2] ## [1,] 3 -2 ## [2,] 5 1 (M2 = cbind(c(4,3),c(5,4))) ## [,1] [,2] ## [1,] 4 5 ## [2,] 3 4 (M3 = cbind(c(4,2),c(10,5))) ## [,1] [,2] ## [1,] 4 10 ## [2,] 2 5 Enter the matrix A in problem 3.7 in the homework. Then compute the matrix G which is A times its transpose. Discuss its meaning. "],["linear-transformations-of-a-house.html", "Section 19 Linear Transformations of a House 19.1 Rotations 19.2 Expansion and contraction 19.3 Reflection 19.4 Shear Transformations 19.5 Dimension Reduction 19.6 Your Turn", " Section 19 Linear Transformations of a House Download this Rmd file from GitHub Here is a plot of my house. You will need to run this chunk of code each time you re-start R to get the house back in memory. house = cbind(c(0,0), c(0,3/4), c(1/2,3/4), c(1/2,0), c(1,0), c(1,1), c(5/4,1), c(0,2), c(-5/4,1), c(-1,1), c(-1,0), c(0,0)); plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) Here we explore linear transformations on the plane by looking at their effect on my house. We give a series of examples of 2D linear transformations. After each example, it’s your turn to play with variations from the same family of transformations. 19.1 Rotations Suppose that we wish to rotate my house by pi/3 radians. As we’ve seen, a 2D rotation matrix by \\(t\\) radians, counter-clockwise, is given by \\[A=\\displaystyle{ \\begin{bmatrix} \\cos(t) &amp; -\\sin(t) \\\\ \\sin(t) &amp; \\cos(t) \\end{bmatrix}}.\\] Here is the code to display this transformation. Observe that I apply the matrix A to the house, call it house2 and plot both the original house and the new house in the same plot. # define the matrix A. This is the only part you should need to edit. t = pi/3 A = cbind(c(cos(t),sin(t)),c(-sin(t),cos(t))) A # display the matrix A ## [,1] [,2] ## [1,] 0.5000000 -0.8660254 ## [2,] 0.8660254 0.5000000 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) Try changing the angle above to achieve different rotations. How can you rotate it clockwise? 19.2 Expansion and contraction Next, we scale the house by 2 in the \\(x\\)-direction and by 3 in the \\(y\\)-direction. # define the matrix A. This is the only part you should need to edit. A = cbind(c(2,0),c(0,3)) A # display the matrix A ## [,1] [,2] ## [1,] 2 0 ## [2,] 0 3 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) Your turn: try some different scale factors. What if you use negative scale factors. 19.3 Reflection Now we reflect over the line y = x. # define the matrix A. This is the only part you should need to edit. A = cbind(c(0,1),c(1,0)) A # display the matrix A ## [,1] [,2] ## [1,] 0 1 ## [2,] 1 0 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) Your turn: try the reflections (1) over the x-axis; (2) over the y-axis; and (3) through the origin, i.e., sending (x,y) to (-x,-y). 19.4 Shear Transformations A shear transformation is of the form \\[ A=\\displaystyle{ \\begin{bmatrix} a &amp; b \\\\ 0 &amp; c \\end{bmatrix}} \\quad \\mbox{and} \\quad A=\\displaystyle{ \\begin{bmatrix} a &amp; 0 \\\\ b &amp; c \\end{bmatrix}} \\] For example: # define the matrix A. This is the only part you should need to edit. A = cbind(c(1,0),c(1,1)) A # display the matrix A ## [,1] [,2] ## [1,] 1 1 ## [2,] 0 1 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) You try: try to get the house to slant in the other direction. 19.5 Dimension Reduction Here we perform the transformation that sends \\(\\mathsf{e}_1\\) to \\((-1,1/2)\\) and \\(\\mathsf{e}_2\\) to \\((2,-1)\\). Notice that they are the same line and the transformation projects the house onto this line. # define the matrix A. This is the only part you should need to edit. A = cbind(c(-1,1/2),c(2,-1)) A # display the matrix A ## [,1] [,2] ## [1,] -1.0 2 ## [2,] 0.5 -1 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 19.6 Your Turn See if you can do the transformations in problem 3.3. # define the matrix A. This is the only part you should need to edit. A = cbind(c(1,0),c(0,1)) A # display the matrix A ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) "],["homogeneous-coordinates.html", "Section 20 Homogeneous Coordinates 20.1 Translation 20.2 Rotation and then Translation 20.3 Your Turn", " Section 20 Homogeneous Coordinates Download this Rmd file A translation of the plane shifts every vector by a constant vector. For example, the mapping \\[ S \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\right) = \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} x +3 \\\\ y - 4 \\end{bmatrix} \\] translates every vector in the plane by \\(\\begin{bmatrix} 3 \\\\ -4~ \\end{bmatrix}\\). The bad news: This is a simple and natural mapping, but it is not a linear transformation! We know that a linear transformation must map \\(\\mathbb{0}\\) to \\(\\mathbb{0}\\), and that is certainly not the case when we translate! This restriction is rather limiting for computer graphics: we can never move our image away from the origin. The good news: We can work around this problem by creating a 3D linear transformation \\(T: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3\\) and then retricting our attention to a plane in this larger space. As discussed in the Homogeneous Coordinates video, we do the following: Embed the \\(xy\\)-plane \\(\\mathbb{R}^2\\) into the plane \\(z = 1\\) in \\(\\mathbb{R}^3\\). Translate in \\(\\mathbb{R}^3\\) using a mapping \\(T\\) that maps this horizontal plane to itself. That is: \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\right) = \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}. \\] When we create our plot, we use only the first two coordinates and ignore the third coordinate (which is still 1). In summary, during our calculations, we replace the vector \\(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\) in \\(\\mathbb{R}^2\\) with the homogeneous coordinate vector \\(\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\) in \\(\\mathbb{R}^3\\). 20.1 Translation Here is my house once again. Note that we have add \\(z=1\\) as the third coordinate to each point. However, when plotting, we only use the first two coordinates. # the third entry always = 1 house = cbind(c(0,0,1), c(0,3/4,1), c(1/2,3/4,1), c(1/2,0,1), c(1,0,1), c(1,1,1), c(5/4,1,1), c(0,2,1), c(-5/4,1,1), c(-1,1,1), c(-1,0,1), c(0,0,1)); # only plot the first two coordinates plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) Next, we translate by \\(\\begin{bmatrix} 3 \\\\ - 4 \\end{bmatrix}\\) by using the linear transformation \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\right) = \\begin{bmatrix} 1 &amp; 0 &amp; 3 \\\\ 0 &amp; 1 &amp; -4 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}= \\begin{bmatrix} x+3 \\\\ y - 4 \\\\ 1 \\end{bmatrix}. \\] Now, let’s do this calculation in R and plot the first two coordiantes: A = cbind(c(1,0,0),c(0,1,0),c(3,-4,1)) house2 = A %*% house # only plot the first two coordinates plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 20.2 Rotation and then Translation We know plenty of interesting 2D linear transformations, including rotation, reflection and shear mappings. We can turn any of them into a 3D transformation by appending a row and a column with a 1 in the lower right corner and zero everywhere else. For example, the 2D rotation \\[ \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta~ \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix} \\] becomes the 3D transformation \\[ \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta~ &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp;0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] This mapping rotates 3D space around the \\(z\\)-axis. So let’s combine two operations: a rotation and a translation First, let’s rotate counterclockwise by \\(2 \\pi/3\\) and then translate by \\(\\begin{bmatrix} -2 \\\\ 4 \\end{bmatrix}\\). And remember: the matrix closest to the vector acts first. So if we want to translate first, the translation matrix needs to be to the right of the rotation matrix: \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\right) = \\begin{bmatrix} 1 &amp; 0 &amp; -2 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\cos \\frac{2\\pi}{3} &amp; -\\sin\\frac{2\\pi}{3}~ &amp; 0 \\\\ \\sin\\frac{2\\pi}{3} &amp; \\cos\\frac{2\\pi}{3} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}. \\] t = 2*pi/3 rot = cbind(c(cos(t),sin(t),0),c(-sin(t),cos(t),0),c(0,0,1)) rot ## [,1] [,2] [,3] ## [1,] -0.5000000 -0.8660254 0 ## [2,] 0.8660254 -0.5000000 0 ## [3,] 0.0000000 0.0000000 1 translate = cbind(c(1,0,0),c(0,1,0),c(-2,4,1)) translate ## [,1] [,2] [,3] ## [1,] 1 0 -2 ## [2,] 0 1 4 ## [3,] 0 0 1 A = translate %*% rot A ## [,1] [,2] [,3] ## [1,] -0.5000000 -0.8660254 -2 ## [2,] 0.8660254 -0.5000000 4 ## [3,] 0.0000000 0.0000000 1 house3 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house3[1,], house3[2,], col = &quot;green&quot;, border = &quot;black&quot;) 20.3 Your Turn 20.3.1 Translation and then Rotation Let’s reverse the order of these matrices in the previous example and see that we get a different transformation. Remember, order matters: matrix multiplication is not commutative. So try changing the problem to first translating and then rotating. But first, in your group, try guessing where the house will go and then do it in R. Does the house end up where you expected it to be? 20.3.2 House of Orange Here is a picture of a gray house and a larger, upside-down orange house. Work as a group to reproduce this image using homogeneous coordinates. You will have to use a combination of translation, rotation, and expansion. You will do this by multiplying three matrices. Think carefully and experiment. Remember that the order of your matrices matters, and the rightmost one happens first. ############################# # your code defining the 3x3 matrices A1 and A2 A1 = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A2 = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A3 = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A = A3 %*% A2 %*% A1 ############################# # you do not need to change this code plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) house2 = A %*% house abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house2[1,], house2[2,], col = &quot;orange&quot;, border = &quot;green&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) 20.3.3 House Party This problem is on PS5. Here is a plot of the grey house and four other houses, colored cyan, red, gold, and purple Reproduce this image using homogeneous coordinates. ############# # your code for 3x3 matrices that create the transformed houses goes here A.red = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.purple = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.gold = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.cyan = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) #################### # you do not need to change this code house = cbind(c(0,0,1), c(0,3/4,1), c(2/4,3/4,1), c(2/4,0,1), c(4/4,0,1), c(4/4,4/4,1), c(5/4,4/4,1), c(0,8/4,1), c(-5/4,4/4,1), c(-4/4,4/4,1), c(-4/4,0,1), c(0,0,1)); plot(house[1,], house[2,], type = &quot;n&quot;, xlim=c(-2.5,2.5),ylim=c(-2.0,3.0),,xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-4:4, v=-4:4, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house.gold = A.gold %*% house polygon(house.gold[1,], house.gold[2,], col = &quot;gold&quot;, border = &quot;blue&quot;) house.cyan = A.cyan %*% house polygon(house.cyan[1,], house.cyan[2,], col = &quot;cyan&quot;, border = &quot;blue&quot;) house.red = A.red %*% house polygon(house.red[1,], house.red[2,], col = &quot;red&quot;, border = &quot;blue&quot;) house.purple= A.purple %*% house polygon(house.purple[1,], house.purple[2,], col = &quot;purple&quot;, border = &quot;blue&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) "],["eigenvectors.html", "Section 21 Eigenvectors 21.1 Computing Eigenvectors and Eigenvalues 21.2 Diagonalization 21.3 Rental Car Example 21.4 Northern Spotted Owl", " Section 21 Eigenvectors Download this Rmd file 21.1 Computing Eigenvectors and Eigenvalues To compute eigenvalues and eigenvectors in R we use the eigen command. For example if our matrix is (A = cbind(c(-14,-20,-23),c(13,19,19),c(-2,-2,1))) ## [,1] [,2] [,3] ## [1,] -14 13 -2 ## [2,] -20 19 -2 ## [3,] -23 19 1 Then we compute its eigenvalues and eigenvectors as eigen(A) ## eigen() decomposition ## $values ## [1] 6 3 -3 ## ## $vectors ## [,1] [,2] [,3] ## [1,] -0.2672612 -0.4082483 0.5773503 ## [2,] -0.5345225 -0.4082483 0.5773503 ## [3,] -0.8017837 0.8164966 0.5773503 One thing to notice about the eigenvectors is that they are scaled to have length one (they are unit vectors). So they often do not look like what we expect. Note for example that the first vector above is a multiple of \\((1,2,3)^T\\), the second is a multiple of \\((-1,-1,2)^T\\) and the third is a multiple of \\((1,1,1)^T\\). We can extract the eigenvectors and eigenvalues as follows vals = eigen(A)$values vecs = eigen(A)$vectors Then, for example, we can see if a vector is an eigenvector as follows. Here I will check the first eigenvalue and first eigenvector: lambda1 = vals[1] v1 = vecs[,1] A %*% v1 ## [,1] ## [1,] -1.603567 ## [2,] -3.207135 ## [3,] -4.810702 lambda1 * v1 ## [1] -1.603567 -3.207135 -4.810702 From this, we see that \\(A v_1 = \\lambda_1 v_1\\). Recall that every scalar multiple of an eigenvector is also an eigenvector of that same eigenvalue. The vectors are currently scaled to have length 1. Another useful scaling is to have them sum to 1. You can accomplish this by dividing them by the sum of their entries. For example, v1 = v1/sum(v1) v1 ## [1] 0.1666667 0.3333333 0.5000000 21.2 Diagonalization In class we diagonalized a few matrices. Here we show how to do this in R. Here is the first matrix from the checkpoint question CP-5.3. (A = cbind(c(-5,-6,-7),c(6,7,8),c(-2,-2,-2))) ## [,1] [,2] [,3] ## [1,] -5 6 -2 ## [2,] -6 7 -2 ## [3,] -7 8 -2 vals = eigen(A)$values vals ## [1] 1.000000e+00 -1.000000e+00 -3.766534e-15 vecs = eigen(A)$vectors vecs ## [,1] [,2] [,3] ## [1,] -0.2672612 0.5773503 -0.6666667 ## [2,] -0.5345225 0.5773503 -0.6666667 ## [3,] -0.8017837 0.5773503 -0.3333333 solve(vecs) %*% A %*% vecs ## [,1] [,2] [,3] ## [1,] 1.000000e+00 3.355686e-15 -6.554309e-16 ## [2,] 6.661338e-15 -1.000000e+00 8.881784e-15 ## [3,] 4.272762e-15 -3.076740e-15 1.776357e-15 Here, we are diagonalizing \\(A\\) by multiplying \\(P^{-1} A P = D\\) where \\(P\\) is the matrix of eigenvectors and \\(D\\) is the diagonal matrix of eigenvalues. We can use zapsmall to round or “zap” very small numbers to 0, and it then looks more like what we are expecting. zapsmall(solve(vecs) %*% A %*% vecs) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 -1 0 ## [3,] 0 0 0 Now we diagonalize the second matrix from CP-5.3 You will recall that this one has a repeated eigenvalue (algebraic multiplicity 2), but it has a 2-dimensional eigenspace (geometric multiplicity 2), so it is digonalizable. B = cbind(c(3,-1,2),c(-1,3,2),c(2,2,0)) eigen(B) ## eigen() decomposition ## $values ## [1] 4 4 -2 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.9128709 0.0000000 -0.4082483 ## [2,] -0.1825742 0.8944272 -0.4082483 ## [3,] 0.3651484 0.4472136 0.8164966 vals = eigen(B)$values vecs = eigen(B)$vectors zapsmall(solve(vecs) %*% B %*% vecs) ## [,1] [,2] [,3] ## [1,] 4 0 0 ## [2,] 0 4 0 ## [3,] 0 0 -2 The third matrix from CP-5.3 is not diagonalizable. It has an eigenvalue of algebraic multiplicity 2 and geometric multiplicity 1. Note that it gives the same two eigenvectors for \\(v_2\\) and \\(v_3\\), because the eigenspace \\(E_2\\) is only 1 dimensional. C = cbind(c(3,-1,1),c(2,2,1),c(1,1,2)) eigen(C) ## eigen() decomposition ## $values ## [1] 3 2 2 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.8017837 0.5773503 0.5773503 ## [2,] -0.2672612 -0.5773503 -0.5773503 ## [3,] 0.5345225 0.5773503 0.5773503 21.3 Rental Car Example In problem PS 3.8, you studied a transition matrix for where cars got returned if they are rented from one of three rental locations: St. Paul, Rochester, and Duluth. Note that the columns of this matrix are probablilities, and as such, the are nonnegative and sum to 1. Such a matrix is called a Markov or stochastic matrix. StP = c(.85,.09,.06) Roch = c(.30,.60,.10) Dul = c(.35,.05,.60) M = cbind(StP,Roch,Dul) rownames(M) &lt;- c(&quot;StP&quot;,&quot;Roch&quot;,&quot;Dul&quot;) M ## StP Roch Dul ## StP 0.85 0.3 0.35 ## Roch 0.09 0.6 0.05 ## Dul 0.06 0.1 0.60 In this assignment you imagined that there were 20 cars at each location, i.e., v = cbind(20,20,20) and you applied the matrix over and over again to this vector watching it converge to a steady state. v = c(20,20,20) for (i in 1: 100) { v = M %*% v } v ## [,1] ## StP 40.969163 ## Roch 10.308370 ## Dul 8.722467 Stochastic matrices always have eigenvalue \\(\\lambda = 1\\). As can be seen here: eigen(M) ## eigen() decomposition ## $values ## [1] 1.000+0.000000i 0.525+0.037081i 0.525-0.037081i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.9497414+0i 0.6201737+0.0000000i 0.6201737+0.0000000i ## [2,] 0.2389672+0i -0.3100868-0.4599331i -0.3100868+0.4599331i ## [3,] 0.2022030+0i -0.3100868+0.4599331i -0.3100868-0.4599331i Note that the first eigenvalue is 1, and that the second and third eigenvalues (and eigenvectors) are complex and have both real and imaginary parts. If some of the eigenvalues have imaginary parts, then it outputs them all in complex form. Here we extract just the real part of the first eigenvector (since its imaginary part is 0), and we rescale it both to sum to 1 and to sum to 60. v = Re(eigen(M)$vectors[,1]) (v = v /sum(v)) ## [1] 0.6828194 0.1718062 0.1453744 60*v ## [1] 40.969163 10.308370 8.722467 Notice that this is the exact same as the steady-state vector that we got by iterating. The steady-state that this system wants to be in — with 40.97 cars in St. Paul, 10.31 cars in Rochester, and 8.72 cars in Duluth. It makes sense that a steady-state vector is an eigenvector with eigenvalue \\(\\lambda = 1\\). That the system converges to this state is eigen-magic that we will learn about soon. 21.4 Northern Spotted Owl This is the opening example in Chapter 5 of the textbook on page 265. It comes from a 1992 study of the northern spotted owl, which was threatened with extinction due to the loss of forest habitat due to logging in the Pacific Northwest. This is currently a story featured in an NPR Podcast called Timber Wars. 21.4.1 The Dynamical System The vector \\[ x_n = \\begin{bmatrix} j_n \\\\ s_n \\\\ a_n \\end{bmatrix} \\] is an age-stage vector in which \\(j_n, s_n\\), and \\(a_n\\) are the number of female owls in the juvenile (up to 1 year), subadult (1-2 year), and adult (over 2 year) age groups in year \\(n\\). The dynamics that take us from one year to the next is given by, the recursive relation \\(x_{n+1} = A x+n\\), where \\(A\\) is the matrix shown here. This is an age-stage matrix model that was published in Conservation Biology. \\[ \\begin{bmatrix} j_{n+1} \\\\ s_{n+1} \\\\ a_{n+1} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 0.33 \\\\ 0.18 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.71 &amp; 0.94 \\end{bmatrix} \\begin{bmatrix} j_n \\\\ s_n \\\\ a_n \\end{bmatrix} \\] If we multiply this system out, we get \\[ \\begin{array} {rcl} j_{n+1} &amp;=&amp; 0.33 a_n \\\\ s_{n+1} &amp;=&amp; 0.18 j_n \\\\ a_{n+1} &amp;=&amp; 0.71 s_n + 0.94 a_n \\end{array} \\] We see that, in this model, 0.33 represents the fertility or fecundity rate. That is, it is the proportion of new juveniles next year to adults this year (the proportion of offspring the adult population is producing). The 0.18 is the survival rate from juvenile to subadult, 0.71 is the survival rate from subadult to adult, and 0.94 proportion of adults that survive from one year to the next. To see the dynamics play out over time, we will start with an original population of owls is distributed into age groups as follows. \\[ x_0 = \\begin{bmatrix} 100 \\\\ 76 \\\\ 502 \\end{bmatrix} \\] We will write a loop to apply the matrix \\(A\\) over and over again. This time we will make a table and store each value in the table. A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix x0 = c(100,76,502) # the inital value N = 10 # iterate N=10 times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } X # display the table ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 100 165.66 173.5272 167.3330 164.27953 161.74110 159.0937 ## [2,] 76 18.00 29.8188 31.2349 30.11993 29.57032 29.1134 ## [3,] 502 525.84 507.0696 497.8168 490.12454 482.10222 474.1710 ## [,8] [,9] [,10] [,11] ## [1,] 156.47643 153.90912 151.38419 148.90038 ## [2,] 28.63687 28.16576 27.70364 27.24915 ## [3,] 466.39127 458.73997 451.21326 443.81005 Having saved the information, we can now plot the data, Note that it appears to support the claim the claim that the owls are threatened with extinction. tot = X[1,] + X[2,] + X[3,] t = seq(1,N+1) plot(t,X[1,],type=&#39;l&#39;,col=&#39;blue&#39;,ylim=c(0,1000),ylab=&quot;population&quot;,xlab=&quot;year&quot;,main=&quot;Spotted Owl Population&quot;) points(t,X[1,],col=&#39;blue&#39;,pch=20,cex=.8) lines(t,X[2,],col=&#39;orange&#39;) points(t,X[2,],col=&#39;orange&#39;,pch=20,cex=.8) lines(t,X[3,],col=&#39;red&#39;) points(t,X[3,],col=&#39;red&#39;,pch=20,cex=.8) points(t,tot,col=&#39;black&#39;,pch=20,cex=.8) lines(t,tot,col=&#39;black&#39;) legend(8, 1050, legend=c(&quot;juvenile&quot;, &quot;subadults&quot;, &quot;adults&quot;,&quot;total&quot;), col=c(&#39;blue&#39;,&#39;orange&#39;,&#39;red&#39;,&#39;black&#39;), lty=1) Let’s run the iteration further. This time, we won’t display the table (gets too big), and we will just show the plot of 100 iterations A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix x0 = c(100,76,502) # the inital value N = 100 # iterate N=10 times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } tot = X[1,] + X[2,] + X[3,] t = seq(1,N+1) plot(t,X[1,],type=&#39;l&#39;,col=&#39;blue&#39;,ylim=c(0,1000),ylab=&quot;population&quot;,xlab=&quot;year&quot;,main=&quot;Spotted Owl Population&quot;) points(t,X[1,],col=&#39;blue&#39;,pch=20,cex=.8) lines(t,X[2,],col=&#39;orange&#39;) points(t,X[2,],col=&#39;orange&#39;,pch=20,cex=.8) lines(t,X[3,],col=&#39;red&#39;) points(t,X[3,],col=&#39;red&#39;,pch=20,cex=.8) points(t,tot,col=&#39;black&#39;,pch=20,cex=.8) lines(t,tot,col=&#39;black&#39;) legend(8, 1050, legend=c(&quot;juvenile&quot;, &quot;subadults&quot;, &quot;adults&quot;,&quot;total&quot;), col=c(&#39;blue&#39;,&#39;orange&#39;,&#39;red&#39;,&#39;black&#39;), lty=1) They do seem to be dying out. 21.4.2 Eigenanalysis Now we check the eigenvectors and eigenvalues to see if they help us understand what is going on. eigen(A) ## eigen() decomposition ## $values ## [1] 0.9835927+0.0000000i -0.0217964+0.2059185i ## [3] -0.0217964-0.2059185i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.31754239+0i 0.6820937+0.0000000i 0.6820937+0.0000000i ## [2,] 0.05811107+0i -0.0624124-0.5896338i -0.0624124+0.5896338i ## [3,] 0.94646180+0i -0.0450520+0.4256233i -0.0450520-0.4256233i The first eigenvalue is \\(\\lambda_1 = 0.98\\), and the other two are complex. R always lists the eigenvalues from largest to smallest, so in this case the largets eigenvalue is less than one. That means that in that direction, the population is dying off by 2% each year. If we extract the corresponding eigenvector, and scale it to sum to 1, we get v1 = eigen(A)$vectors[,1] # get the first eigenvector v1 = Re(v1) # drop the imaginary part v1/sum(v1) # scale it to sum to 1 ## [1] 0.24017754 0.04395311 0.71586935 What this is telling us that as the population dies off, it does so in this eigenvector direction with 24.0% of the population being juveniles, 4.4% subadults, and 71.5% adults. The owls were going extinct because of the logging in the Pacific Northwest. Suppose that we make the case that by stopping logging we will increase the survival rate from juvenile to subadult from 0.18 to 0.26 (by improving the habititat the juvinile owls have a better chance of surviving the first year). In this case, the eigenvalues and eigenvectors becomes: A = cbind(c(0,0.26,0),c(0,0,.71),c(0.33,0,0.94)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.0008184+0.0000000i -0.0304092+0.2448335i ## [3] -0.0304092-0.2448335i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.3121152+0i -0.0754384+0.6073766i -0.0754384-0.6073766i ## [2,] 0.0810836+0i 0.6450012+0.0000000i 0.6450012+0.0000000i ## [3,] 0.9465778+0i -0.4436732-0.1119384i -0.4436732+0.1119384i Notice that the largest eigenvalue now becomes 1. And if we iterate, we see that the population does not die off (it even grows slightly). Finally, we go back to the original system, which is dying out, but start with a totally different age distribution. You can see the non-dominant eigevectors dying out quickly at the beginning and the dominant eigenvector, of eigenvalue 0.98, taking over. A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix x0 = c(70,600,8) # the inital value N = 20 # iterate N=100 times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } tot = X[1,] + X[2,] + X[3,] t = seq(1,N+1) plot(t,X[1,],type=&#39;l&#39;,col=&#39;blue&#39;,ylim=c(0,1000),ylab=&quot;population&quot;,xlab=&quot;year&quot;,main=&quot;Spotted Owl Population&quot;) points(t,X[1,],col=&#39;blue&#39;,pch=20,cex=.8) lines(t,X[2,],col=&#39;orange&#39;) points(t,X[2,],col=&#39;orange&#39;,pch=20,cex=.8) lines(t,X[3,],col=&#39;red&#39;) points(t,X[3,],col=&#39;red&#39;,pch=20,cex=.8) points(t,tot,col=&#39;black&#39;,pch=20,cex=.8) lines(t,tot,col=&#39;black&#39;) legend(8, 1050, legend=c(&quot;juvenile&quot;, &quot;subadults&quot;, &quot;adults&quot;,&quot;total&quot;), col=c(&#39;blue&#39;,&#39;orange&#39;,&#39;red&#39;,&#39;black&#39;), lty=1) s "],["dynamical-systems-in-2d.html", "Section 22 Dynamical Systems in 2D 22.1 Helper Function to Plot Dynamical Systems 22.2 Our first example 22.3 CheckPoint Question for today 22.4 Discussion Question 1 22.5 Discussion Question 2 22.6 Discussion Question 3 22.7 Discussion Question 4 22.8 Your Turn", " Section 22 Dynamical Systems in 2D Download this Rmd file Let \\(A\\) be a square \\(n \\times n\\) matrix and let \\(\\mathsf{x}_0 \\in \\mathbb{R}^n\\). A dynamical system is a sequence of vectors \\(\\mathsf{x}_0,\\mathsf{x}_1,\\mathsf{x}_2, \\ldots, \\mathsf{x}_t, \\ldots\\) where \\[ \\mathsf{x}_{t} = A \\mathsf{x}_{t-1} = A^t \\mathsf{x}_0 \\quad \\mbox{for} \\quad t \\geq 1. \\] The sequence \\(\\mathsf{x}_0,\\mathsf{x}_1,\\mathsf{x}_2, \\ldots, \\mathsf{x}_t, \\ldots\\) is called the trajectory for initial vector \\(\\mathsf{x}_0\\). A dynamical system evolves over time. The long-term behavior is governed by the eigenvalues of matrix \\(A\\). We will look at visualizations of some \\(2 \\times 2\\) dynamical systems to develop some intuition about eigensystems. 22.1 Helper Function to Plot Dynamical Systems Here is some special code, written by Professor Beveridge, that makes helpful plots. You need to execute this code chunk before the others. get_traj &lt;- function(mat, x0, num) { traj = cbind(x0) num for (i in 1:num) { traj = cbind(traj, mat %*% traj[,dim(traj)[2]]) traj } return(traj) } plot_traj &lt;- function(mat, x0, num) { traj = get_traj(mat,x0,num) points(traj[1,],traj[2,], pch=20, col=rainbow(length(traj))) } trajectory_plot &lt;- function(mat, t=20, datamax=5, plotmax=10, numpoints=10, showEigenspaces=TRUE) { # initialize plot par(pty = &quot;s&quot;) plot(c(0),c(0),type=&quot;n&quot;, xlim=c(-plotmax,plotmax),ylim=c(-plotmax,plotmax), xlab=&#39;x&#39;, ylab=&#39;y&#39;) abline(h=-plotmax:plotmax, v=-plotmax:plotmax, col=&quot;gray&quot;) mygrid &lt;- expand.grid(x=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1), y=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1)) for (i in 1:dim(mygrid)[1]) { plot_traj(mat,c(mygrid[i,1],mygrid[i,2]),t) } if (showEigenspaces) { eigen = eigen(mat) #mylabel = cat(&#39;lambda=&#39;, eigen$values[1], &#39;and lambda=&#39;, eigen$values[2]) #title(xlab=mylabel) v1 = zapsmall(eigen$vectors[,1]) v2 = zapsmall(eigen$vectors[,2]) if (! class(v1[1]) == &quot;complex&quot;) { if (v1[1] == 0) { abline(v=0) } else { abline(a=0,b=v1[2]/v1[1], col=&quot;blue&quot;) } if (v2[1] == 0) { abline(v=0) } else { abline(a=0,b=v2[2]/v2[1], col=&quot;blue&quot;) } } } } 22.2 Our first example Let’s start by looking at the example from the video \\[ A = \\frac{1}{30} \\begin{bmatrix} 31 &amp; 4 \\\\ 2 &amp; 29 \\end{bmatrix}. \\] We get the most complete picture when we plot multiple trajectories at once. So we use the helper function trajectory_plot to plot the trajectories of a grid of points. It also plots the eigenspaces for the matrix. You can specify the matrix A the number of iterations the size of the square where the initial points lie the size of the plot the number of points along the side of the grid A = 1/30 * cbind(c(31,2),c(4,29)) trajectory_plot(A, t=30, datamax=5, plotmax=15, numpoints=5) This system is best understood by comparing what we see with the eigenvector and eigenvalues. eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] 0.8944272 -0.7071068 ## [2,] 0.4472136 0.7071068 We can see that we have slight expansion along \\([ 2, 1]^{\\top}\\) and slight contraction along \\([-1,1]\\). The long term behavior is an expansion in the direction of \\([2, 1]^{\\top}\\). 22.3 CheckPoint Question for today Here is the checkpoint question for today (for which you found a closed-form solution). A = 1/110 * cbind(c(97,-8),c(6,123)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] -0.2425356 -0.9486833 ## [2,] -0.9701425 -0.3162278 And here is the corresponding plot of the dynamical system: A = 1/110 * cbind(c(97,-8),c(6,123)) trajectory_plot(A, t=10, datamax=15, plotmax=25, numpoints=5) 22.4 Discussion Question 1 In class, we looked at a matrix with eigenvalues 1 and 1/2, and we plotted a trajectory starting at \\((8,7)\\) by hand. Its eigensystem is shown here: A = rbind(c(0.4, 0.4), c(-0.15, 1.1)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.0 0.5 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 And here is a trajectory plot trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5) 22.5 Discussion Question 2 Here the matrix has the same eigenvectors, but now the eigenvalues are 1.0 and 0.9. It’s a little easier to see when the smaller eigenvalue converges more slowly. A = rbind(c(0.88, 0.08), c(-0.03, 1.02)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.0 0.9 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5) 22.6 Discussion Question 3 Here the matrix has the same eigenvectors, but now the eigenvalues are 1.1 and 0.9. A = rbind(c(0.86, 0.16), c(-0.06, 1.14)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5) 22.7 Discussion Question 4 Finally, here again the matrix has the same eigenvectors, but now the eigenvalues are 0.99 and 0.9. A = rbind(c(0.882, 0.072), c(-0.027, 1.008)) eigen(A) ## eigen() decomposition ## $values ## [1] 0.99 0.90 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 trajectory_plot(A, t=50, datamax=10, plotmax=15, numpoints=5) 22.8 Your Turn Now it’s your turn to explore some dynamical systems. Create trajectory plots for each of these dynamical systems. Characterize the long-term behavior. What direction to vectors converge to? Do magnitudes increase? decrease? stabilize? Calculate the eigenvectors and eigenvalues and compare them to your plot. The eigensystem should tell the same story as your plot. If your original plot is confusing, try changing the parameters (initial square size, plot size, number of grid points). Here is some code for you to adapt for the examples. A = cbind(c(1,0),c(0,1)) trajectory_plot(A, t=30, datamax=5, plotmax=10, numpoints=10) eigen(A) ## eigen() decomposition ## $values ## [1] 1 1 ## ## $vectors ## [,1] [,2] ## [1,] 0 -1 ## [2,] 1 0 22.8.1 Example 1 \\[ A = \\frac{1}{60} \\begin{bmatrix} 55&amp; -8 \\\\ -1 &amp; 53 \\end{bmatrix} \\] 22.8.2 Example 2 \\[ A = \\frac{1}{20} \\begin{bmatrix} 24&amp; -6 \\\\ 1 &amp; 19 \\end{bmatrix} \\] 22.8.3 Example 3 \\[ A = \\frac{1}{110} \\begin{bmatrix} 106&amp; 12 \\\\ 6 &amp; 92 \\end{bmatrix} \\] 22.8.4 Example 4 \\[ A = \\frac{1}{16} \\begin{bmatrix} 17&amp; -15 \\\\ 15 &amp; 17 \\end{bmatrix} \\] "],["complex-eigenvalues.html", "Section 23 Complex Eigenvalues 23.1 Rotation-Dilation 23.2 General 2x2 Matrices with Complex Eigenvalues 23.3 A 3D example", " Section 23 Complex Eigenvalues Download this Rmd file Now we will explore what happens if the matrix has complex eigenvalues. 23.1 Rotation-Dilation First we explore a special case of 2x2 matrices with complex eigenvalues of the following form: \\[ R=\\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\\\ \\end{bmatrix} \\] As we see in the image below, this matrix rotates by angle of \\(\\theta\\) and expands (dilates) or contracts by a factor of \\(r\\) where \\[ \\begin{align} \\theta &amp;= \\arctan(b/a) \\\\ r &amp;= \\sqrt{a^2 + b^2} \\end{align} \\] Furthermore the eigenvalues of this matrix are the complex values \\[ \\lambda_1 = a + b i \\qquad \\lambda_2 = a - b i \\] where \\(i = \\sqrt{-1}\\). These eigenvalues are conjugate pairs and are often written as \\(\\lambda = a \\pm b i\\). They come from applying the quadratic formula to the characteristic polynomial and getting a negative discriminant under the square root. It is important to note that both the angle of rotation \\(\\theta\\) and the dilation factor \\(r\\) are contained in the eigenvalues. The fact that these are the eigenvalues is derived in the video. We will illustrate it here in three examples. 23.1.1 Example 1 Our first example has \\(a = .9\\) and \\(b = .2\\). ## [,1] [,2] ## [1,] 0.9 -0.2 ## [2,] 0.2 0.9 We look at its eigenvectors and eigenvalues and see that \\(\\lambda = .9 \\pm .2 i\\): eigen(A) ## eigen() decomposition ## $values ## [1] 0.9+0.2i 0.9-0.2i ## ## $vectors ## [,1] [,2] ## [1,] 0.7071068+0.0000000i 0.7071068+0.0000000i ## [2,] 0.0000000-0.7071068i 0.0000000+0.7071068i Notics that the eigenvectors also come in conjugate pairs, with a real and a complex part. This always happens. \\[ \\vec{\\mathsf{v}} = \\begin{bmatrix}0.707 \\\\ 0.000 \\end{bmatrix} \\pm \\begin{bmatrix} 0.000 \\\\ .707 \\end{bmatrix} i \\] Now, let’s find the angle of rotation. We will use the Arg command which finds the angle (in radians) of a complex number. We also convert it to degrees here. vals = eigen(A)$values v1 = vals[1] Arg(v1) # gives the argument, or angle, of a complex number (in radians) ## [1] 0.2186689 Arg(v1) / (2*pi) * 360 # convert to degrees ## [1] 12.52881 For good measure, we can compare with using the arctan function. atan(.2/.9) ## [1] 0.2186689 Next we find the dilation/contraction factor. We can do so using the Mod command, which finds the “modulus” or absolute value or length of a complex number. Mod(v1) # gives the length of a complex number ## [1] 0.9219544 And, again for good measure, we compare with using the Pythagorean theorem: sqrt(.9^2 + .2^2) ## [1] 0.9219544 Now, we observe the trajectory of a single point \\((0,1)^T\\) under this matrix. In this picture you can see that it is contracting and rotating by 12.5 degrees. Note that 360/12.5 is about 29, and it takes 29 applications to go once around the circle. You can count them in the plot below. Furthermore, \\((0.9219544)^29 =0.095\\) and after 29 applications the vector is about 1/10 of its original length. We can also view this by looking at a plot of the x and y coordinates over time as the point (x,y) circles around in the xy-plane. Key point: complex eigenvalues lead to oscillating values of the individual coordinates We can also use trajectory_plot from Dynamical Systems in 2D to watch what happens to a whole grid of points under this transformation. It is beautiful! trajectory_plot(A, t=30, datamax=5, plotmax=5, numpoints=10) 23.1.2 Example 2 Here is a second example of a rotation-dilation matrix, this time with \\(a = .96\\) and \\(b = .28\\). (A = cbind(c(.96,.28),c(-.28,.96))) ## [,1] [,2] ## [1,] 0.96 -0.28 ## [2,] 0.28 0.96 eigen(A) ## eigen() decomposition ## $values ## [1] 0.96+0.28i 0.96-0.28i ## ## $vectors ## [,1] [,2] ## [1,] 0.7071068+0.0000000i 0.7071068+0.0000000i ## [2,] 0.0000000-0.7071068i 0.0000000+0.7071068i We check the angle of rotation and the dilation factor vals = eigen(A)$values v1 = vals[1] Arg(v1) # gives the argument, or angle, of a complex number (in radians) ## [1] 0.2837941 Arg(v1) / (2*pi) * 360 # convert to degrees ## [1] 16.2602 Mod(v1) # gives the length of a complex number ## [1] 1 Notice that the dilation factor is 1, which is seen in the following plots. Here are 21 iterations: And here are 200 iterations And a trajectory plot: 23.1.3 Example 3 A third and final example. (A = cbind(c(.99,.16),c(-.16,.99))) ## [,1] [,2] ## [1,] 0.99 -0.16 ## [2,] 0.16 0.99 eigen(A) ## eigen() decomposition ## $values ## [1] 0.99+0.16i 0.99-0.16i ## ## $vectors ## [,1] [,2] ## [1,] 0.0000000-0.7071068i 0.0000000+0.7071068i ## [2,] -0.7071068+0.0000000i -0.7071068+0.0000000i vals = eigen(A)$values v1 = vals[1] Arg(v1) / (2*pi) * 360 # convert to degrees ## [1] 9.180542 Mod(v1) # gives the length of a complex number ## [1] 1.002846 We see that the dilation factor is \\(r = 1.0028\\) and the angle of rotation is \\(9.18\\) degrees. Here are 100 iterations. And a trajectory plot: 23.2 General 2x2 Matrices with Complex Eigenvalues Now suppose we have a 2x2 matrix with complex eigenvalues \\(\\lambda = a \\pm b i\\) and complex eigenvectors \\(\\mathsf{v} = \\mathsf{u } \\pm \\mathsf{w} i\\) that is not in rotation-dilation form. Here is an example: \\[ A = \\begin{bmatrix} 1.19 &amp; -0.38 \\\\ 0.29 &amp; 0.78 \\end{bmatrix} \\] It has eigenvalues and eigenvectors \\[ \\lambda = 0.985 \\pm 0.261 i \\qquad \\mathsf{v} = \\begin{bmatrix} 0.753 \\\\ 0.406 \\end{bmatrix} \\pm \\begin{bmatrix} 0.000 \\\\ -0.517 \\end{bmatrix} i \\] as seen here: ## [,1] [,2] ## [1,] 1.19 -0.38 ## [2,] 0.29 0.78 ## eigen() decomposition ## $values ## [1] 0.985+0.2611034i 0.985-0.2611034i ## ## $vectors ## [,1] [,2] ## [1,] 0.7531030+0.0000000i 0.7531030+0.0000000i ## [2,] 0.4062793-0.5174679i 0.4062793+0.5174679i The angle of rotation and factor of dilation are \\(\\theta = 14.8\\) degrees and \\(r = 1.019\\) as we see from these computations: ## [1] 14.84649 ## [1] 1.019019 A trajectory plot shows us that it is still rotating by 14.8 degrees and dilating by 1.019, but it is taking more of an elliptical pattern. To see precisely what happens, we change to basis \\(\\{\\mathsf{w}, \\mathsf{u}\\}\\) where \\(\\mathsf{w}\\) and \\(\\mathsf{u}\\) are the imaginary and real parts of the eigenvector \\(\\mathsf{v} =\\mathsf{u} + \\mathsf{w} i\\). In this case the eigenvalues and eigenvectors are \\[ \\lambda = 0.985 \\pm 0.261 i \\qquad \\mathsf{v} = \\begin{bmatrix} 0.753 \\\\ 0.406 \\end{bmatrix} \\pm \\begin{bmatrix} 0.000 \\\\ -0.517 \\end{bmatrix} i \\] So if we make the change of basis matrix \\(P = [\\mathsf{u},\\mathsf{w}]\\) \\[ P = \\begin{bmatrix} 0.000 &amp; 0.753 \\\\ -0.517 &amp; 0.406 \\end{bmatrix} \\] then we can factor \\(A\\) as \\[ A = \\begin{bmatrix} 1.19 &amp; -0.38 \\\\ 0.29 &amp; 0.78 \\end{bmatrix} = \\underbrace{\\begin{bmatrix}0.000 &amp; 0.753 \\\\-0.517 &amp; 0.406 \\end{bmatrix}}_P \\underbrace{\\begin{bmatrix} 0.985 &amp; - 0.261 \\\\ 0.261 &amp; 0.985 \\end{bmatrix}}_R \\underbrace{\\begin{bmatrix}0.000 &amp; 0.753 \\\\-0.517 &amp; 0.406 \\end{bmatrix}^{-1}}_{P^{-1}} \\] We have not diagonalized \\(A\\). Rather we have rotation-dilationalized (made up term) the matrix \\(A\\). At is core \\(A\\) is a rotation-dilation matrix whose angle and dilation factor come from the eigenvalue. The matrix \\(P\\) is a change of basis matrix. It is rotating and dilating in this new coordinate system, which are the vectors in the plot above. If we multiply the other way, we get \\[ P^{-1} A P = \\begin{bmatrix} 0.985 &amp; - 0.261 \\\\ 0.261 &amp; 0.985 \\end{bmatrix} = R \\] Which we can see using R ## [,1] [,2] ## [1,] 0.0000000 0.7531030 ## [2,] -0.5174679 0.4062793 ## [,1] [,2] ## [1,] 0.9850000 -0.2611034 ## [2,] 0.2611034 0.9850000 23.3 A 3D example Here is the Northern Spotted Owl matrices from the [Eigenvalues] examples. We saw that it has one real and two complex eigenvalues: ## eigen() decomposition ## $values ## [1] 0.9835927+0.0000000i -0.0217964+0.2059185i ## [3] -0.0217964-0.2059185i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.31754239+0i 0.6820937+0.0000000i 0.6820937+0.0000000i ## [2,] 0.05811107+0i -0.0624124-0.5896338i -0.0624124+0.5896338i ## [3,] 0.94646180+0i -0.0450520+0.4256233i -0.0450520-0.4256233i The eigenvalues are always listed in descending order by magnitude as we see here when we compute their modulus and arguments ## [1] 0.9835927 0.2070688 0.2070688 ## [1] 0.00000 96.04223 -96.04223 We can diagonalize this over the complex numbers. ## [,1] [,2] [,3] ## [1,] 0.9835927+0i 0.0000000+0.0000000i 0.0000000+0.0000000i ## [2,] 0.0000000+0i -0.0217964+0.2059185i 0.0000000+0.0000000i ## [3,] 0.0000000+0i 0.0000000+0.0000000i -0.0217964-0.2059185i It has complex eigenvalues on the diagonal and requires using the complex vectors in our basis. Often, this is not what we want to do if our matrix has real entries and comes from a real-valued problem. Instead, let’s rotation-dilationalize this matrix. The block diagonalization consists of a 1x1 block of the real eigenvalue and a 2x2 block of the rotation-dilation part of the complex eigenvalues. ## [,1] [,2] [,3] ## [1,] 0.31754239 0.0000000 0.68209367 ## [2,] 0.05811107 -0.5896338 -0.06241245 ## [3,] 0.94646180 0.4256233 -0.04505202 ## [,1] [,2] [,3] ## [1,] 0.9835927 0.0000000 0.0000000 ## [2,] 0.0000000 -0.0217964 -0.2059185 ## [3,] 0.0000000 0.2059185 -0.0217964 In 3 dimensions, there the geometry is as follows: * there is one basis vector, corresponding to the real eigenvalue \\(\\lambda = 0.985\\) * there is a plane in which the system rotates by \\(\\theta = 96\\) degrees and contracts by \\(r = 0.207\\). * thus, the complex part dies off pretty quickly (\\(r = 0.207\\)), and it converges to the dominant real eigenvector, which is also dying off but more slowly (\\(r = 0.983\\)) We can see this in the traectory plot below. The oscillating part at the beginning comes from the complex eigenvalues as they quickly die off and the system converges to the dominant real eigenvector. "],["network-centralities.html", "Section 24 Network Centralities 24.1 Graphs and Networks 24.2 Degree Centrality 24.3 Gould’s Index 24.4 Your Turn: The Rise of Moscow", " Section 24 Network Centralities Download this Rmd file In this example, we will use a package called igraph. To install it, you need to go to the packages window (bottom right), choose install, and search for and install igraph from the packages window. library(igraph) The igraph R package isn’t all that well documented. Here are some places to look for documentation if you want to learn about other features. Let me know if you find any other good references: http://kateto.net/netscix2016 http://igraph.org/r/doc/aaa-igraph-package.html 24.1 Graphs and Networks Graphs consists of vertices and the edges between them. These edges are used to model connections in a wide array of applications, including but not limited to, physical, biological, social, and information networks. To emphasize the application to real-world systems, the term Network Science is sometimes used. So we will use the terms graph and network interchangeably. In this application, we will see that linear algebra is an important tool in the study of graphs. 24.1.1 Adjacency Matrices Matrices are used to represent graphs and networks in a very direct way: we place a 1 in position \\((i,j)\\) of the adjacency matrix \\(A\\) of the graph \\(G\\), if there is an edge from vertex \\(i\\) to vertex \\(j\\) in \\(G\\). Here is the adjacency matrix we will use today. A = rbind( c(0,1,0,1,0,0,0,0,1,0,0,0), c(1,0,1,1,1,0,1,0,0,0,0,0), c(0,1,0,0,1,0,0,0,0,0,0,0), c(1,1,0,0,0,1,0,1,0,0,0,0), c(0,1,1,0,0,0,1,1,0,0,0,1), c(0,0,0,1,0,0,1,0,0,0,0,0), c(0,1,0,0,1,1,0,1,0,0,0,0), c(0,0,0,1,1,0,1,0,0,1,1,0), c(1,0,0,0,0,0,0,0,0,0,0,0), c(0,0,0,0,0,0,0,1,0,0,0,0), c(0,0,0,0,0,0,0,1,0,0,0,0), c(0,0,0,0,1,0,0,0,0,0,0,0)) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] ## [1,] 0 1 0 1 0 0 0 0 1 0 0 ## [2,] 1 0 1 1 1 0 1 0 0 0 0 ## [3,] 0 1 0 0 1 0 0 0 0 0 0 ## [4,] 1 1 0 0 0 1 0 1 0 0 0 ## [5,] 0 1 1 0 0 0 1 1 0 0 0 ## [6,] 0 0 0 1 0 0 1 0 0 0 0 ## [7,] 0 1 0 0 1 1 0 1 0 0 0 ## [8,] 0 0 0 1 1 0 1 0 0 1 1 ## [9,] 1 0 0 0 0 0 0 0 0 0 0 ## [10,] 0 0 0 0 0 0 0 1 0 0 0 ## [11,] 0 0 0 0 0 0 0 1 0 0 0 ## [12,] 0 0 0 0 1 0 0 0 0 0 0 ## [,12] ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 1 ## [6,] 0 ## [7,] 0 ## [8,] 0 ## [9,] 0 ## [10,] 0 ## [11,] 0 ## [12,] 0 We make a graph from the adjacency matrix as follows: g=graph_from_adjacency_matrix(A,mode=&#39;undirected&#39;) plot(g, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) Observe that there is an edge from vertex \\(i\\) to vertex \\(j\\) if and only if there is a 1 in position \\((i,j)\\) in the matrix. This network is the route map of a small airline. We will add vertex labels and change the vertex size: airports = c(&quot;ATL&quot;,&quot;LAX&quot;,&quot;ORD&quot;,&quot;MSP&quot;,&quot;DEN&quot;,&quot;JFK&quot;,&quot;SFO&quot;,&quot;SEA&quot;,&quot;PHL&quot;,&quot;PDX&quot;,&quot;MDW&quot;,&quot;LGA&quot;) V(g)$label = airports plot(g,vertex.size=30, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 24.1.2 Graph Layouts There are a variety of graph layout algorithms which place the vertices in the plane. You can find many algorithms in the igraph documentation. For example, here is a layout on a circle coords = layout_in_circle(g) plot(g, layout=coords, vertex.size = 30,vertex.label.cex=0.85, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) The Fruchterman-Reingold algorithm is one of the most popular graph vertex layout algorithms. It is a force-directed layout that tries to get a nice-looking graph where edges are similar in length and cross each other as little as possible. The algorithm simulates the graph as a physical system. Vertices are electrically charged particles that repulse each other when they get too close. The edges act as springs that attract connected vertices closer together. As a result, vertices are evenly distributed through the chart area. The resulting layout is intuitive: vertices which share more connections are closer to each other. coords = layout_with_fr(g) plot(g, layout=coords, vertex.size = 30, vertex.label.cex=0.85, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) We can also choose to layout vertices by hand: locations = rbind( c(20,0),c(-10,0),c(11,7),c(10,15),c(3,12),c(25,10), c(-10,10),c(-12,15),c(20,6),c(-15,12),c(12,4),c(25,13) ) plot(g,vertex.size=20, layout=locations, vertex.label.cex=0.85, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 24.2 Degree Centrality If we are considering placing an office in one of our airport locations, we may want to chose the most central hub for that office. It turns out that there are many interesting centrality measures for networks. We will talk about two of them today. The simplest measure centrality is the degree of the vertex, or the number of edges connected to that vertex. We calculate the degree centralities from the adjacency matrix as follows: First make a vector \\(\\mathsf{v}\\) of all 1’s; then multiply \\(\\mathsf{p} = A\\mathsf{v}\\) to get the degree proportions; and divide the vector \\(\\mathsf{p}\\) by the sum of its entries. The result is a normalized vector \\(\\mathsf{p}\\) whose entries sum to 1. Each entry of vector \\(\\mathsf{p}\\) represents to proportion of edges incident with the corresponding vertex. v=rep(1,nrow(A)) # all 1s vector d = A %*% v # degrees p=d/sum(d) # proportion of degrees dp = cbind(d,p) # show d and p together side-by-side in a matrix rownames(dp) = airports colnames(dp) = c(&quot;degree&quot;,&quot;proportion&quot;) dp ## degree proportion ## ATL 3 0.08823529 ## LAX 5 0.14705882 ## ORD 2 0.05882353 ## MSP 4 0.11764706 ## DEN 5 0.14705882 ## JFK 2 0.05882353 ## SFO 4 0.11764706 ## SEA 5 0.14705882 ## PHL 1 0.02941176 ## PDX 1 0.02941176 ## MDW 1 0.02941176 ## LGA 1 0.02941176 We can also sort the vertices by degree. ii=order(d,decreasing=TRUE) # find the decreasing order of d dp2 = dp[ii,] # sort the data frame in that order dp2 ## degree proportion ## LAX 5 0.14705882 ## DEN 5 0.14705882 ## SEA 5 0.14705882 ## MSP 4 0.11764706 ## SFO 4 0.11764706 ## ATL 3 0.08823529 ## ORD 2 0.05882353 ## JFK 2 0.05882353 ## PHL 1 0.02941176 ## PDX 1 0.02941176 ## MDW 1 0.02941176 ## LGA 1 0.02941176 Now let’s create a data visualization. We plot the network and size each vertex according to the vector \\(p\\). The larger vertices have more edges connected to them. This conveys information to the viewer about the relative importance of the vertices. plot(g, layout=locations, vertex.size=250*p,vertex.label.cex=0.65, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 24.3 Gould’s Index Gould’s Index is a measure of centrality that uses the dominant eigenvector of a matrix. It was introduced by geographer P. R. Gould in 1967 to analyze the geographical features on maps. We will build up Gould’s Index step-by-step so that we can understand what it measures. 24.3.1 Step 1: Add Layovers The first step is typically to add the identity matrix to the adjancency matrix \\(A\\) to get a new matrix \\[ B = A + I. \\] The \\(n \\times n\\) identity matrix in R is obtained by using diag(n). Adding the identity gives a connection from a vertex to itself. This loop edge corresponds to staying at the current city during a layover. (B = A + diag(nrow(A))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] ## [1,] 1 1 0 1 0 0 0 0 1 0 0 ## [2,] 1 1 1 1 1 0 1 0 0 0 0 ## [3,] 0 1 1 0 1 0 0 0 0 0 0 ## [4,] 1 1 0 1 0 1 0 1 0 0 0 ## [5,] 0 1 1 0 1 0 1 1 0 0 0 ## [6,] 0 0 0 1 0 1 1 0 0 0 0 ## [7,] 0 1 0 0 1 1 1 1 0 0 0 ## [8,] 0 0 0 1 1 0 1 1 0 1 1 ## [9,] 1 0 0 0 0 0 0 0 1 0 0 ## [10,] 0 0 0 0 0 0 0 1 0 1 0 ## [11,] 0 0 0 0 0 0 0 1 0 0 1 ## [12,] 0 0 0 0 1 0 0 0 0 0 0 ## [,12] ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 1 ## [6,] 0 ## [7,] 0 ## [8,] 0 ## [9,] 0 ## [10,] 0 ## [11,] 0 ## [12,] 1 Here is what the corresponding network (with layovers) looks like. You can see why we refer to these additional edges as “loops.” However, we usually do not draw the network with these added loops to keep the figure less cluttered. g2=graph_from_adjacency_matrix(B,mode=&#39;undirected&#39;) airports = c(&quot;ATL&quot;,&quot;LAX&quot;,&quot;ORD&quot;,&quot;MSP&quot;,&quot;DEN&quot;,&quot;JFK&quot;,&quot;SFO&quot;,&quot;SEA&quot;,&quot;PHL&quot;,&quot;PDX&quot;,&quot;MDW&quot;,&quot;LGA&quot;) V(g2)$label = airports coords = layout_with_fr(g2) plot(g2, layout=coords, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 24.3.2 Step 2: Dynamical System Starting with the all 1’s vector \\(\\mathsf{v}_0 = [1, 1, \\ldots ,1 ]^{\\top}\\) create the dynamical system \\[ \\mathsf{v}_0, \\quad \\mathsf{v}_1 = B \\mathsf{v}_0, \\quad \\mathsf{v}_2 = B \\mathsf{v}_1, \\quad \\mathsf{v}_3 = B \\mathsf{v}_2, \\quad \\cdots , \\quad \\mathsf{v}_n = B \\mathsf{v}_{n-1}. \\] Here we calculate \\(\\mathsf{v}_1, \\ldots, \\mathsf{v}_{10}\\) using a loop: N = 10 X = matrix(0,nrow=nrow(B),ncol=N+1) # make a a table of 0s X[,1] = rep(1,nrow(B)) # put v0 in first column for (i in 1:N) { # loop N times X[,i+1] = B %*% X[,i] # apply B to the ith column and make it the (i+1)st column } rownames(X) = airports colnames(X) = 0:10 X ## 0 1 2 3 4 5 6 7 8 9 10 ## ATL 1 4 17 76 347 1603 7442 34638 161411 752642 3510616 ## LAX 1 6 29 139 650 3044 14211 66352 309652 1445058 6743119 ## ORD 1 3 15 72 343 1614 7567 35389 165336 771972 3603377 ## MSP 1 5 24 109 507 2349 10936 50930 237450 1107376 5165837 ## DEN 1 6 28 132 621 2909 13611 63595 296984 1386347 6470458 ## JFK 1 3 13 63 294 1377 6418 29939 139617 651292 3038392 ## SFO 1 5 26 122 576 2692 12585 58748 274225 1279724 5971890 ## SEA 1 6 26 120 551 2563 11923 55591 259246 1209469 5642972 ## PHL 1 2 6 23 99 446 2049 9491 44129 205540 958182 ## PDX 1 2 8 34 154 705 3268 15191 70782 330028 1539497 ## MDW 1 2 8 34 154 705 3268 15191 70782 330028 1539497 ## LGA 1 2 8 36 168 789 3698 17309 80904 377888 1764235 Discuss with your group: Each of the entries of the vector \\(\\mathsf{v}_{t}\\) in the columns of the table above corresponds to “a trip of length \\(t\\).” What kinds of trips does the \\(i\\)th entry of \\(\\mathsf{v}_{t}\\) count? Here is how you can figure this out: Compare the table of vectors with the picture of the network with layovers. Start by looking at the \\(t=1\\) column. The \\(i\\)th entry has something to do with the \\(i\\)th city. Next, look at the \\(t=2\\) column. And so on. Once you have noticed the connection between the network and data, explain why the rule \\(\\mathsf{v}_t = B \\mathsf{v}_{t-1}\\) leads to this result. These numbers get big fast! Let’s normalize by dividing by the sum each time. The vectors will always be proportions then, which sum to 1. See the table below. What do the entries in this table tell us? N = 10 X = matrix(0,nrow=nrow(B),ncol=N+1) X[,1] = rep(1,nrow(B)) for (i in 2:(N+1)) { X[,i] = B %*% X[,i-1] X[,i] = X[,i]/sum(X[,i]) } rownames(X) = airports colnames(X) = 0:10 X ## 0 1 2 3 4 5 ## ATL 1 0.08695652 0.08173077 0.07916667 0.07773297 0.07708213 ## LAX 1 0.13043478 0.13942308 0.14479167 0.14560932 0.14637430 ## ORD 1 0.06521739 0.07211538 0.07500000 0.07683692 0.07761108 ## MSP 1 0.10869565 0.11538462 0.11354167 0.11357527 0.11295441 ## DEN 1 0.13043478 0.13461538 0.13750000 0.13911290 0.13988267 ## JFK 1 0.06521739 0.06250000 0.06562500 0.06586022 0.06621466 ## SFO 1 0.10869565 0.12500000 0.12708333 0.12903226 0.12944797 ## SEA 1 0.13043478 0.12500000 0.12500000 0.12343190 0.12324485 ## PHL 1 0.04347826 0.02884615 0.02395833 0.02217742 0.02144643 ## PDX 1 0.04347826 0.03846154 0.03541667 0.03449821 0.03390075 ## MDW 1 0.04347826 0.03846154 0.03541667 0.03449821 0.03390075 ## LGA 1 0.04347826 0.03846154 0.03750000 0.03763441 0.03793999 ## 6 7 8 9 10 ## ATL 0.07674064 0.07657108 0.07647933 0.07643081 0.07640399 ## LAX 0.14654141 0.14667834 0.14671848 0.14674567 0.14675521 ## ORD 0.07802962 0.07823125 0.07833906 0.07839377 0.07842281 ## MSP 0.11277017 0.11258632 0.11250792 0.11245405 0.11242772 ## DEN 0.14035431 0.14058369 0.14071617 0.14078356 0.14082110 ## JFK 0.06618132 0.06618343 0.06615295 0.06613871 0.06612665 ## SFO 0.12977438 0.12986887 0.12993256 0.12995600 0.12997042 ## SEA 0.12294795 0.12288997 0.12283525 0.12282160 0.12281194 ## PHL 0.02112894 0.02098089 0.02090908 0.02087259 0.02085358 ## PDX 0.03369906 0.03358136 0.03353774 0.03351435 0.03350515 ## MDW 0.03369906 0.03358136 0.03353774 0.03351435 0.03350515 ## LGA 0.03813315 0.03826343 0.03833372 0.03837453 0.03839628 24.3.3 Step 4: Eigen-analysis We see that the vectors are converging to a common direction, and we know that dynamical systems converge to the dominant eigenvector (if there is one). We can see below that there is a dominant eigenvector in this case. eigen(B) ## eigen() decomposition ## $values ## [1] 4.66618847 2.64207538 2.41909839 1.80037113 1.27260439 ## [6] 1.00000000 1.00000000 0.49835918 0.02718633 -0.67732874 ## [11] -0.92557189 -1.72298263 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.23401334 0.57249329 -0.03677166 0.289569576 -0.11347970 ## [2,] -0.44971357 0.23508235 0.28327135 -0.009053428 0.29280642 ## [3,] -0.24039858 -0.04730698 0.44304618 0.102332378 0.42192015 ## [4,] -0.34439328 0.35635470 -0.30954196 -0.120977575 0.09253829 ## [5,] -0.43163295 -0.31276398 0.34545477 0.090957309 -0.17778914 ## [6,] -0.20257820 0.10572025 -0.24645181 -0.612347635 -0.18646435 ## [7,] -0.39829657 -0.18275409 -0.04019741 -0.369127791 -0.14336929 ## [8,] -0.37630557 -0.32813461 -0.43931838 0.235004528 0.03236396 ## [9,] -0.06383014 0.34864008 -0.02591199 0.361794131 -0.41627977 ## [10,] -0.10264218 -0.19982920 -0.30957570 0.293619448 0.11872135 ## [11,] -0.10264218 -0.19982920 -0.30957570 0.293619448 0.11872135 ## [12,] -0.11773343 -0.19046871 0.24343257 0.113643916 -0.65218735 ## [,6] [,7] [,8] [,9] ## [1,] 0.000000e+00 0.000000e+00 0.17344569 0.19570619 ## [2,] 2.626816e-16 1.203957e-16 0.27532162 0.33415547 ## [3,] -1.576610e-16 -4.434216e-17 -0.59911594 -0.08232149 ## [4,] 4.851644e-01 1.208947e-01 -0.01657232 -0.32336571 ## [5,] 1.198948e-16 1.395385e-16 0.02521939 -0.25407199 ## [6,] -5.005332e-18 -9.291441e-17 -0.52296040 0.18953651 ## [7,] -4.851644e-01 -1.208947e-01 0.27891061 0.13898200 ## [8,] -1.021498e-16 -3.393381e-17 0.08250644 -0.40482358 ## [9,] -4.851644e-01 -1.208947e-01 -0.34575674 -0.20117541 ## [10,] -1.709709e-01 6.861261e-01 -0.16447314 0.41613681 ## [11,] 1.709709e-01 -6.861261e-01 -0.16447314 0.41613681 ## [12,] 4.851644e-01 1.208947e-01 -0.05027381 0.26117231 ## [,10] [,11] [,12] ## [1,] 6.653425e-01 0.08742351 0.026476499 ## [2,] -4.571765e-01 -0.14917762 0.397124122 ## [3,] 1.379670e-01 0.40635541 -0.072909426 ## [4,] -2.621537e-01 0.02623868 -0.459495824 ## [5,] 2.257605e-01 -0.63328894 -0.198593023 ## [6,] 1.563407e-01 -0.22162883 0.307139725 ## [7,] -8.104161e-05 0.40052356 -0.376840313 ## [8,] 7.521122e-02 0.23285847 0.520458800 ## [9,] -3.966679e-01 -0.04540132 -0.009723345 ## [10,] -4.483988e-02 -0.12092951 -0.191135557 ## [11,] -4.483988e-02 -0.12092951 -0.191135557 ## [12,] -1.345953e-01 0.32888356 0.072932167 For an adjacency matrix \\(A\\), the dominant eigenvector of \\(B + I\\), scaled to sum to 1, is called Gould’s Index of network centrality. Here we extract it, scale it to sum to 1, and we show that the dynamical system is converging to it. # Get the dominant eigenvector vecs = eigen(B)$vectors gould = vecs[,1] gould = gould/sum(gould) # Compute the dynamical system N = 30 X = matrix(0,nrow=nrow(B),ncol=N+1) X[,1] = rep(1,nrow(B))/nrow(B) for (i in 1:N) { X[,i+1] = B %*% X[,i] X[,i+1] = X[,i+1]/sum(X[,i+1]) } # Display the data Y = cbind(X[,1],X[,2],X[,3],X[,11],X[,21],X[,31],gould) rownames(Y) = airports colnames(Y) = cbind(&quot;n=0&quot;,&quot;n=1&quot;,&quot;n=2&quot;,&quot;n=10&quot;,&quot;n=20&quot;,&quot;n=30&quot;,&quot;Gould&quot;) Y ## n=0 n=1 n=2 n=10 n=20 ## ATL 0.08333333 0.08695652 0.08173077 0.07640399 0.07637073 ## LAX 0.08333333 0.13043478 0.13942308 0.14675521 0.14676475 ## ORD 0.08333333 0.06521739 0.07211538 0.07842281 0.07845441 ## MSP 0.08333333 0.10869565 0.11538462 0.11242772 0.11239338 ## DEN 0.08333333 0.13043478 0.13461538 0.14082110 0.14086400 ## JFK 0.08333333 0.06521739 0.06250000 0.06612665 0.06611175 ## SFO 0.08333333 0.10869565 0.12500000 0.12997042 0.12998468 ## SEA 0.08333333 0.13043478 0.12500000 0.12281194 0.12280789 ## PHL 0.08333333 0.04347826 0.02884615 0.02085358 0.02083114 ## PDX 0.08333333 0.04347826 0.03846154 0.03350515 0.03349742 ## MDW 0.08333333 0.04347826 0.03846154 0.03350515 0.03349742 ## LGA 0.08333333 0.04347826 0.03846154 0.03839628 0.03842243 ## n=30 Gould ## ATL 0.07637062 0.07637062 ## LAX 0.14676474 0.14676474 ## ORD 0.07845446 0.07845446 ## MSP 0.11239329 0.11239329 ## DEN 0.14086410 0.14086410 ## JFK 0.06611172 0.06611172 ## SFO 0.12998472 0.12998472 ## SEA 0.12280792 0.12280792 ## PHL 0.02083107 0.02083107 ## PDX 0.03349744 0.03349744 ## MDW 0.03349744 0.03349744 ## LGA 0.03842249 0.03842249 24.3.4 Step 5 Now let’s plot the network with: the vertices sized by Gould’s Index the labels sized by degree centrality plot(g, layout=locations, vertex.size=250*gould,vertex.label.cex=8*p, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot; ) And we show the data containing Gould’s Index and the Degree Centrality. We order the data using the Gould Index and then compare the two. Observe that degree centrality and Gould’s Index do not always agree. Z = cbind(gould,p) rownames(Z)=airports colnames(Z)=c(&#39;Gould&#39;, &#39;Degree&#39;) ii=order(gould,decreasing=TRUE) Z = Z[ii,] Z ## Gould Degree ## LAX 0.14676474 0.14705882 ## DEN 0.14086410 0.14705882 ## SFO 0.12998472 0.11764706 ## SEA 0.12280792 0.14705882 ## MSP 0.11239329 0.11764706 ## ORD 0.07845446 0.05882353 ## ATL 0.07637062 0.08823529 ## JFK 0.06611172 0.05882353 ## LGA 0.03842249 0.02941176 ## PDX 0.03349744 0.02941176 ## MDW 0.03349744 0.02941176 ## PHL 0.02083107 0.02941176 Discuss with your group: Degree centrality and Gould’s Index give different rankings. Look at the table and observe that: LAX, DEN and SEA have the same degree centrality. However LAX and DEN have higher Gould Index than SEA. SFO has lower degree centrality than SEA, but higher Gould centrality! So these two centralities give different rankings. Why does the Gould Index value SFO more than SEA? Find another pair of cities where the rankings of degree centrality and Gould’s Index differ. Look at the plot of the network and explain why this is the case. 24.3.5 Gould Index Summary Now that we understand what Gould’s Index means, let’s summarize how to find the Gould Index values for an adjacency matrix \\(A\\). Create the matrix \\(B = A+I\\). Find the dominant eigenvector \\(\\mathbf{v}\\) of \\(B\\). Normalize the values of \\(\\mathbf{v}\\) so that the entries sum to 1. 24.4 Your Turn: The Rise of Moscow Russian historians often attribute the dominance and rise to power of Moscow to its strategic position on medieval trade routes (see Figure 1). Others argue that sociological and political factors aided Moscow’s rise to power, and thus Moscow did not rise to power strictly because of its strategic location on the trade routes. The figure below shows the major cities and trade routes of medieval Russia. Use Gould’s Index to form a geographer’s opinion about this debate. Either: Moscow’s location was the primary reason for its rise to power, or Other forces must have come into play. Here is the adjacency matrix for this transportation network into an adjacency matrix and a plot of the network. RusCity = c(&quot;Novgorod&quot;, &quot;Vitebsk&quot;, &quot;Smolensk&quot;, &quot;Kiev&quot;, &quot;Chernikov&quot;, &quot;Novgorod Severskij&quot;, &quot;Kursk&quot;, &quot;Bryansk&quot;, &quot;Karachev&quot;, &quot;Kozelsk&quot;, &quot;Dorogobusch&quot;, &quot;Vyazma&quot;, &quot;A&quot;, &quot;Tver&quot;, &quot;Vishnij Totochek&quot;, &quot;Ksyatyn&quot;, &quot;Uglich&quot;, &quot;Yaroslavl&quot;, &quot;Rostov&quot;, &quot;B&quot;, &quot;C&quot;, &quot;Suzdal&quot;, &quot;Vladimir&quot;, &quot;Nizhnij Novgorod&quot;, &quot;Bolgar&quot;, &quot;Isad&#39;-Ryazan&quot;, &quot;Pronsk&quot;, &quot;Dubok&quot;, &quot;Elets&quot;, &quot;Mtsensk&quot;, &quot;Tula&quot;, &quot;Dedoslavl&quot;, &quot;Pereslavl&quot;, &quot;Kolomna&quot;, &quot;Moscow&quot;, &quot;Mozhaysk&quot;, &quot;Dmitrov&quot;, &quot;Volok Lamskij&quot;, &quot;Murom&quot;) A = rbind(c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)) g=graph_from_adjacency_matrix(A,mode=&#39;undirected&#39;) V(g)$label = RusCity # Plot network plot(g) Create a vector containing the normalized Degree Centralities. See Section 24.2 for help. Create a vector containing the Gould Index values. See Section 24.3.5 for help. Plot the network where the size of the vertices is determined by Gould’s Index and the size of the label is determined by degree centrality. Create a data frame that contains Gould’s Index and Degree Centralities. The rows should be labeled with the city names and the columns should be named by the centrality measures. Sort according to Gould’s Index. Use Gould’s Index to decide whether Moscow’s dominance was solely due to its geographic location. Compare the Gould’s Index and Degree Centrality rankings and note any interesting findings. See Section 24.3.4 for help. "],["geometry-in-mathbbrn.html", "Section 25 Geometry in \\(\\mathbb{R}^n\\) 25.1 Dot Product 25.2 Length, Distance, Angle 25.3 Orthogonal Complement", " Section 25 Geometry in \\(\\mathbb{R}^n\\) Download this Rmd file 25.1 Dot Product We will look at the gometry of \\(n\\)-dimensional vectors. For example, here are three vectors in \\(\\mathbb{R}^6\\). We can compute the dot product two different ways. If you have included lit pracma library, you can use the dot command. ## [1] 79 ## [1] -1 ## [1] 0 We can also compute the dot product in native R by multiplying \\(u^T v\\). It is important to remember that this works. ## [,1] ## [1,] 79 ## [,1] ## [1,] -1 ## [,1] ## [1,] 0 25.2 Length, Distance, Angle The length of a vector can be computed using \\(\\sqrt{u\\cdot u}\\) or as the built in 2-norm of a vector. The reason it is the 2-norm is because we are squaring (second power) and taking the square root. ## [,1] ## [1,] 9.539392 ## [1] 9.539392 The distance between two vectors, is the length of the difference between them ## [,1] ## [1,] 1.732051 ## [1] 1.732051 The angle between two vectors is given by the formula \\[ \\theta = \\arccos\\left(\\frac{ v \\cdot w } {||v|| ||w||} \\right) \\] which can be comuted using arccosine function acos. ## [,1] ## [1,] 0.1427914 Sometimes we use the cosine of the angle between the two vectors (we will see an example of this in the homework) \\[ \\cos(\\theta) = \\frac{ v \\cdot w } {||v|| ||w||} \\] which is computed as ## [,1] ## [1,] 0.9898226 This is a number between -1 and 1, with numbers close to 1 meaning that they are closely aligned, numbers close to 0 meaning that they are close to orthogonal, and numbers close to -1 meaning that they are close to opposite. ## [,1] ## [1,] 0 ## [,1] ## [1,] -0.05345225 25.3 Orthogonal Complement The orthogonal complement of a vector space \\(W\\) is \\[ W^\\perp = \\left\\{ v \\in \\mathbb{R}^n \\mid v \\cdot w = 0 \\text{ for every } w \\in W \\right\\}. \\] The orthogonal complement is a subspace. Furthermore, it is enough to check that \\(w\\) is orthogonal to a basis of \\(W\\). Tnat is, you don’t have to check every vector in \\(W\\); if you are orthogonal to the basis then you are orthogonal to \\(W\\). For example, if \\[ W = \\mathsf{span} \\left\\{ \\begin{bmatrix} 1\\\\2\\\\3\\\\4\\\\5 \\end{bmatrix}, \\begin{bmatrix} 1\\\\1\\\\1\\\\1\\\\1 \\end{bmatrix}, \\begin{bmatrix} 1\\\\2\\\\2\\\\2\\\\1 \\end{bmatrix}, \\begin{bmatrix} 3\\\\5\\\\6\\\\7\\\\7 \\end{bmatrix}, \\begin{bmatrix} 0\\\\2\\\\1\\\\0\\\\-4\\end{bmatrix} \\right\\}, \\] then we can put the vectors of \\(W\\) into the rows of a matrix. So in this case, we make the matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 1 1 1 1 1 ## [3,] 1 2 2 2 1 ## [4,] 3 5 6 7 7 ## [5,] 0 2 1 0 -4 Now, \\(W\\) is the row space of \\(A\\). That is, \\(W = \\mathsf{Row}(A)\\). And the row space is orthogonal to the null space. Therefore \\(W^\\perp = \\mathsf{Nul}(A)\\), so we row reduce, ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 1 ## [2,] 0 1 0 -1 -4 ## [3,] 0 0 1 2 4 ## [4,] 0 0 0 0 0 ## [5,] 0 0 0 0 0 There are 2 free variables, so the null space and, thus, \\(W^\\perp\\) are 2 dimensional. We describe a basis of the null space \\[ W^\\perp = \\mathsf{Nul}(A) = \\mathsf{span} \\left\\{ \\begin{bmatrix} 0 \\\\ 1 \\\\ -2 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 4 \\\\ -4 \\\\ 0 \\\\ 1 \\end{bmatrix} \\right\\}. \\] We can check that these vectors are orthogonal to \\(W\\) by multiplying ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 0 ## [3,] 0 0 ## [4,] 0 0 ## [5,] 0 0 "],["least-squares-approximation.html", "Section 26 Least Squares Approximation 26.1 Introduction 26.2 Example 1 26.3 Template 26.4 Fitting for a Linear Function 26.5 Fitting for a Quadratic Function", " Section 26 Least Squares Approximation Download this Rmd file 26.1 Introduction Let’s start with a summary of Least Squares Approximation. The Why: Given a matrix \\(A\\) and a vector \\(\\mathsf{b}\\) that is not in \\(W = \\mathrm{Col}(A)\\), we want to find the “best approximate solution” \\(\\hat{\\mathsf{b}} \\in W\\). In other words, we want to pick the best possible \\(\\hat{\\mathsf{b}} \\approx \\mathsf{b}\\) that lies in the column space of \\(A\\). The What: The answer is to use projections. This “best approximation” is the projection \\(\\hat{\\mathsf{b}} = \\mbox{proj}_W \\mathsf{b}\\). The residual vector vector \\(\\mathsf{r} = \\mathsf{b} - \\hat{\\mathbf{b}}\\) is in \\(W^{\\perp}\\). The length \\(\\| \\mathsf{r} \\|\\) of the residual vector measures the closeness the approximation. The approximate solution to our original problem is the vector \\(\\hat{\\mathsf{x}}\\) such that \\(A \\hat{\\mathsf{x}} = \\hat{\\mathsf{b}}\\). The How: A clever way to solve this is to use the normal equations. The best choice for \\(\\hat{\\mathsf{x}}\\) satisfies \\[ A^{\\top} A \\hat{\\mathsf{x}} = A^{\\top} \\mathsf{b}. \\] 26.2 Example 1 Here is an example that we did by hand in class. We now see how to do it in R. Find the least-squares solution to \\({\\mathsf{A}}x = {\\mathsf{b}}\\) if \\[\\begin{equation} {\\mathsf{A}}= \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 0 &amp; -1 \\\\ \\end{bmatrix} \\quad \\mbox{and} \\quad {\\mathsf{b}}= \\begin{bmatrix} 1 \\\\ 1 \\\\ 3 \\end{bmatrix}. \\tag{26.1} \\end{equation}\\] First, for good measure, let’s see if the system is inconsistent A = cbind(c(1,1,0),c(1,2,-1)) b = c(1,1,3) Ab = cbind(A,b) Ab ## b ## [1,] 1 1 1 ## [2,] 1 2 1 ## [3,] 0 -1 3 rref(Ab) ## b ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 True indeed: \\({\\mathsf{b}}\\not \\in Col({\\mathsf{A}})\\). Now we compute the normal equations to see what they look like: t(A) %*% A ## [,1] [,2] ## [1,] 2 3 ## [2,] 3 6 t(A) %*% b ## [,1] ## [1,] 2 ## [2,] 0 So we are going to instead solve the following normal equations instead of (26.1): \\[\\begin{equation} \\begin{bmatrix} 2 &amp; 3 \\\\ 3 &amp; 6 \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}. \\tag{26.2} \\end{equation}\\] We can do this in the following nice, single R command (xhat = solve(t(A) %*% A, t(A) %*% b)) ## [,1] ## [1,] 4 ## [2,] -2 To compute \\(\\hat {\\mathsf{b}}\\) we use (bhat = A %*% xhat) ## [,1] ## [1,] 2 ## [2,] 0 ## [3,] 2 And to get the residual, we use (r = b - bhat) ## [,1] ## [1,] -1 ## [2,] 1 ## [3,] 1 sqrt(t(r) %*% r) ## [,1] ## [1,] 1.732051 We can also check that the residual is orthogonal to \\(Col({\\mathsf{A}})\\): t(A) %*% r ## [,1] ## [1,] 0 ## [2,] 0 26.3 Template The following R code does it all. You can use this as a template for future problems. Just enter the matrix A and the vector b. # Given: the matrix A A = cbind(c(1,1,0),c(1,2,-1)) # Given: the target vector b b = c(1,1,3) #solve the normal equation (xhat = solve(t(A) %*% A, t(A) %*% b)) ## [,1] ## [1,] 4 ## [2,] -2 # find the projection (bhat = A %*% xhat) ## [,1] ## [1,] 2 ## [2,] 0 ## [3,] 2 # find the residual vector (r = b - bhat) ## [,1] ## [1,] -1 ## [2,] 1 ## [3,] 1 # check that z is orthogonal to Col(A) t(A) %*% r ## [,1] ## [1,] 0 ## [2,] 0 # measure the distance between bhat and b sqrt( t(r) %*% r) ## [,1] ## [1,] 1.732051 26.4 Fitting for a Linear Function Here are some points that we’d like to fit to a linear function \\(y = a_0 + a_1 x\\). Note: Here we use y instead of b because we like to write linear equations as “\\(y = cx + d\\).” So the expression “\\(b = a_0 + a_1 x\\)” looks funny to us. So we will talk about y and yhat instead of b and bhat. x = c(1,2,3,4,5,6) y = c(7,2,1,3,7,7) plot(x,y,pch=19,ylim=c(0,10)) grid() The linear equations that we want to fit are as follows. \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\\\ 1 &amp; 4 \\\\ 1 &amp; 5 \\\\ 1 &amp; 6 \\\\ \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 2 \\\\ 1 \\\\ 3 \\\\ 7 \\\\ 7 \\end{bmatrix}. \\] These equations are inconsistent, so we solve the normal equations \\(A^T A x = A^T y\\) and find an approximate solution instead. Pro Tip: a clever way to create the desired matrix \\(A\\) is to use the fact that \\(x^0=1\\) for any number \\(x\\). (A = cbind(x^0,x)) ## x ## [1,] 1 1 ## [2,] 1 2 ## [3,] 1 3 ## [4,] 1 4 ## [5,] 1 5 ## [6,] 1 6 Let’s take a look at the normal equations: t(A) %*% A ## x ## 6 21 ## x 21 91 t(A) %*% y ## [,1] ## 27 ## x 103 So the normal equations to solve are below. It’s surprising how, even though there are 6 variables, we only have to solve a 2x2 equation, since there are 2 unknowns. \\[ \\begin{bmatrix} 6 &amp; 21 \\\\ 21 &amp; 91 \\\\ \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix} = \\begin{bmatrix} 27 \\\\ 103 \\end{bmatrix}. \\] (xhat = solve(t(A) %*% A, t(A) %*% y)) ## [,1] ## 2.8000000 ## x 0.4857143 This tells us that the desired intercept is \\(a_0 = 2.8\\), the desired slope is \\(a_1 = 0.4856\\), and the linear model is \\(y = 2.8 + 0.4856x\\). We can plot the points together with the solution using: #plot the original set of points plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main=&#39;the best-fit linear function&#39;) # generate points for the fitted line and plot it tt = seq(1,6,len=100) lines(tt,xhat[1]+xhat[2]*tt,col=&#39;blue&#39;) # get yhat yhat = A %*% xhat # add the residuals to the plot for (i in 1:length(x)) { lines(c(x[i],x[i]),c(y[i],yhat[i]), col=&#39;red&#39;) } #add yhat to the plot points(x,yhat,pch=19,col=&#39;orange&#39;) #put the original points back on the plot last so we can see them points(x,y,pch=19,col=&quot;black&quot;) grid() In this visualization we see the following: The black points: the original data points cbind(x,y). This represents the entries of the desired target vector y. The blue curve: the fitted curve, created from the approximate solution xhat. The orange points: the approximations cbind(x,yhat) of the data points cbind(x,y). This represents entries of the projection yhat. The red line segments: the distances between the original data points (block dots) and their approximations (orange dots). The lengths of these red segments are the entries of the residual vector r. Let’s look at the residual and see that it is indeed orthogonal to the columns of \\(A\\). yhat = A %*% xhat r = y - yhat res=cbind(y,yhat,r) colnames(res) = c(&quot;y&quot;,&quot;yhat&quot;,&quot;r&quot;) res ## y yhat r ## [1,] 7 3.285714 3.714286 ## [2,] 2 3.771429 -1.771429 ## [3,] 1 4.257143 -3.257143 ## [4,] 3 4.742857 -1.742857 ## [5,] 7 5.228571 1.771429 ## [6,] 7 5.714286 1.285714 t(A) %*% r ## [,1] ## -8.881784e-16 ## x 3.552714e-15 sqrt(t(r) %*% r) ## [,1] ## [1,] 5.947388 26.5 Fitting for a Quadratic Function The data we have been working with has a quadratic look to it, so let’s try adding an \\(x^2\\) term. That is, we will fit the model \\(y = a_0 + a_1 x + a_2 x^2\\). The equations we want to solve are In this case, the linear model that we’d like to solve is: \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 2 &amp; 4 \\\\ 1 &amp; 3 &amp; 9 \\\\ 1 &amp; 4 &amp; 16 \\\\ 1 &amp; 5 &amp; 25 \\\\ 1 &amp; 6 &amp; 36 \\\\ \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 2 \\\\ 1 \\\\ 3 \\\\ 7 \\\\ 7 \\end{bmatrix}. \\] It is easy enough to add this to our matrix \\(A\\). (A = cbind(x^0,x,x^2)) ## x ## [1,] 1 1 1 ## [2,] 1 2 4 ## [3,] 1 3 9 ## [4,] 1 4 16 ## [5,] 1 5 25 ## [6,] 1 6 36 In this case our normal equations are 3x3 t(A) %*% A ## x ## 6 21 91 ## x 21 91 441 ## 91 441 2275 t(A) %*% y ## [,1] ## 27 ## x 103 ## 499 \\[ \\begin{bmatrix} 6 &amp; 21 &amp; 91 \\\\ 21 &amp; 91 &amp; 441 \\\\ 91 &amp; 441 &amp; 2275 \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} 27 \\\\ 103 \\\\ 499 \\end{bmatrix}. \\] Whose solution is computed by (xhat = solve(t(A) %*% A, t(A) %*% y)) ## [,1] ## 10.3000000 ## x -5.1392857 ## 0.8035714 Notice that our solution is now \\(y = 10.3 - 5.1393 x + 0.8036 x^2\\). The linear term is now negative, but there is a positive quadratic term. Let’s look at the same plo but with the addex \\(x^2\\) term. We see that the residuals are smaller and, importantly, the model appears to better fit the data. #plot the original set of points plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main=&#39;the best-fit quadratic function&#39;) # generate points for the fitted line and plot it tt = seq(0,7,len=100) lines(tt,xhat[1]+xhat[2]*tt+xhat[3]*tt^2,col=&#39;blue&#39;) # get yhat yhat = A %*% xhat # add the residuals to the plot for (i in 1:length(x)) { lines(c(x[i],x[i]),c(y[i],yhat[i]), col=&#39;red&#39;) } #add yhat to the plot points(x,yhat,pch=19,col=&#39;orange&#39;) #put the original points back on the plot last so we can see them points(x,y,pch=19,col=&quot;black&quot;) grid() Let’s look again at the residual and see that it is indeed orthogonal to the columns of \\(A\\) and see that the residual got shorter. yhat = A %*% xhat r = y - yhat res=cbind(y,yhat,r) colnames(res) = c(&quot;y&quot;,&quot;yhat&quot;,&quot;r&quot;) res ## y yhat r ## [1,] 7 5.964286 1.035714 ## [2,] 2 3.235714 -1.235714 ## [3,] 1 2.114286 -1.114286 ## [4,] 3 2.600000 0.400000 ## [5,] 7 4.692857 2.307143 ## [6,] 7 8.392857 -1.392857 t(A) %*% r ## [,1] ## -1.820766e-14 ## x -2.486900e-14 ## -1.847411e-13 sqrt(t(r) %*% r) ## [,1] ## [1,] 3.356231 "],["quiz-1-review.html", "Section 27 Quiz 1 Review 27.1 Overview 27.2 Practice Problems 27.3 Solutions to Practice Problems", " Section 27 Quiz 1 Review 27.1 Overview Our first quiz covers sections 1.1 - 1.8 and part of 1.9 in Lay’s book. This corresponds to Problem Sets 1 and 2. In 1.8 and 1.9, it will cover the definition of a linear transformation as well as one-to-one and onto, but it will not cover the matrix of a linear transformation. 27.1.1 Vocabulary and Concepts You should understand these concepts and be able to read and use these terms correctly: elementary row operations REF and RREF pivot position linear combination span linear independence homogeneous and nonhomogeneous equations Understand the geometric relationship between the solutions to \\(Ax = 0\\) and \\(Ax=b\\) Understand Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b\\) in \\(\\mathbb{R}^m\\), \\(A x = b\\) has a solution Each \\(b\\) in \\(\\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. The columns of \\(A\\) are linearly independent if and only if \\(Ax=0\\) only has the trivial solution Understand Theorem 8 in Section 1.7: if you have more than \\(n\\) vectors in \\(\\mathbb{R}^n\\) they must be linearly dependent. linear transformation one-to-one and onto Understand Theorem 12 in Section 1.9 which states that \\(T(x)=Ax\\) is onto if and only if the column of \\(A\\) span \\(\\mathbb{R}^m\\) \\(T(x)=Ax\\) is one-to-one if and only if the columns of \\(A\\) are linearly independent. 27.1.2 Skills You should be able to perform these linear algebra tasks. Identify linear systems from nonlinear systems Make the augmented matrix from a set of equations Row reduce a system of equations into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Write the solution set to \\(Ax=b\\) as a parametric vector equation. Convert back and forth between systems of equations, vector equations, and matrix equations. Compute the matrix-vector product \\(Ax\\) Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) Manipulate matrix vector products using: \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) Determine whether a linear transformation is one-to-one and/or onto 27.2 Practice Problems 27.2.1 I have performed some row operations below for you on a matrix \\(A\\). Write out the complete set of solutions to \\(A \\mathsf{x} = {\\bf 0}\\). \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1 \\\\ 1&amp; 2&amp; 1&amp; 1&amp; 0&amp; -2 \\\\ 2&amp; 4&amp; -2&amp; 6&amp; 1&amp; 2 \\\\ 1&amp; 2&amp; 0&amp; 2&amp; -1&amp; -3 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 1&amp; -1&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 1&amp; 2\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ \\end{bmatrix} \\] 27.2.2 I have performed some row operations below for you on a matrix \\(B\\). \\[ B= \\begin{bmatrix} 1&amp; 1&amp; 0 \\\\ 0&amp; 1&amp; 1 \\\\ 2&amp; 1&amp; 2 \\\\ 1&amp; -1&amp; 1 \\\\ 2&amp; 3&amp; 1 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 0&amp; 0 \\\\ 0&amp; 1&amp; 0 \\\\ 0&amp; 0&amp; 1 \\\\ 0&amp;0&amp;0 \\\\ 0&amp;0&amp;0 \\\\ \\end{bmatrix} \\] a. Describe the solutions to the equation \\(B \\mathsf{x} = {\\bf 0}\\). Fill in the boxes: the transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{\\square}\\) to \\(\\mathbb{R}^{\\square}\\). 27.2.3 I want to know if it is possible to write \\(\\mathsf{w}\\) as a linear combination of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) below. Write down, but do not solve, a matrix equation that would solve this problem. Your answer should be of the form \\(A \\mathsf{x} = \\mathsf{b}\\), where I can clearly see what \\(A, \\mathsf{x}\\), and \\(\\mathsf{b}\\) are. I should also be able to tell how many unknowns there are. \\[ \\mathsf{v}_1 = \\left[ \\begin{matrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_2 = \\left[ \\begin{matrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_3 = \\left[ \\begin{matrix} 1 \\\\ 1 \\\\ 0 \\\\ -2 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{w} = \\left[ \\begin{matrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\\\ \\end{matrix}\\right] . \\] 27.2.4 Describe all vectors that are not in the span of the columns of the matrix \\(A\\) below: \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 4 \\\\ -3&amp; -5&amp; -11\\\\ 1&amp; 1&amp; 3 \\\\ \\end{bmatrix} \\] 27.2.5 The matrix below is \\(3 \\times 3\\) but the third column is missing. Add a nonzero third column so that the columns of \\(A\\) are linearly dependent and add a 3rd column so that the columns of \\(A\\) are linearly independent. Briefly describe your strategy. \\[ \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\qquad\\qquad \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\] 27.2.6 In each case below, find the matrix of the linear transformation that is described, if you believe that the matrix exists. Otherwise, demonstrate that the transformation is not linear. The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix}\\right) = \\begin{bmatrix} x_1 + x_2 \\\\ 2 x_1 \\\\ -x_2 \\\\\\end{bmatrix}. \\] The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)= \\begin{bmatrix} x_1 + x_2 + x_3 \\\\ x_1 x_2 \\\\ -x_2 + 2 x_3 \\end{bmatrix}. \\] The transformation \\(L: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) The transformation \\(R: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) 27.2.7 Write the following systems of equations in vector and matrix form. \\[ \\begin{array} {ccccccccccc} 5 x_1 &amp;+&amp; 3 x_2 &amp;+&amp; x_3 &amp;+&amp; 11 x_4 &amp;-&amp; x_5 &amp;=&amp; 10 \\\\ 4 x_1 &amp;+&amp; x_2 &amp;+&amp; 3 x_3 &amp;+&amp; 2 x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 11 \\\\ - x_1 &amp;+&amp; 3 x_2 &amp;-&amp; 2 x_3 &amp;+&amp; x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 12 \\\\ \\end{array} \\] 27.2.8 Let \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) be the vectors in the columns of the matrix \\(A\\) below. \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 3 &amp; 1 \\\\ 2 &amp; 0 &amp; 2 &amp; 3 \\\\ 1 &amp; 1 &amp; 3 &amp; 1 \\\\ -1 &amp; 0 &amp; -1 &amp; 0 \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right] \\] a. Are the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) linear independent or dependent? If they are linearly dependent, please give a dependence relation among them. b. Describe the span of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) inside of \\(\\mathbb{R}^4\\)? 27.2.9 Find a solution to \\(A \\mathsf{x}=0\\) that no one else in the class has. \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 2 &amp; 0 &amp; 4 &amp; 1 &amp; 4 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 1 &amp; 0 &amp; 2 &amp; 1 &amp; 3 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\] 27.3 Solutions to Practice Problems 27.3.1 The parametric vector form of the solution is \\[\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\\\ 1 \\\\0 \\\\ 0 \\end{bmatrix} u \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\-2 \\\\ 1 \\end{bmatrix}\\] 27.3.2 There is one solution: \\(\\mathsf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\). The transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{3}\\) to \\(\\mathbb{R}^{5}\\). 27.3.3 \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 0 &amp; 1 \\\\ 3 &amp; 1 &amp; 0 \\\\ 4 &amp; 0 &amp; -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\end{bmatrix} \\] 27.3.4 We want to find all target vectors \\(\\mathsf{b}\\) such that \\(A \\mathsf{x} = \\mathsf{b}\\) is inconsistent. So we want the augmented matrix \\(\\begin{bmatrix} A \\,| \\, b \\end{bmatrix}\\) to have a pivot in the last column. \\[ \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ -3&amp; -5&amp; -11 &amp; b_2\\\\ 1&amp; 1&amp; 3 &amp; b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; -1&amp; -1 &amp; -b_1+b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; 0&amp; 0 &amp; 2b_1+b_2+b_3 \\\\ \\end{array} \\right] \\] So the set of target vectors that are not in the span of the columns of \\(A\\) are the vectors \\[ \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\qquad \\mbox{where} \\qquad 2b_1 + b_2 + b_3 \\neq 0. \\] 27.3.5 This is on PS2. 27.3.6 This is a linear transformation with \\[A = \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}.\\] This is not a linear transformation because \\[ 2 \\, T \\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 2 \\\\ 2 \\end{bmatrix} \\quad \\mbox{while} \\quad T \\left( \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 6 \\\\ 4 \\\\ 2 \\end{bmatrix}. \\] \\(A= \\begin{bmatrix} 1/2 &amp; 1/2 \\\\ 1/2 &amp; 1/2 \\end{bmatrix}\\) \\(A= \\begin{bmatrix} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{bmatrix}\\) 27.3.7 Vector Form: \\[ x_1 \\begin{bmatrix} 5 \\\\ 4 \\\\ -1 \\end{bmatrix} + x_2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 3 \\end{bmatrix} + x_3 \\begin{bmatrix} 1 \\\\ 3 \\\\ -2 \\end{bmatrix} + x_4 \\begin{bmatrix} 11 \\\\ 2 \\\\ 1 \\end{bmatrix} + x_5 \\begin{bmatrix} -1 \\\\ 6 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] Matrix Form: \\[ \\begin{bmatrix} 5 &amp; 3 &amp; 1 &amp; 11 &amp; -1 \\\\ 4 &amp; 1 &amp; 3 &amp; 2 &amp; 6 \\\\ -1 &amp; 3 &amp; -2&amp; 1 &amp; 6 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] 27.3.8 \\(-\\mathsf{v}_1 - 2\\mathsf{v}_2 + \\mathsf{v}_3 + 0 \\mathsf{v}_4 = 0\\). \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4)\\) looks like a copy of \\(\\mathbb{R}^3\\) sitting inside \\(\\mathbb{R}^4\\). In other words, is 3-dimensional subset of \\(\\mathbb{R}^4\\). 27.3.9 The general solution is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] My solution is \\[77,083,679 \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - 72,159,215 \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] "],["quiz-2-review.html", "Section 28 Quiz 2 Review 28.1 Overview 28.2 Practice Problems 28.3 Solutions to Practice Problems", " Section 28 Quiz 2 Review 28.1 Overview Our second quiz covers the following sections: Linear Transformations 2.1: Matrix Multiplication and the Matrix of a Linear Transformation 2.2: Matrix Inverses 2.3: The Invertible Matrix Theorem Subspaces 4.1: Subspaces of \\(\\mathbb{R}^n\\) 4.2: Null Space and Column Space 4.3: Bases 4.4: Coordinates This corresponds to Problem Sets 3 and 4. The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Look over the Edfinity homework assingments Do practice problems from the Edfinity Practice assignments. These allow you to “Practice Similar” by generating new variations of the same problem. Redo the Jamboard problems Try to resolve the Problem Sets and compare your answers to the solutions. Do the practice problems below. Compare your answers to the solutions. 28.1.1 Vocabulary and Concepts You should understand these concepts and be able to read and use these terms correctly: all of the Important Definitions found here. matrix multiplication matrix inverses the Invertible Matrix Theorem subspaces null space and column space of a matrix kernel and image of a linear transformation basis (span and linearly independent) coordinate vector with respect to a basis \\(\\mathcal{B}\\) change-of-coordinates matrix dimension 28.1.2 Skills You should be able to perform these linear algebra tasks. solve matrix algebra equations find a matrix inverse show that a subset is a subspace or demonstrate that it is not a subspace describe the null space and the column space determine if a vector is in a null space or column space find a basis of a subspace answer questions about the connections between all these ideas write short proofs of basic statements using the Important Definitions 28.2 Practice Problems 28.2.1 In each case below, find the matrix of the linear transformation that is described, if you believe that the matrix exists. Otherwise, demonstrate that the transformation is not linear. The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix}\\right) = \\begin{bmatrix} x_1 + x_2 \\\\ 2 x_1 \\\\ -x_2 \\\\\\end{bmatrix}. \\] The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)= \\begin{bmatrix} x_1 + x_2 + x_3 \\\\ x_1 x_2 \\\\ -x_2 + 2 x_3 \\end{bmatrix}. \\] The transformation \\(L: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) The transformation \\(R: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) 28.2.2 Find the inverse of the matrix \\[ \\left[ \\begin{array}{rrr} 1 &amp; -2 &amp; 2 \\\\ 1 &amp; 0 &amp; 0 \\\\ 2 &amp;-4 &amp; 5 \\end{array} \\right] \\] 28.2.3 Suppose that a linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) has the property that \\(T(\\mathsf{u}) = T(\\mathsf{v})\\) for some pair of distinct vectors \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\). Can \\(T\\) map \\(\\mathbb{R}^n\\) onto \\(\\mathbb{R}^n\\)? Why or why not? 28.2.4 Let \\(U\\) and \\(W\\) be subspaces of a vector space \\(\\mathbb{R}^n\\). Prove or disprove the following statements. Prove them by showing that the conditions are being a subspace are satisfied. Disprove them with a specific counter example. \\(U \\cap W = \\{ \\mathsf{v} \\in \\mathbb{R}^n \\mid \\mathsf{v} \\in U \\mbox{ and } \\mathsf{v} \\in W \\}\\) is a subspace \\(U \\cup W = \\{ \\mathsf{v} \\in \\mathbb{R}^n \\mid \\mathsf{v} \\in U \\mbox{ or } \\mathsf{v} \\in W \\}\\) is a subspace \\(U+W = \\{\\mathsf{u} + \\mathsf{w} \\mid \\mathsf{u} \\in U \\mbox{ and } \\mathsf{w} \\in W \\}\\) is a subspace 28.2.5 Let \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) be a linear transformation. Suppose that \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is one-to-one. Prove that if \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) are linearly independent, then \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) are linearly independent. Suppose that \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is onto. Prove that if \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) span \\(\\mathbb{R}^n\\) then \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) span \\(\\mathbb{R}^m\\). 28.2.6 I have performed some row operations below for you on a matrix \\(A\\). Find a basis for the column space and the null space of \\(A\\). \\[ A= \\left[ \\begin{matrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1 \\\\ 1&amp; 2&amp; 1&amp; 1&amp; 0&amp; -2 \\\\ 2&amp; 4&amp; -2&amp; 6&amp; 1&amp; 2 \\\\ 1&amp; 2&amp; 0&amp; 2&amp; -1&amp; -3 \\\\ \\end{matrix}\\right] \\longrightarrow \\left[ \\begin{matrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 1&amp; -1&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 1&amp; 2\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ \\end{matrix}\\right] \\] 28.2.7 Consider the matrix \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 5 &amp; 2 &amp; -4 \\\\ 3 &amp; 10 &amp; 2 &amp; 8 \\\\ 4 &amp; 15 &amp; 4 &amp; 4 \\end{array} \\right] \\] Find a basis for \\(\\mathrm{Col}(A)\\). Find a basis for \\(\\mathrm{Nul}(A)\\). 28.2.8 Are the vectors in \\({\\mathcal B}\\) a basis of \\(\\mathbb{R}^3\\)? If not, find a basis of \\(\\mathbb{R}^3\\) that consists of as many of the vectors from \\({\\mathcal B}\\) as is possible. Explain your reasoning. \\[ \\mathcal{B}=\\left\\{ \\begin{bmatrix} 1 \\\\ -1 \\\\ -2 \\end{bmatrix},\\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix},\\begin{bmatrix} -1 \\\\ -1 \\\\ -8 \\end{bmatrix} \\right\\} \\] 28.2.9 Find the coordinates of \\(\\mathsf{w}\\) in the standard basis and of \\(\\mathsf{v}\\) in the \\(\\mathcal{B}\\)-basis. \\[ \\mathcal{B} = \\left\\{ \\mathsf{v}_1=\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_2=\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_3=\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_4=\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right\\}, \\] \\[ \\mathsf{w} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\\\ -1 \\end{bmatrix}_{\\mathcal{B}}, \\qquad \\mathsf{v} = \\begin{bmatrix} 10 \\\\ 9 \\\\ 7 \\\\ 4 \\end{bmatrix}_{\\mathcal{S}} \\] 28.2.10 The subspace \\(S \\subset \\mathbb{R}^5\\) is given by \\[ \\mathsf{S} = \\mathsf{span} \\left( \\begin{bmatrix}1\\\\ 1\\\\ 0\\\\ -1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\ 1\\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 3\\\\ 1\\\\ -2\\\\ -5\\\\ 4 \\end{bmatrix}, \\begin{bmatrix} 1\\\\ 0\\\\ 1\\\\ 0\\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 2\\\\ -1\\\\ -1\\\\ -3\\\\ 1 \\end{bmatrix}, \\right)\\] Use the following matrix to find a basis for \\(S\\). What is the dimension of \\(S\\)? \\[ A=\\left[ \\begin{array}{ccccc} 1 &amp; 0 &amp; 3 &amp; 1 &amp; 2 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 &amp; -1 \\\\ -1 &amp; 1 &amp; -5 &amp; 0 &amp; -3 \\\\ 2 &amp; 1 &amp; 4 &amp; 1 &amp; 1 \\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{ccccc} 1 &amp; 0 &amp; 3 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -2 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] Find a basis for \\(\\mathrm{Nul}(A)\\). What is the dimension of this nullspace? 28.2.11 A \\(6 \\times 8\\) matrix \\(A\\) contains 5 pivots. For each of \\(\\mathrm{Col}(A)\\) and \\(\\mathrm{Nul}(A)\\), Determine the dimension of the subspace, Indicate whether it is subspace of \\(\\mathbb{R}^6\\) or \\(\\mathbb{R}^8\\), and Decide how you would find a basis of the subspace. 28.3 Solutions to Practice Problems 28.3.1 This is a linear transformation with \\[A = \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}.\\] This is not a linear transformation because, for example, \\[ 2 \\, T \\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 2 \\\\ 2 \\end{bmatrix} \\quad \\mbox{while} \\quad T \\left( \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 6 \\\\ 4 \\\\ 2 \\end{bmatrix}. \\] \\(A= \\begin{bmatrix} 1/2 &amp; 1/2 \\\\ 1/2 &amp; 1/2 \\end{bmatrix}\\) \\(A= \\begin{bmatrix} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{bmatrix}\\) 28.3.2 The inverse is \\[ \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ -5/2 &amp; 1/2 &amp;1 \\\\ -2 &amp; 0 &amp; 1 \\end{bmatrix} \\] 28.3.3 No \\(T\\) cannot be an onto mapping by the Invertible Matrix Theorem. Since \\(T\\) is not one-to-one, the mapping cannot be onto. 28.3.4 True Since \\(U\\) and \\(W\\) are subspaces, we know that \\(\\mathbb{0} \\in U\\) and \\(\\mathbb{0} \\in W\\). Therefore \\(\\mathbb{0} \\in U \\cap W\\). Let \\(\\mathsf{v}_1 \\in U \\cap W\\) and \\(\\mathsf{v}_2 \\in U \\cap W\\). We know that \\(\\mathsf{v}_1 \\in U\\) and \\(\\mathsf{v}_2 \\in U\\). Since \\(U\\) is a subspace, we have \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in U\\). We know that \\(\\mathsf{v}_1 \\in W\\) and \\(\\mathsf{v}_2 \\in W\\). Since \\(W\\) is a subspace, we have \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in W\\). Therefore \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in U \\cap W\\). Let \\(\\mathsf{v} \\in U \\cap W\\) and \\(c \\in \\mathbb{R}\\). We know that \\(\\mathsf{v} \\in U\\) and \\(c \\in R\\). Since \\(U\\) is a subspace, we have \\(c \\mathsf{v} \\in U\\). We know that \\(\\mathsf{v} \\in W\\) and \\(c \\in R\\). Since \\(W\\) is a subspace, we have \\(c \\mathsf{v} \\in W\\). Therefore \\(c \\mathsf{v} \\in U \\cap W\\). False. Here is an example that shows this is not always true. Let \\(V= \\mathbb{R}^2\\), \\(U = \\{ { x \\choose 0} \\mid x \\in \\mathbb{R} \\}\\) and \\(W= \\{ { 0 \\choose y} \\mid y \\in \\mathbb{R} \\}\\). The set \\(U \\cup W\\) is not closed under addition. For example, \\({1 \\choose 0} + {0 \\choose 1} = { 1 \\choose 1} \\notin U \\cup W\\). True. Since \\(U\\) and \\(W\\) are subspaces, we know that \\(\\mathbb{0} \\in U\\) and \\(\\mathbb{0} \\in W\\). Therefore \\(\\mathbb{0} = \\mathbb{0} + \\mathbb{0} \\in U + W\\). Let \\(\\mathsf{u}_1 + \\mathsf{w}_1 \\in U + W\\) and \\(\\mathsf{u}_1 + \\mathsf{w}_2 \\in U + W\\), where \\(\\mathsf{u}_1, \\mathsf{u}_2 \\in U\\) and \\(\\mathsf{w}_1, \\mathsf{w}_2 \\in W\\). Then \\[ (\\mathsf{u}_1 + \\mathsf{w}_1) + (\\mathsf{u}_2 + \\mathsf{w}_2) = (\\mathsf{u}_1 + \\mathsf{u}_2) + (\\mathsf{w}_1 + \\mathsf{w}_2) \\] and \\(\\mathsf{u}_3 = (\\mathsf{u}_1 + \\mathsf{u}_2) \\in U\\) (because \\(U\\) is a subspace) and \\(\\mathsf{w}_3 = (\\mathsf{w}_1 + \\mathsf{w}_2) \\in W\\) (because \\(W\\) is a subspace). Therefore \\((\\mathsf{u}_1 + \\mathsf{v}_1) + (\\mathsf{u}_2 + \\mathsf{w}_2) = \\mathsf{u}_3 + \\mathsf{w}_3 \\in U + W\\). Let \\(\\mathsf{u} + \\mathsf{w} \\in U + W\\) and \\(c \\in \\mathbb{R}\\). Then \\(c(\\mathsf{u} + \\mathsf{w}) = c \\mathsf{u} + c \\mathsf{w}\\). We know that \\(c \\mathsf{u} \\in U\\) (since \\(U\\) is a subspace) and \\(c \\mathsf{w} \\in W\\) (since \\(W\\) is a subspace). Therefore \\(c(\\mathsf{u} + \\mathsf{w}) = c \\mathsf{u} + c \\mathsf{w} \\in U+W\\). 28.3.5 Suppose that \\(c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_3) = 0\\). We must show that \\(c_1 = c_2 = c_3 = 0\\). Since \\(T\\) is a linear transformation, this means that \\(T(c_1 \\mathsf{v}_1+ c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_3 )= 0\\). Since \\(T\\) is one-to-one and \\(T(\\mathbf{0}) = \\mathbf{0}\\), we must have \\(c_1 \\mathsf{v}_1+ c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_3 = \\mathbf{0}\\). Because \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) are linearly independent, this means that \\(c_1 = c_2 = c_3 = 0\\). This proves that \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) are linearly independent. Given \\(\\mathsf{w} \\in W\\). We must show that there exist constants \\(c_1, c_2, c_3\\) such that \\(\\mathsf{w} = c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_3)\\). Here we go! Since \\(T\\) is onto, we know that there exists \\(\\mathsf{v} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{v}) = \\mathsf{w}\\). Since \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) span \\(\\mathbb{R}^n\\), we know that there exist constants \\(c_1, c_2, c_3\\) such that \\(\\mathsf{v}= c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_k\\) Therefore \\[ \\mathsf{w} = T(\\mathsf{v})= T(c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_k) = c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_k) \\] because \\(T\\) is a linear transformation. This proves that \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) span \\(W\\). 28.3.6 A basis for \\(\\mathrm{Col}(A)\\) is \\[ \\begin{bmatrix} 1 \\\\1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\1 \\\\ -2 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\0 \\\\ 1 \\\\ -1 \\end{bmatrix} \\] and a basis for \\(\\mathrm{Nul}(A)\\) is \\[ \\begin{bmatrix} -2 \\\\1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} -2 \\\\0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\0 \\\\ 1\\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] 28.3.7 Using RStudio we find: ## [,1] [,2] [,3] [,4] ## [1,] 1 5 2 -4 ## [2,] 3 10 2 8 ## [3,] 4 15 4 4 ## [,1] [,2] [,3] [,4] ## [1,] 1 0 -2.0 16 ## [2,] 0 1 0.8 -4 ## [3,] 0 0 0.0 0 A basis for \\(\\mathrm{Col}(A)\\) is \\[ \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\quad \\begin{bmatrix} 5 \\\\ 10 \\\\ 15 \\end{bmatrix}. \\] A basis for \\(\\mathrm{Nul}(A)\\) is \\[ \\begin{bmatrix} 2 \\\\ -0.8 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} -16 \\\\ 4 \\\\ 0 \\\\ 1 \\end{bmatrix}. \\] 28.3.8 A = cbind(c(1,-1,-2),c(2,-1,1),c(-1,-1,-8)) A ## [,1] [,2] [,3] ## [1,] 1 2 -1 ## [2,] -1 -1 -1 ## [3,] -2 1 -8 rref(A) ## [,1] [,2] [,3] ## [1,] 1 0 3 ## [2,] 0 1 -2 ## [3,] 0 0 0 No they are not a basis. The corresponding matrix only has two pivots. Let’s add the three elementary vectors to create matrix \\(B\\) and then row reduce. B = cbind(A, c(1,0,0),c(0,1,0),c(0,0,1)) B ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 -1 1 0 0 ## [2,] -1 -1 -1 0 1 0 ## [3,] -2 1 -8 0 0 1 rref(B) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 3 0 -0.3333333 -0.3333333 ## [2,] 0 1 -2 0 -0.6666667 0.3333333 ## [3,] 0 0 0 1 1.6666667 -0.3333333 From this matrix, we can see that the vectors \\[ \\begin{bmatrix} 1 \\\\ -1 \\\\ -2 \\end{bmatrix}, \\quad \\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}. \\] are linearly independent because they correspond to the basis columns of \\(B\\). 28.3.9 We use the change of basis matrix. \\[ P_{\\cal B} = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Then, the desired coordinate vectors are \\[ \\mathsf{w} = \\begin{bmatrix} 0 \\\\ -3 \\\\ -1 \\\\ -1 \\end{bmatrix}_{\\mathcal{S}}, \\qquad \\mathsf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}_{\\mathcal{B}} \\] You can find these vectors by multiplying by \\(P_\\mathcal{B}\\) and by augmenting and row reducing as seen here. A = cbind(c(1,0,0,0),c(1,1,0,0),c(1,1,1,0),c(1,1,1,1)) w = c(3,-2,0,-1) v = c(10,9,7,4) A %*% w ## [,1] ## [1,] 0 ## [2,] -3 ## [3,] -1 ## [4,] -1 Av = cbind(A,v) rref(Av) ## v ## [1,] 1 0 0 0 1 ## [2,] 0 1 0 0 2 ## [3,] 0 0 1 0 3 ## [4,] 0 0 0 1 4 Or we can use the inverse of \\(P_\\mathcal{B}\\). \\[ P_{\\cal B}^{-1} = \\begin{bmatrix} 1 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp;-1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Ainv = solve(A) Ainv %*% v ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 28.3.10 \\(\\dim(S) = 3\\) and a basis for \\(S\\) is \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ -1 \\\\2 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\1 \\end{bmatrix}. \\] \\(\\dim(\\mathrm{Nul}(A))=2\\) and a basis is \\[ \\begin{bmatrix} -3 \\\\ 2 \\\\ 1 \\\\ 0 \\\\0\\end{bmatrix}, \\quad \\begin{bmatrix} -1 \\\\ 2 \\\\ 0 \\\\ -1 \\\\1 \\end{bmatrix}. \\] 28.3.11 \\(\\mathrm{Col}(A)\\) has dimension 5, and it is a subspace of \\(\\mathbb{R}^6\\). You would find a basis by taking the pivot columns of \\(A\\). \\(\\mathrm{Nul}(A)\\) has dimension 3, and it is a subspace of \\(\\mathbb{R}^8\\). You would find a basis by finding the parametric solution to \\(A \\mathsf{x}= \\mathbb{0}\\). "],["quiz-3-review.html", "Section 29 Quiz 3 Review 29.1 Overview 29.2 Practice Problems 29.3 Solutions to Practice Problems", " Section 29 Quiz 3 Review 29.1 Overview Our third quiz covers sections 5.1-5.3 and 5.5-5.6 in Lay’s book. This corresponds to Problem Set 6 and the corresponding Edfinity questions. The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Look over the Edfinity homework assingments Redo the Jamboard problems. Look at Jamboard problems that your group didn’t get to. Try to resolve the Problem Sets and compare your answers to the solutions. Do the practice problems below. Compare your answers to the solutions. 29.1.1 Vocabulary, Concepts and Skills See the Week 5-6 Learning Goals for the list of vocabulary, concepts and skills. 29.2 Practice Problems 29.2.1 Consider the \\(3 \\times 3\\) matrix \\[ A = \\left[ \\begin{array}{rrr} 2 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -2 &amp; 5 &amp; -2 \\\\ \\end{array} \\right] \\] with characteristic equation \\[ p(\\lambda) = -(\\lambda -1)(\\lambda -2)(\\lambda +2). \\] Find the eigenvalues and corresponding eigenvectors for \\(A\\). 29.2.2 Let \\(A\\) be a \\(2 \\times 2\\) matrix. We view \\(A\\) as a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). Describe the eigenvalues for each of the following types of matrices. \\(A\\) maps all of \\(\\mathbb{R}^2\\) onto a line through the origin in \\(\\mathbb{R}^2\\). \\(A\\) is a reflection of \\(\\mathbb{R}^2\\) about a line through the origin \\(A\\) is a reflection of \\(\\mathbb{R}^2\\) through the origin \\(A\\) is a horizontal shear 29.2.3 Below are the eigenvalues of four different \\(5 \\times 5\\) matrices. For each, decide if the matrix is invertible and if it is diagonalizable. Answer Yes, No or “Not enough information to determine this.” \\(A\\) has eigenvalues \\(\\lambda = -4, -3,0,1, 2\\) \\(B\\) has eigenvalues \\(\\lambda = -3, -1, 1, \\sqrt{2}, 8.\\) \\(C\\) has eigenvalues \\(\\lambda = 1, 2, 2, 7, 8.\\) \\(D\\) has eigenvalues \\(\\lambda = -1, 0, 3,3, 10\\) 29.2.4 Here the diagonalization of a matrix: \\[ \\mathsf{A}=\\left[ \\begin{array}{ccc} 5 &amp; 2 &amp; -1 \\\\ 2 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 \\\\ \\end{array} \\right] = \\left[ \\begin{array}{ccc} -5 &amp; 0 &amp; 1 \\\\ -2 &amp; 1 &amp; -2 \\\\ 1 &amp; 2 &amp; 1 \\\\ \\end{array} \\right] \\left[ \\begin{array}{ccc} 6 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\left[ \\begin{array}{ccc} -\\frac{1}{6} &amp; -\\frac{1}{15} &amp; \\frac{1}{30} \\\\ 0 &amp; \\frac{1}{5} &amp; \\frac{2}{5} \\\\ \\frac{1}{6} &amp; -\\frac{1}{3} &amp; \\frac{1}{6} \\\\ \\end{array} \\right]. \\] Is the matrix \\(\\mathsf{A}\\) invertible? Find a nonzero vector in \\(\\mathrm{Nul}(\\mathsf{A})\\) if one exists. Find a steady-state vector \\(\\mathsf{v}\\) such that \\(\\mathsf{A} \\mathsf{v} = \\mathsf{v}\\) if one exists. Give the coordinates of \\(\\mathsf{v} = [1,2,3]^T\\) in the eigenbasis without row reductions. Find a formula for \\(\\mathsf{A}^{2020} \\mathsf{v}\\) if \\(\\mathsf{v} = [1,2,3]^T\\) in terms of the eigenbasis. 29.2.5 The eigensystem of matrix \\(A\\) is given below. It has complex eigenvalues. What angle does it rotate by? What factor does it scale by? \\[ \\begin{bmatrix} 3 &amp; -5 \\\\ 1 &amp; -1 \\end{bmatrix}, \\qquad \\lambda = 1 \\pm i, \\qquad v = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\pm \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} i. \\] 29.2.6 Using the matrix \\(B = = \\begin{bmatrix} .97 &amp; -.71 \\\\ .71 &amp; .97 \\end{bmatrix}\\) and the starting vector \\(\\mathsf{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), I plotted the points \\[\\mathsf{v}, B \\mathsf{v}, B^2\\mathsf{v}, B^3 \\mathsf{v}, \\ldots.\\] I saw that these points are, roughly, going around in a circle. How many multiplications by \\(B\\) does it take to get back around to the positive \\(x\\)-axis? When I come full circle, am I closer to the origin, farther from the origin, or the same distance to the origin? 29.2.7 For each matrix below, decide if it is diagonalizable. You do not need to diagonalize the matrix (though you can!), but you must give a reason for why the matrix is or is not diagonalizable. \\(A = \\begin{bmatrix} 0 &amp; -4 &amp; 2 \\\\ 2 &amp; -4 &amp; -1 \\\\ -6 &amp; 4 &amp; 7 \\end{bmatrix}\\) has eigenvalues \\(4, -1, 0\\). \\(B = \\begin{bmatrix} 3 &amp; -1 &amp; 2 \\\\ -1 &amp; 3 &amp; 2 \\\\ 2&amp;2 &amp; 0 \\end{bmatrix}\\) has eigenvalues \\(4,4,-2\\). 29.2.8 Consider the matrix with eigenvalues and eigenvectors \\[ A = \\begin{bmatrix} 0.7 &amp; 0.2 \\\\ 0.3 &amp; 0.8 \\end{bmatrix} \\qquad \\begin{array}{cc} \\lambda_1 = 1 &amp; \\lambda_2 = .5 \\\\ \\mathsf{v}_1 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} &amp; \\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\end{array} \\] Diagonalize \\(A\\). What can you say about \\(\\displaystyle{\\lim_{n \\to \\infty}} A^n\\)? Give a formula for \\(A^n \\mathsf{x}_0\\) if \\(\\mathsf{x}_0 = \\begin{bmatrix} 25 \\\\ 0 \\end{bmatrix}\\) in terms of the eigenbasis. What is \\(\\displaystyle{\\lim_{n \\to \\infty}} A^n \\begin{bmatrix} 25 \\\\ 0 \\end{bmatrix}\\)? 29.2.9 The matrix \\(A\\) below has the given eigenvalues and eigenvectors. \\[ A = \\left[ \\begin{array}{cc} \\frac{1}{2} &amp; \\frac{1}{5} \\\\ -\\frac{2}{5} &amp; \\frac{9}{10} \\\\ \\end{array} \\right] \\qquad \\begin{array}{c} \\lambda = .7 \\pm .2 i \\\\ \\mathsf{v} = \\begin{bmatrix} \\frac{1}{2} \\\\ 1 \\end{bmatrix} \\pm \\begin{bmatrix} -\\frac{1}{2} \\\\ 0 \\end{bmatrix} i \\end{array}\\hskip5in \\] Factor \\(A=PCP^{-1}\\) where \\(C\\) is a rotation-scaling matrix. What is the angle of rotation? What is the factor of dilation? 29.2.10 In a 1962 study of rainfall in Tel Aviv, it was determined that if today is a wet day, then the probability that tomorrow will be wet is 0.662 and the probability that tomorrow it will be dry is 0.338. If today is a dry day, then the probability that tomorrow is wet is 0.250 and the probability that tomorrow is dry will be 0.75. From this I computed the following: \\[ A = \\begin{bmatrix} 0.662 &amp; 0.25 \\\\ 0.338 &amp; 0.75\\end{bmatrix}; \\qquad \\begin{array}{cc} \\lambda_1 = 1.0 &amp; \\lambda_2 = 0.412 \\\\ \\mathsf{v}_1 = \\begin{bmatrix}-0.595 \\\\ -0.804 \\end{bmatrix} &amp; \\quad \\mathsf{v}_2 = \\begin{bmatrix}-0.707\\\\ 0.707 \\end{bmatrix} \\end{array} \\] If Monday is a dry day, what is the probability that Wednesday will be wet? In the long-run, what is the distribution of wet and dry days? 29.2.11 A population of female bison is split into three groups: juveniles who are less than one year old; yearlings between one and two years old; and adults who are older than two years. Each year, * 80% of the juveniles survive to become yearlings. * 90% of the yearlings survive to become adults. * 80% of the adults survive. * 40% of the adults give birth to a juvenile Let \\(\\mathsf{x}_t = \\begin{bmatrix} J_t \\\\ Y_t \\\\ A_t \\end{bmatrix}\\) be the state of the system in year \\(t\\). Find the Leslie matrix \\(L\\) such that \\(\\mathsf{x}_{t+1} = B \\mathsf{x}_t.\\). Find the eigenvalues of \\(L\\). The matrix \\(L\\) has two complex eigenvalues and one real eigenvalue. How do the complex eigenvectors manifest in the trajectory of a population? What is the long-term behavior of the herd? Will the size of the herd grow, stablilize or shrink? What will be the proportions of juveniles, yearlings and adults in the herd? 29.2.12 Let \\(A\\) and \\(B\\) be \\(n \\times n\\) matrices. Suppose that \\(v\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) and \\(v\\) is an eigenvector of \\(B\\) with eigenvalue \\(\\mu\\) such that \\(\\lambda \\not= \\mu\\). Is \\(v\\) an eigenvector of either of the matrices below? If so give its eigenvalue. \\(A + B\\) \\(AB\\) 29.2.13 Suppose that \\(A\\) is invertible. Show that if \\(v\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\), then \\(v\\) is an eigenvector of \\(A^{-1}\\) with eigenvalue \\(1/\\lambda\\). If \\(A\\) is diagonalizable with diagonalization \\(A = P D P^{-1}\\), then show that \\(A^{-1}\\) is diagonalizable and find its diagonalization from that of \\(A\\). 29.2.14 Suppose that \\(A\\) is an \\(n \\times n\\) matrix with eigenvector \\(\\vec w\\) of eigenvalue 5 and eigenvector \\(\\vec v\\) of eigenvalue -3. Is \\(\\vec v + \\vec w\\) an eigenvector of \\(A\\), and if so, what is its eigenvalue? Is \\(2021 \\vec v\\) an eigenvector of \\(A\\), and if so what is its eigenvalue? Is \\(\\vec w\\) an eigenvector of \\(A^2\\), and if so what is its eigenvalue? Is \\(\\vec v\\) an eigenvector of \\(A - 2021 I_n\\) and if so, what is its eigenvalue? 29.3 Solutions to Practice Problems 29.3.1 There are three eigenvalues: 1, 2, and \\(-2\\). We find an eigenvector for each of them. * Eigenvalue \\(\\lambda = 1\\) \\[ A - I = \\left[ \\begin{array}{rrr} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ -2 &amp; 5 &amp; -3 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 3 &amp; -3 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] So one eigenvector is \\([1,1,1]^{\\top}\\) Eigenvalue \\(\\lambda = 2\\) \\[ A - 2I = \\left[ \\begin{array}{rrr} 0 &amp; -1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\\\ -2 &amp; 5 &amp; -4 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} -2 &amp; 5 &amp; -4 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} -2 &amp; 0 &amp; -4 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] So one eigenvector is \\([-2,0,1]^{\\top}\\) Eigenvalue \\(\\lambda = -2\\) \\[ A - 2I = \\left[ \\begin{array}{rrr} 4 &amp; -1 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ -2 &amp; 5 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 4 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -2 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] So one eigenvector is \\([0,0,1]^{\\top}\\) 29.3.2 In this problem, we are to think about the geometry of a 2D transformation, and see if we can find any vectors which get re-scaled by the transformation. The direction of these vectors cannot change (other than to flip to the opposite direction). This maps all of \\(\\mathbb{R}^2\\) to a line. Therefore it is not one-to-one, nor onto, and so it is not invertible. This means that \\(\\lambda = 0\\) is an eigenvalue Any vector that is already on the line must stay on the line, so it is an eigenvector, but we don’t know its eigenvalue. Thus, the eigenvalues are \\(\\lambda_1 = 0\\) and \\(\\lambda_2\\) we don’t know. There are two kinds of eigenvectors. Those vectors on the line are fixed, so they are eigenvectors of eigenvalue 1. Vectors that are perpendicular to the line get sent to their negatives, so they are eigenvectors of eigenvalue \\(-1\\). Thus, the eigenvalues are \\(\\lambda_1 = 1\\) and \\(\\lambda_2=-1\\). Perhaps this one is easiest to understand by looking at a particular example. Let’s look at a reflection across the \\(y\\)-axis. This means that \\(A [ x, y]^{\\top} = [-x, y]^{\\top}\\). In particular, we have \\(A [ 1, 0]^{\\top} = [ -x, 0]^{\\top} = - [ x, 0]^{\\top}\\). So \\(-1\\) is an eigenvalue. Meanwhile, we also have \\(A [ 0, 1]^{\\top} = [ 0, 1]^{\\top}\\) So 1 is an eigenvalue. In this transformation, every vector gets sent to its negative. \\[ T\\left( \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\right) = \\begin{bmatrix} -x_1 \\\\ -x_2 \\end{bmatrix} = \\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\] This means that every vector is an eigenvector of eigenvalue \\(-1\\). The eigenvalues are \\(\\lambda_1 = \\lambda_2=-1\\). A horizontal shear (we did not talk about these very much) has a matrix of the form \\[ \\begin{bmatrix} 1 &amp; a \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x + a y \\\\ y \\end{bmatrix} \\] It fixes the \\(x\\)-axis since \\((x,0)^\\top\\) maps to \\((x,0)^\\top\\), but no other directions are fixed. You can see by the fact that the matrix is upper triangluar that the eigenvalues are on the diagonal and are \\(\\lambda_1 = \\lambda_2 = 1\\). Note: if you calculate, you find that the geometric multiplicity of \\(\\lambda = 1\\) is 1 (only the \\(x\\)-axis), and this matrix is not diagonalizable. The only eigenspace is the \\(x\\)-axis. 29.3.3 \\(A\\) is not invertible because \\(0\\) is an eigenvalue. \\(A\\) is diagonalizable because it have 5 distinct eigenvalues. \\(B\\) is invertible because \\(0\\) is not an eigenvalue. \\(B\\) is diagonalizable because it have 5 distinct eigenvalues. \\(C\\) is invertible because \\(0\\) is not an eigenvalue. We cannot tell whether \\(C\\) is diagonalizable without more information. The eigenvalue \\(\\lambda=2\\) has algebraic multiplicity 2. We need to know whether the geometric multiplicity is 1 or 2. \\(D\\) is not invertible because \\(0\\) is an eigenvalue. We cannot tell whether \\(D\\) is diagonalizable without more information. The eigenvalue \\(\\lambda=3\\) has algebraic multiplicity 2. We need to know whether the geometric multiplicity is 1 or 2. 29.3.4 No, \\(A\\) is not invertible because \\(0\\) is an eigenvalue. \\(\\mathsf{v} = [1, -2, 1]^{\\top}\\) is an eigenvector for \\(\\lambda=0\\). Therefore \\(\\mathsf{v} \\in \\mbox{Nul}(A)\\). The vector \\(\\mathsf{v} = [0,1,2]^{\\top}\\) is an eigenvector for \\(\\lambda=1\\). So this is a steady-state vector. (However, the dynamical system will not converge to this steady state because \\(\\lambda=6\\) is the dominant eigenvalue.) When \\(A=P D P^{-1}\\), we can find the coordinates of a vector with respect to the eigenbasis via multiplication by \\(P^{-1}\\). Pinv =cbind(c(-1/6,0,1/6),c(-1/15,1/5,-1/3),c(1/30,2/5,1/6)) v = c(1,2,3) Pinv %*% v ## [,1] ## [1,] -0.2 ## [2,] 1.6 ## [3,] 0.0 So \\([ \\mathsf{v}]_{\\mathcal{B}} = [-1/5, 8/5, 0]^{\\top}\\). \\(-\\frac{1}{5} \\cdot 6^{2020} \\cdot \\begin{bmatrix} -5 \\\\ -2 \\\\ 1 \\end{bmatrix} + \\frac{8}{5} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}\\) 29.3.5 This system scales by \\(\\sqrt{1+1} = \\sqrt{2}\\) and it rotates by \\(\\tan^{-1} (1/1) = \\pi/4\\). 29.3.6 We have \\[ \\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix} = \\begin{bmatrix} .97 &amp; -.71\\\\ .71 &amp; .97 \\end{bmatrix} \\] Let’s turn to RStudio a = .97 b = .71 scale = sqrt(a^2+b^2) angle = atan (b/a) scale ## [1] 1.202082 angle ## [1] 0.6318544 2 * pi / angle ## [1] 9.94404 It takes 10 iterations to rotate past the \\(x\\)-axis. We are further from the origin because \\(| \\lambda| \\approx 1.2 &gt; 1\\). 29.3.7 The matrix \\(A\\) is diagonalizable because it has 3 distinct eigenvalues We must see whether \\(\\lambda=4\\) has geometric multiplicty 2 (to match its algebraic multiplicity). rref( cbind(c(-1,-1,2), c(-1,-1,2), c(2,2,-4))) ## [,1] [,2] [,3] ## [1,] 1 1 -2 ## [2,] 0 0 0 ## [3,] 0 0 0 We see that \\(B - 4I\\) has two free columns, so \\(\\dim ( \\mbox{Nul}(B-4I))=2\\). This means that \\(\\lambda=4\\) has geometric multiplicity 2. Therefore \\(B\\) is diagonalizable. 29.3.8 We set \\(P = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; -1 \\end{bmatrix}\\). So \\[ P^{-1} = - \\frac{1}{5} \\begin{bmatrix} -1 &amp; -1 \\\\ -3 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 0.2 &amp; 0.2 \\\\ 0.6 &amp; -0.4 \\end{bmatrix} \\] Or we can find this inverse using RStudio. A = cbind(c(2,3),c(1,-1)) solve(A) ## [,1] [,2] ## [1,] 0.2 0.2 ## [2,] 0.6 -0.4 Therefore \\[ A = \\begin{bmatrix} 0.7 &amp; 0.2 \\\\ 0.3 &amp; 0.8 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0.5 \\end{bmatrix} \\begin{bmatrix} 0.2 &amp; 0.2 \\\\ 0.6 &amp; -0.4 \\end{bmatrix} \\] \\(\\lim_{n \\rightarrow \\infty} A^n = \\begin{bmatrix} 0.4 &amp; 0.6 \\\\ 0.4 &amp; 0.6 \\end{bmatrix}\\) because \\(\\lambda=1\\) is the dominant eigenvalue. So each column of \\(A\\) convergence to vector in the eigenspace for the dominant eigenvalue. Basically, we treat each column of \\(A\\) as a “starting vector” repeated multiplication by \\(A\\) converges to the dominant eigenspace. Here are the details. Let \\(\\mathsf{v}_1 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\) and \\(\\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\). Let’s see why we can treat each column \\(\\mathsf{a}_1, \\mathsf{a}_2\\) of \\(A\\) as a “starting vector.” We can choose to view \\[A^2 = A \\begin{bmatrix} \\mathsf{a}_1 &amp; \\mathsf{a}_2 \\end{bmatrix} = \\begin{bmatrix} A \\mathsf{a}_1 &amp; A \\mathsf{a}_2 \\end{bmatrix} \\quad \\mbox{and} \\quad A^3 = A \\begin{bmatrix} A \\mathsf{a}_1 &amp; A \\mathsf{v}_2 \\end{bmatrix} = \\begin{bmatrix} A^{2} \\mathsf{a}_1 &amp; A^{2} \\mathsf{v}_2 \\end{bmatrix} \\] and in general \\[ A^{n} = \\begin{bmatrix} A^{n-1} \\mathsf{a}_1 &amp; A^{n-1} \\mathsf{a}_2 \\end{bmatrix}. \\] Therefore \\[ \\lim_{n \\rightarrow \\infty} A^n = \\lim_{n \\rightarrow \\infty} \\begin{bmatrix} A^{n-1} \\mathsf{a}_1 &amp; A^{n-1} \\mathsf{a}_2 \\end{bmatrix}. \\] So we need to write the columns of \\(A\\) as linear combinations of the eigenvectors \\(\\mathsf{v}_1, \\mathsf{v}_2\\). For any constants \\(c_1, c_2\\), we have \\[ \\lim_{n \\rightarrow \\infty} A^n (c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2) = \\lim_{n \\rightarrow \\infty} \\left( c_1 A^n \\mathsf{v}_1 + c_2 A^n \\mathsf{v}_2 \\right) = \\lim_{n \\rightarrow \\infty} \\left( c_1 \\mathsf{v}_1 + c_2 (0.5)^n \\mathsf{v}_2 \\right) = c_1 \\mathsf{v}_1. \\] So we need to find \\(c_1,c_2\\) for each column of \\(A\\). Let’s use R Studio. a1 = c(.7,.3) a2 = c(.2,.8) P = cbind(c(2,3), c(1,-1)) solve(P,a1) ## [1] 0.2 0.3 solve(P,a2) ## [1] 0.2 -0.2 In each case, \\(c_1 = 0.2\\). Therefore \\[ \\lim_{n \\rightarrow \\infty} A^n = \\begin{bmatrix} 0.4 &amp; 0.4 \\\\ 0.6 &amp; 0.6 \\end{bmatrix}. \\] We need to find the coefficients for \\(x_0 = [25, 0]^{\\top}\\). P = cbind(c(2,3), c(1,-1)) v = c(25,0) solve(P,v) ## [1] 5 15 So the formula is \\[ 5 \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} + 15 \\left( \\frac{1}{2} \\right)^n \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\] This converges to \\(5 \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\). 29.3.9 We have \\[ A = \\left[ \\begin{array}{cc} \\frac{1}{2} &amp; \\frac{1}{5} \\\\ -\\frac{2}{5} &amp; \\frac{9}{10} \\\\ \\end{array} \\right] = \\begin{bmatrix} -1/2 &amp; 1/2 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 0.7 &amp; -0.2 \\\\ 0.2 &amp; 0.7 \\end{bmatrix} \\begin{bmatrix} -2 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} \\] Here are some R calculations to check the answer for (a) and to find the values for (b) and (c). A = cbind(c(1/2,-2/5), c(1/5,9/10)) A ## [,1] [,2] ## [1,] 0.5 0.2 ## [2,] -0.4 0.9 eigen(A) ## eigen() decomposition ## $values ## [1] 0.7+0.2i 0.7-0.2i ## ## $vectors ## [,1] [,2] ## [1,] 0.4082483-0.4082483i 0.4082483+0.4082483i ## [2,] 0.8164966+0.0000000i 0.8164966+0.0000000i P = cbind(c(-1/2,0),c(1/2,1)) C = cbind(c(.7,.2),c(-.2,.7)) Pinv = solve(P) Pinv ## [,1] [,2] ## [1,] -2 1 ## [2,] 0 1 P %*% C %*% Pinv ## [,1] [,2] ## [1,] 0.5 0.2 ## [2,] -0.4 0.9 atan(.2/.7) ## [1] 0.2782997 sqrt(.7^2 + .2^2) ## [1] 0.728011 The angle of rotation is \\(\\tan^{-1} (.2/.7) = 0.278\\) radians The dilation factor is \\(\\sqrt{0.49 + 0.04} = \\sqrt{0.53} = 0.728\\). 29.3.10 Let’s use RStudio. A = cbind(c(0.662, 0.338),c(0.25, 0.75)) A %*% A %*% c(0,1) ## [,1] ## [1,] 0.353 ## [2,] 0.647 v1 = c(-0.595, -0.804 ) v1/sum(v1) ## [1] 0.4253038 0.5746962 If Monday is dry, then the probability of a wet Wednesday is \\(0.353\\). The easiest way to calculate this \\(A^2 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\\) In the long run, \\(42.5\\%\\) of days are wet and \\(57.5\\%\\) of days are dry. 29.3.11 Here is the Leslie matrix, as well as some eigensystem computations. L = cbind(c(0,.8,0),c(0,0,.9),c(.4,0,.8)) L ## [,1] [,2] [,3] ## [1,] 0.0 0.0 0.4 ## [2,] 0.8 0.0 0.0 ## [3,] 0.0 0.9 0.8 vecs = eigen(L)$vectors v = vecs[,1] Re(v/sum(v)) # get it to sum to 1 AND remove the 0 imaginary part ## [1] 0.2272578 0.1719172 0.6008250 The eigenvalues are \\(1.058, -0.129 \\pm 0.506 i\\). If we start outside of the span of the dominant eigenvalue, then the trajectory will wiggle with a mild oscillation with an overall growth trend of \\(1.058\\), or \\(5.8\\%\\). The size of the herd grows. The proportions are \\([0.227, 0.172, 0.601]\\). 29.3.12 \\((A + B) v = A v + B v = \\lambda v + \\mu v = (\\lambda + \\mu) v\\), so yes, \\(v\\) is an eigenvector of \\(A+B\\) of eigenvalue \\(\\lambda + \\mu\\). \\(A B v = A (B v) = A (\\mu v) = \\mu (A v) = \\mu \\lambda v\\), so yes, \\(v\\) is an eigenvector of \\(AB\\) of eigenvalue \\(\\lambda\\mu\\). 29.3.13 We are given \\(A v = \\lambda v\\). Thus, \\[ \\begin{array}{cccl} A v &amp; = &amp; \\lambda v &amp; \\text{given} \\\\ A^{-1} A v &amp; = &amp; \\lambda A^{-1} v &amp; \\text{multiply on the left by $A^{-1}$} \\\\ v &amp; = &amp; \\lambda A^{-1} v \\\\ \\frac{1}{\\lambda} v &amp; = &amp; A^{-1} v \\\\ \\end{array} \\] This shows that \\(A^{-1} v = \\frac{1}{\\lambda} v\\) so \\(v\\) is an eigenvector of \\(A^{-1}\\) with eigenvalue \\(\\frac{1}{\\lambda}\\) (method 1) If \\(A\\) is diagonal, then there is a basis \\(\\{v_1, v_2, \\ldots, v_n\\}\\) of eigenvectors of \\(A\\) with eigenvalues \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\). By the previous part, \\(\\{v_1, v_2, \\ldots, v_n\\}\\) are eigenvectors of \\(A^{-1}\\) with eigenvalues \\(1/\\lambda_1, 1/\\lambda_2, \\ldots, 1/\\lambda_n\\). Thus \\(A^{-1}\\) has the same eigenbasis, and the diagonalization of \\(A^{-1}\\) is \\[ A^{-1} = \\underbrace{ \\begin{bmatrix} \\vert &amp;\\vert &amp;&amp;\\vert \\\\ v_1 &amp; v_2 &amp; \\cdots &amp; v_n \\\\ \\vert &amp;\\vert &amp;&amp;\\vert \\\\ \\end{bmatrix} }_P \\begin{bmatrix} 1/\\lambda_1 &amp; &amp; &amp; \\\\ &amp; 1/\\lambda_2 &amp; &amp; \\\\ &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; 1/\\lambda_n \\\\ \\end{bmatrix} P^{-1} \\] (method 2) If \\(A = P D P^{-1}\\) then by the fact that the order reverses when computing inverses (the shoes-and-socks property), we have \\(A^{-1} = (P D P^{-1})^{-1} = (P^{-1})^{-1} D^{-1} P^{-1} = P D^{-1} P^{-1}.\\) Furthermore \\(D^{-1}\\) is a diagonal matrix such that \\[ \\text{if} \\qquad D = \\begin{bmatrix} \\lambda_1 &amp; &amp; &amp; \\\\ &amp; \\lambda_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\lambda_n \\\\ \\end{bmatrix} \\qquad\\text{then}\\qquad D^{-1} = \\begin{bmatrix} 1/\\lambda_1 &amp; &amp; &amp; \\\\ &amp; 1/\\lambda_2 &amp; &amp; \\\\ &amp; &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; &amp; 1/\\lambda_n \\\\ \\end{bmatrix} \\] Note that \\(A\\) is invertible, 0 is not an eigenvalue, so each \\(1/\\lambda_i\\) does not cause division by 0. 29.3.14 We are given that \\(A w = 5 w\\) and \\(A v = -3 v\\). \\(A (v + w) = A v + A w = -3 v + 5 w \\not = \\lambda(v + w)\\) for any \\(\\lambda\\), so \\(v + w\\) is not an eigenvector of \\(A\\). Note: it would be if they had the same eigenvalue. \\(A (2021 v) = 2021 A v = 2021 (-3) v = (-3) (2021 v)\\) so \\(2021 v\\) is an eigenvector also of eigenvalue \\(-3\\). \\(A^2 w = A (A w) = A (5 w) = 5 (A w) = 5 (5 w) = 25 w\\), so \\(w\\) is an eigenvector of \\(A^2\\) of eigenvalue 25. \\((A - 20201I_n)v = A v - 2021 I_n v = -3 v - 2021 v = -2024 v\\), so \\(v\\) is an eigenvector of \\((A - 20201I_n)\\) of eigenvalue \\(-2024\\). "],["quiz-4-review.html", "Section 30 Quiz 4 Review 30.1 Overview 30.2 Vocabulary, Concepts and Skills 30.3 Comprehensive Review 30.4 Skills 30.5 Practice Problems 30.6 30.7 30.8 Watch this! 30.9 Solutions to Practice Problems", " Section 30 Quiz 4 Review 30.1 Overview Our fourth quiz has two goals It will cover the new material from Sections 6.1-6.5. It will have some comprehensive material. The comprehensive material will not be tricky. My goal is to be sure that you understand the fundamentals of the course. The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Look over the Edfinity homework assingments Do practice problems from the Edfinity Practice assignments. These allow you to “Practice Similar” by generating new variations of the same problem. Redo the Jamboard problems Try to resolve the Problem Sets and compare your answers to the solutions. Do the practice problems below. Compare your answers to the solutions. 30.2 Vocabulary, Concepts and Skills See the Week 7-8 Learning Goals for the list of vocabulary, concepts and skills. 30.3 Comprehensive Review Here are some terms and ideas that you should know: Pivot position. Elementary row operations. Ways to compute and think about \\(A v\\) in both words and symbols. Linear combination. \\(Span(v_1, . . . , v_k).\\) In particular, you should be able to visualize \\(Span(v)\\), \\(Span(u,v)\\) and give geometric interpretations of these sets in \\(\\mathbb{R}^2\\) or \\(\\mathbb{R}^3\\). Linearly independent and linearly dependent. Linear transformation. Domain, codomain, image, range, onto, and one-to-one. The transpose of a matrix, the inverse of a matrix, invertible matrix. Subspace. Null space, column space, and row space of a matrix. Kernel and range of a linear transformation. Basis and dimension. Rank. Eigenvalue, eigenvector, eigenspace. Characteristic polynomial and characteristic equation. Diagonalizable matrix. Dot product, length of a vector, angle between vectors, cosine similarity Orthogonal vectors, orthogonal spaces. Orthogonal complement of a subspace. Orthogonal basis, orthonormal basis, orthogonal matrix. Orthogonal projection, least squares solutions. 30.4 Skills Form an augmented matrix and reduce a matrix or augmented matrix into row echelon or reduced row echelon form. Determine whether a given matrix is in either of those forms. Determine whether a particular form of a matrix is a possible row echelon or reduced echelon form. Determine whether a system is consistent and if it has a unique solution. Write the general solution in parametric vector form. Describe the set of solutions geometrically. Interpret a system of equations as (i) a vector equation (ii) a matrix equation. Determine when a vector is in a subset spanned by specified vectors. Exhibit a vector as a linear combination of specified vectors. Determine whether a specified vector is in the range of a linear transformation. Determine whether the columns of an \\(m \\times n\\) matrix span \\(\\mathbb{R}^m\\). Determine whether the columns are linearly independent. Compute a matrix-vector product, and interpret it as a linear combination of the columns of \\(A\\). Use linearity of matrix multiplication to compute \\(A(u + v)\\) or \\(A(c u)\\). Find the matrix of a linear transformation. Determine whether a transformation is linear. Determine whether a linear transformation \\(T( x)=A x\\) is one-to-one or onto, using the properties of the matrix \\(A\\). Determine whether a subset of vectors is a subspace. Determine whether a set of vectors is linearly independent and whether it is a basis for some subspace or vector space. Find a basis for a vector space, or for the null space or column space of a matrix, or for the kernel or range of a linear transformation. Find the dimension of a vector space or subspace. Find the dimension of the null space (number of free variables) and column space (number of pivot columns) of a matrix. Find and interpret the rank of a matrix. Calculate the characteristic equation, eigenvalues, and eigenvectors of a square matrix. Find eigenvectors for a specific eigenvalue. Check if a vector is an eigenvector of a given matrix. Determine whether a square matrix is diagonalizable. Factor a diagonalizable matrix into \\(A=PDP^{-1}\\), where \\(D\\) is a diagonal matrix. Use eigenvalues and eigenvectors to analyze the long-term behavior of discrete dynamical systems. Find the length of a vector, the distance between two vectors, or the angle between two vectors. Determine whether a set of vectors are orthogonal. Determine whether a vector is orthogonal to a subspace or whether two subspaces are orthogonal, by checking whether their basis vectors are orthogonal. Find the orthogonal projection of (i) one vector onto another vector, or (ii) one vector onto a subspace (using an orthogonal basis for the subspace). Find the distance between a vector and a space (by computing the residual). Set up the matrix equation to find the ``best-fitting’’ function to a set of data using least squares. Interpret the normal equations \\(A^{\\top}A x = A^{\\top}b\\). Find the least squares approximation by solving the normal equations \\(\\hat{x}=(A^{\\top}A)^{-1}A^{\\top}b\\). 30.5 Practice Problems 30.5.1 Let \\(\\mathsf{v} = \\begin{bmatrix}1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathsf{w}= \\begin{bmatrix}5 \\\\ 2 \\\\ 3 \\end{bmatrix}\\). Find \\(\\| \\mathsf{v} \\|\\) and \\(\\| \\mathsf{w} \\|\\). Find the distance between \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\). Find the cosine of the angle between \\(\\mathsf{v}\\) and \\(\\mathsf{v}\\). Find \\(\\mbox{proj}_{\\mathsf{v}} \\mathsf{w}\\). Let \\(W=\\mbox{span} (\\mathsf{v}, \\mathsf{w})\\). Use the residual from the previous projection to create an orthonormal basis \\(\\mathsf{u}_1, \\mathsf{u}_2\\) for \\(W\\) such that \\(\\mathsf{u}_1\\) is a vector in the same direction as \\(\\mathsf{v}\\). 30.5.2 Let \\(\\mathsf{u} \\neq 0\\) be a vector in \\(\\mathbb{R}^n\\). Define the function \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) by \\(T(\\mathsf{x}) = \\mbox{proj}_{\\mathsf{u}} \\mathsf{x}\\). Recall that the kernel of \\(T\\) is the subspace \\(\\mbox{ker}(T) = \\{ \\mathsf{x} \\in \\mathbb{R}^n \\mid T(x) = \\mathbf{0} \\}\\). Describe \\(\\mbox{ker}(T)\\) as explicitly as you can. 30.5.3 The vectors \\(\\mathsf{u}_1, \\mathsf{u}_2\\) form an orthonormal basis of a subspace \\(W\\) of \\(\\mathbb{R}^4\\). Find the projection of \\(\\mathsf{v}\\) onto \\(W\\) and determine how close \\(\\mathsf{v}\\) is to \\(W\\). \\[ \\mathsf{u}_1 = \\frac{1}{2}\\begin{bmatrix} 1\\\\ -1\\\\ -1\\\\ 1 \\end{bmatrix}, \\quad \\mathsf{u}_2 = \\frac{1}{2}\\begin{bmatrix} 1\\\\ -1\\\\ 1\\\\ -1 \\end{bmatrix}, \\quad \\mathsf{v} = \\begin{bmatrix} 2\\\\ 2\\\\ 4\\\\ 2 \\end{bmatrix} \\] 30.5.4 Consider vectors \\(\\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\-1 \\end{bmatrix}\\) and \\(\\mathsf{v}_2= \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\) in \\(\\mathbb{R}^3\\). Let \\(W=\\mbox{span}(\\mathsf{v}_1, \\mathsf{v}_2)\\). Show that \\(\\mathsf{v}_1\\) and \\(\\mathsf{v}_2\\) are orthogonal. Find a basis for \\(W^{\\perp}\\). Use orthogonal projections to find the representation of \\(\\mathsf{y} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 2 \\end{bmatrix}\\) as \\(\\mathsf{y} = \\hat{\\mathsf{y}} + \\mathsf{z}\\) where \\(\\hat{\\mathsf{y}} \\in W\\) and \\(\\mathsf{z} \\in W^{\\perp}\\). 30.5.5 Let \\(W\\) be the span of the vectors \\[ \\begin{bmatrix} 1 \\\\ -2 \\\\ 1 \\\\ 0 \\\\1 \\end{bmatrix}, \\quad \\begin{bmatrix} -1 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 3 \\\\1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\\\4 \\end{bmatrix} \\] Find a basis for \\(W\\). What is the dimension of this subspace? Find a basis for \\(W^{\\perp}\\) 30.5.6 Consider the system \\(A \\mathsf{x} = \\mathsf{b}\\) given by \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; -1 \\\\ 1 &amp; 1 &amp; -1 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 4\\\\ 1 \\\\ -2 \\\\ -1 \\end{bmatrix}. \\] Show that this system is inconsistent. Find the projected value \\(\\hat{\\mathsf{b}}\\), and the residual \\(\\mathsf{z}\\). How close is your approximate solution to the desired target vector? 30.5.7 Here is an inconsistent system of equations: \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 2 \\\\ 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 6\\\\ 4 \\\\ -4 \\end{bmatrix} \\] State the normal equations for this problem (be sure to do all of the necessary matrix multiplications). Find the least squares solution to the problem. How close is your approximate solution to the desired target vector? 30.5.8 According to the COVID Tracking Project, Minnesota had \\(54,463\\) positive COVID-19 cases between March 6 and 31 July, 2020. As of 16 December, 2020, that count has reached \\(386,412\\). The vector covid.mn lists the total number of new COVID-19 cases in Minnesota between August 1, 2020 and December 16, 2020 (on top of the previously reported \\(54,463\\)). covid.start = 54463 covid.mn = c(725, 1484, 2097, 2699, 3316, 4177, 4722, 5638, 6435, 7053, 7376, 7840, 8530, 9260, 9950, 10689, 11253, 11598, 12155, 12845, 13670, 14404, 15121, 15835, 16244, 16773, 17927, 18777, 19794, 20726, 21401, 21892, 22622, 23660, 24503, 25417, 26124, 26762, 27145, 27405, 27786, 28253, 29125, 29848, 30486, 30888, 31350, 32259, 33344, 34258, 35554, 36479, 36959, 37637, 38549, 39726, 41196, 42271, 43175, 43984, 44671, 45737, 46903, 48324, 49363, 50336, 51277, 52188, 53459, 54849, 56365, 57805, 58976, 60111, 61480, 62643, 64933, 66627, 68349, 69976, 71068, 72128, 73689, 75400, 77659, 79339, 80909, 83073, 84981, 87848, 91002, 94009, 96209, 99157, 102633, 106460, 110402, 115844, 120491, 126399, 130325, 135218, 140107, 147332, 152876, 161565, 169118, 176555, 182486, 187580, 195443, 202237, 208489, 215694, 222037, 228453, 234840, 234840, 240538, 249560, 258506, 264300, 267849, 273014, 279163, 284510, 290818, 296399, 301689, 304740, 309256, 312755, 316505, 320935, 324360, 327378, 329701, 331949) Find the best fitting exponential function \\(f(t) = a e^{k t}\\) for the number of COVID-19 cases in Minnesota since 31 July, 2020. Here \\(t\\) is the number of days since 31 July. Run the following code to plot your function. This code assumes that your least squares solution is given by xhat. Does it look like a good fit? 30.5.9 Consider the symmetric matrix \\[ A = \\begin{bmatrix} 3 &amp; 0 &amp; 34 &amp; 3 \\\\ 0 &amp; 6 &amp; -34 &amp; 0 \\\\ 34 &amp; -34 &amp; 74 &amp; 34 \\\\ 3 &amp; 0 &amp; 34 &amp; 3 \\end{bmatrix} \\] Use RStudio to find the eigenvalues \\(\\lambda_1 &gt; \\lambda_2 &gt; \\lambda_3 &gt; \\lambda_4\\) and their corresponding eigenvectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_3\\). Confirm that these eigenvectors form an orthonormal set. Is the linear transformation \\(T(\\mathsf{x}) = Ax\\) invertible? How do you know? Confirm that \\[ A = \\lambda_{1} \\mathsf{v}_1 \\mathsf{v}_1^{\\top} + \\lambda_{2} \\mathsf{v}_2 \\mathsf{v}_2^{\\top} + \\lambda_{3} \\mathsf{v}_3 \\mathsf{v}_3^{\\top} + \\lambda_{4} \\mathsf{v}_4 \\mathsf{v}_4^{\\top}. \\] Use your answer in part (d) to find the best rank 2 approximation for \\(A\\). (Be careful!) 30.5.10 Here is a matrix \\(A\\) and its reduced row echelon form \\(B\\) \\[ A = \\begin{bmatrix} 1 &amp; 3 &amp; -3 &amp; 1 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 &amp; 6 &amp; 5 \\\\ 3 &amp; 3 &amp; -3 &amp; 6 &amp; 3 \\\\ -1 &amp; 4 &amp; -3 &amp; -3 &amp; -1 \\end{bmatrix} \\qquad \\longrightarrow \\qquad B = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 2.5 &amp; 1.5 \\\\ 0 &amp; 1 &amp; 0 &amp; 1.0 &amp; 2.0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1.5 &amp; 2.5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0.0 &amp; 0.0 \\end{bmatrix}. \\] Find a basis for \\(\\mbox{Nul}(A)\\) and \\(\\mbox{Col}(A)\\). Is the linear transformation \\(T(\\mathsf{x}) = A \\mathsf{x}\\) one-to-one? Onto? How is the SVD for \\(A\\) related to the SVD for \\(B\\)? What properties will they share? What properties will be different? Make some conjectures. Now find the SVD of both \\(A\\) and \\(B\\), and test your conjectures. Compare the singular values, the right singular vectors and the left singular vectors. Be sure to compare each of the four fundamental subspaces: \\(\\mbox{Nul}(M), \\mbox{Col}(M), \\mbox{Row}(M), \\mbox{Nul}(M^{\\top})\\). 30.6 \\(\\mathsf{A}\\) is a \\(4 \\times 5\\) matrix and \\(b \\in \\mathbb{R}^4\\). The augmented matrix \\([\\,\\mathsf{A}\\mid b\\,]\\) row reduces as shown here. \\[ [\\,\\mathsf{A}\\mid b\\,] = \\left[ \\begin{array}{ccccc|c} \\vert &amp; \\vert &amp; \\vert &amp; \\vert &amp; \\vert &amp;\\vert\\\\ v_1 &amp; v_2 &amp; v_3 &amp; v_4 &amp; v_5 &amp; b\\\\ \\vert &amp; \\vert &amp; \\vert &amp; \\vert &amp; \\vert &amp;\\vert\\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{rrrrr|r} 1 &amp; 0 &amp; 2 &amp; 0 &amp; -1 &amp;1\\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; -1 &amp;1\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp;-2\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right] \\] a. Give all of the solutions to o \\(\\mathsf{A} x = b\\) in parametric form. Give a dependence relation among the columns of \\(\\mathsf{A}\\). These true-false questions refer to the coefficient matrix \\(\\mathsf{A}\\) above. Decide if the statement is T = True or F = False. No justification necessary. \\(\\mathsf{A} x = b\\) has a solution for all \\(b \\in \\mathbb{R}^4\\). The columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^4\\). If \\(\\mathsf{A} x = b\\) has a solution, then it has infinitely many solutions. The linear transformation \\(x \\mapsto \\mathsf{A} x\\) is one-to-one The linear transformation \\(x \\mapsto \\mathsf{A} x\\) is onto 30.7 Suppose that \\(\\mathsf{A}\\) is an \\(n \\times n\\) matrix and that \\(\\vec{\\mathsf{x}}_1\\) and \\(\\vec{\\mathsf{x}}_2\\) are two solutions to \\(\\mathsf{A}x = \\vec{\\mathsf{b}}\\) with \\(\\vec{\\mathsf{b}}\\not= \\vec{\\mathbf{0}}\\) and \\(\\vec{\\mathsf{x}}_1 \\not= \\vec{\\mathsf{x}}_2\\). Give a nonzero solution to \\(\\mathsf{A}\\vec{\\mathsf{x}}= \\vec{\\mathbf{0}}\\). Give a solution to \\(\\mathsf{A}\\vec{\\mathsf{x}}= \\vec{\\mathsf{b}}\\) that no one else in the class has. Decide if the statement is T = True or F = False, or I = there is not enough information to know. The equation \\(\\mathsf{A}x = \\vec{\\mathsf{y}}\\) has a solution for all \\(\\vec{\\mathsf{y}}\\in \\mathbb{R}^n\\) \\(\\lambda = 0\\) is an eigenvalue of \\(\\mathsf{A}\\) \\(\\mathsf{A}\\) is invertible \\(\\mathsf{A}\\) is diagonalizable 30.8 Watch this! The answer to at least one question on Quiz 4 is contained in this video. 30.9 Solutions to Practice Problems 30.9.1 \\[\\begin{align} \\| \\mathsf{v} \\| &amp;= \\sqrt{ \\mathsf{v} \\cdot \\mathsf{v}} = \\sqrt{1+1+1} = \\sqrt{3} \\\\ \\| \\mathsf{w} \\| &amp;= \\sqrt{ \\mathsf{w} \\cdot \\mathsf{vw}} = \\sqrt{25+4+9} = \\sqrt{38} \\\\ \\end{align}\\] We have \\(\\mathsf{v} - \\mathsf{w} = \\begin{bmatrix} -4 \\\\ -3 \\\\ -2 \\end{bmatrix}\\) and so \\[ \\| \\mathsf{v} - \\mathsf{w}\\| = \\sqrt{16+9+4} = \\sqrt{29} \\] \\[ \\cos \\theta = \\frac{\\mathsf{v} \\cdot \\mathsf{w}}{\\| \\mathsf{v} \\| \\, \\|\\mathsf{w} \\| } = \\frac{5-2+3}{\\sqrt{3} \\, \\sqrt{38} } = \\frac{2\\sqrt{3}}{\\sqrt{38} } \\] \\[ \\hat{\\mathsf{w}} = \\mbox{proj}_{\\mathsf{v}} \\mathsf{w} = \\frac{\\mathsf{v} \\cdot \\mathsf{w}}{ \\mathsf{v} \\cdot \\mathsf{v} } \\, \\mathsf{v} = \\frac{5-2+3}{1+1+1} \\mathsf{v} = 2 \\mathsf{v} = \\begin{bmatrix} 2 \\\\ -2 \\\\ 2 \\end{bmatrix} \\] Using \\(\\hat{\\mathsf{w}}\\) from the previous problem, we know that \\[ \\mathsf{z} = \\mathsf{w} - \\hat{\\mathsf{w}} = \\begin{bmatrix} 5 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} \\] is orthogonal to \\(\\mathsf{v}\\).So an orthonormal basis is \\[ \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} \\quad \\mbox{and} \\quad \\frac{1}{\\sqrt{26}} \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} \\] 30.9.2 Here are a few ways to describe \\(\\mbox{ker}(T)\\). + \\(\\mbox{ker}(T) = \\{ \\mathsf{x} \\in \\mathbb{R}^n \\mid \\mathsf{x} \\cdot \\mathsf{u} = 0 \\}\\). + \\(\\mbox{ker}(T)\\) is the set of vectors that are orthogonal to \\(\\mathsf{u}\\). + Let \\(A\\) be the \\(1 \\times n\\) matrix \\(\\mathsf{u}^{\\top}\\). Then \\(\\mbox{ker}(T)= \\mbox{Nul}(A)\\). 30.9.3 We have \\(\\mathsf{u}_1 \\cdot \\mathsf{v} = 2-2-4+2=-2\\) and \\(\\mathsf{u}_1 \\cdot \\mathsf{v} = 2-2+4-2=2\\) so \\[ \\hat{\\mathsf{v}} = \\mbox{proj}_W \\mathsf{v} = -2 \\mathsf{u}_1 + 2 \\mathsf{u}_2 = \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -2 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] with residual vector \\[ \\mathsf{z} = \\mathsf{v} - \\hat{\\mathsf{v}} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 4 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -2 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 6 \\\\ 4 \\\\ 2 \\end{bmatrix} \\] and the distance is \\(\\| \\mathsf{z} \\| = \\sqrt{36 + 16 + 4} = \\sqrt{56}\\). 30.9.4 \\(\\mathsf{v}_1 \\cdot \\mathsf{v}_2 = 1 +2 - 3 =0\\). We must find \\(\\mbox{Nul}(A)\\) where \\(A = \\begin{bmatrix} \\mathsf{v}_1^{\\top} \\\\ \\mathsf{v}_2^{\\top}\\end{bmatrix}\\). \\[ \\begin{bmatrix} 1 &amp; 1 &amp; -1 \\\\ 1 &amp; 2 &amp; 3 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 1 &amp; -1 \\\\ 0 &amp; 1 &amp; 4 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; -5 \\\\ 0 &amp; 1 &amp; 4 \\end{bmatrix} \\] so the vector \\(\\begin{bmatrix} 5 \\\\ -4 \\\\ 1 \\end{bmatrix}\\) is a basis for \\(W^{\\perp}\\) We have \\[\\begin{align} \\hat{\\mathsf{y}} &amp;= \\frac{\\mathsf{y} \\cdot \\mathsf{v_1}}{\\mathsf{v_1} \\cdot \\mathsf{v_1}} \\, \\mathsf{v_1} + \\frac{\\mathsf{y} \\cdot \\mathsf{v_2}}{\\mathsf{v_2} \\cdot \\mathsf{v_2}} \\, \\mathsf{v_2} = \\frac{8-2}{1+1+1} \\mathsf{v_1} + \\frac{8+6}{1+4+9} \\mathsf{v_2} \\\\ &amp;= 2\\mathsf{v_1} +\\mathsf{v_2} = \\begin{bmatrix} 2 \\\\ 2 \\\\ -2 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} \\end{align}\\] and so \\[ \\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ -4 \\\\ 1 \\end{bmatrix}. \\] 30.9.5 a .We will answer this one using RStudio. A = cbind(c(1,-2,1,0,1), c(-1,3,-1,1,-1), c(0,0,1,3,1), c(0,2,0,0,4)) rref(A) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## [5,] 0 0 0 0 So we need all four vectors to span the column space. We obtain a basis for \\(W^{\\perp}\\) by finding \\(\\mbox{Nul(A^{\\top})}\\) So let’s row reduce \\(A^{\\top}\\) rref(t(A)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 -2 ## [2,] 0 1 0 0 2 ## [3,] 0 0 1 0 7 ## [4,] 0 0 0 1 -2 The vector \\(\\begin{bmatrix} 2 \\\\ -2 \\\\ -7 \\\\ 2 \\\\ 1\\end{bmatrix}\\) spans \\(W^{\\perp}\\) 30.9.6 We will show that \\(\\| \\mathsf{v} \\|^2 = ( \\mathsf{v} \\cdot \\mathsf{u}_1)^2 + (\\mathsf{v} \\cdot \\mathsf{u}_2)^2 + \\cdots +(\\mathsf{v} \\cdot \\mathsf{u}_n)^2.\\) Let’s write \\(\\mathsf{v}\\) in terms of the orthonormal basis: \\(\\mathsf{v} = c_1 \\mathsf{u}_1 + c_2 \\mathsf{u}_2 + \\cdots + c_n \\mathsf{u}_n\\). We then have \\[\\begin{align} \\| \\mathsf{v} \\|^2 &amp;= \\mathsf{v} \\cdot \\mathsf{v} \\\\ &amp;= (c_1 \\mathsf{u}_1 + c_2 \\mathsf{u}_2 + \\cdots + c_n \\mathsf{u}_n) \\cdot (c_1 \\mathsf{u}_1 + c_2 \\mathsf{u}_2 + \\cdots + c_n \\mathsf{u}_n) \\\\ &amp;= c_1^2 + c_2 + \\cdots + c_n^2 \\end{align}\\] because \\(\\mathsf{u}_i \\cdot \\mathsf{u}_i =1\\) and \\(\\mathsf{u}_i \\cdot \\mathsf{u}_j=0\\) for \\(i \\neq j\\). Finally, we note that \\(\\mathsf{v} \\cdot \\mathsf{u_i} = c_i\\) using the same facts about the dot projects for the orthonormal basis. So we have verified the claim above. 30.9.7 A = cbind(c(1,1,1,1), c(1,2,1,2),c(1,-1,-1,1)) b = c(4,1,-2,-1) rref(cbind(A,b)) ## b ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 There is a pivot in the last column of this augmented matrix, so this system is inconsistent. Here is the least squares calculation. #solve the normal equation (xhat = solve(t(A) %*% A, t(A) %*% b)) ## [,1] ## [1,] 2 ## [2,] -1 ## [3,] 1 # find the projection (bhat = A %*% xhat) ## [,1] ## [1,] 2.000000e+00 ## [2,] -1.000000e+00 ## [3,] 6.661338e-16 ## [4,] 1.000000e+00 # find the residual vector (z = b - bhat) ## [,1] ## [1,] 2 ## [2,] 2 ## [3,] -2 ## [4,] -2 # check that z is orthogonal to Col(A) t(A) %*% z ## [,1] ## [1,] -8.881784e-16 ## [2,] 0.000000e+00 ## [3,] 0.000000e+00 # measure the distance between bhat and b sqrt( t(z) %*% z) ## [,1] ## [1,] 4 The projection is \\(\\hat{\\mathsf{b}} = [2,-1,0,1]^{\\top}\\). The residual is \\(\\mathsf{z} = [2,2,-2,-2]^{\\top}\\) The distance of between \\(\\mathsf{b}\\) and \\(\\hat{\\mathsf{b}}\\) is \\[ \\| = \\| \\mathsf{z} \\| = \\sqrt{4+4+4+4} = \\sqrt{16} = 4. \\] ### x = 1:length(covid.mn) y = log(covid.mn) A = cbind(x^0, x) (xhat = solve(t(A) %*% A, t(A) %*% y)) ## [,1] ## 8.66588980 ## x 0.03138331 a = exp(xhat[1]) k = xhat[2] f=function(y){a * exp(k*(y))} plot(x,f(x)+ covid.start,type=&quot;l&quot;,lwd=3,ylab=&quot;new positive COVID-19 cases&quot;, xlab=&quot;days since July 31, 2020&quot;, main=&quot;best fit exponential function&quot;) points(x,covid.mn + covid.start,pch=20,cex=.7,col=&quot;red&quot;) The curve is a pretty good fit (unfortunately?). However, it does look like the additional restrictions of the last two weeks are slowing the COVID spread. 30.9.8 A = cbind(c(3,0,34,3), c(0,6,-34,0), c(34,-34,74,34),c(3,0,34,3)) A ## [,1] [,2] [,3] [,4] ## [1,] 3 0 34 3 ## [2,] 0 6 -34 0 ## [3,] 34 -34 74 34 ## [4,] 3 0 34 3 syst = eigen(A) (P = syst$vectors) ## [,1] [,2] [,3] [,4] ## [1,] -0.2886751 4.082483e-01 -7.071068e-01 0.5 ## [2,] 0.2886751 8.164966e-01 -3.295975e-15 -0.5 ## [3,] -0.8660254 -3.330669e-16 7.771561e-16 -0.5 ## [4,] -0.2886751 4.082483e-01 7.071068e-01 0.5 zapsmall(P %*% t(P)) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 The eigenvectors are the columns of the second matrix \\(P\\) shown above. The last matrix shows that \\(P P^{\\top} = I\\), so the columns are orthonormal. ## [1] 108 6 0 -28 \\(A\\) is not invertible because 0 is an eigenvalue. v1 = syst$vectors[,1] v2 = syst$vectors[,2] v3 = syst$vectors[,3] v4 = syst$vectors[,4] syst$values[1] * v1 %*% t(v1) + syst$values[2] * v2 %*% t(v2) + syst$values[3] * v3 %*% t(v3) + syst$values[4] * v4 %*% t(v4) ## [,1] [,2] [,3] [,4] ## [1,] 3.000000e+00 -1.776357e-15 34 3.000000e+00 ## [2,] -1.776357e-15 6.000000e+00 -34 5.595524e-14 ## [3,] 3.400000e+01 -3.400000e+01 74 3.400000e+01 ## [4,] 3.000000e+00 5.595524e-14 34 3.000000e+00 We must remember to use the eigenvalues of largest magnitude: these are \\(\\lambda_1 = 108\\) and \\(\\lambda_4 = -28\\). syst$values[1] * v1 %*% t(v1) + syst$values[4] * v4 %*% t(v4) ## [,1] [,2] [,3] [,4] ## [1,] 2 -2 34 2 ## [2,] -2 2 -34 -2 ## [3,] 34 -34 74 34 ## [4,] 2 -2 34 2 30.9.9 This SVD factorization is \\(U \\Sigma V^{\\top}\\). \\(\\mbox{Nul}(A) = \\{ 0 \\}\\) so it has no basis. An orthonormal basis for \\(\\mbox{Col}(A)\\) is the first three columns of \\(U\\). An orthonormal basis for \\(\\mbox{Row}(A)\\) is the three rows of \\(V^{\\top}\\). +An orthonormal basis for \\(\\mbox{Nul}(A^{\\top})\\) is the last two columns of \\(U\\). This is true because the zero vector is orthogonal to every vector. The matrix \\(\\Sigma\\) has 3 pivots. So the nullspace of \\(A\\) is trivial. The 3D volume expands because the product of the singular values is greater than 1. 11.4 * cbind(c(0.48,0.61,0.30,0.032,0.56)) %*% rbind(c(0.16,-0.98,0.062)) ## [,1] [,2] [,3] ## [1,] 0.875520 -5.362560 0.3392640 ## [2,] 1.112640 -6.814920 0.4311480 ## [3,] 0.547200 -3.351600 0.2120400 ## [4,] 0.058368 -0.357504 0.0226176 ## [5,] 1.021440 -6.256320 0.3958080 30.9.10 The null space is the span of \\([-2.5,-1,-1.5,1,0]^{\\top}\\) and \\([-1.5,-2,-2.5,0,1]^{\\top}\\). The column space is the span of \\([1,2,3,-1]^{\\top}\\) and \\([3,1,3,4]^{\\top}\\) and \\([-3,0,-3,-3]^{\\top}\\) This mapping is not one-to-one because the null space is two-dimensional. The mapping is not onto because there is no pivot in the final row of \\(B\\). This is a conceptual question without a particular “right answer.” Here are some observations. The nullspace of \\(A\\) is the same as the nullspace of \\(B\\), so an orthogonal basis for \\(\\mbox{Nul}(A)\\) is also an orthogonal basis for \\(\\mbox{Nul}(B)\\). However, the orthonormal basis vector in the SVD for \\(A\\) do not need to be the same as the orthonormal basis in the SVD for \\(B\\). The columnspace of \\(A\\) is different from the columnspace of \\(B\\). However, each of them is three-dimensional. The singular values for \\(A\\) do not need to equal the singular values of \\(B\\). Row reduction change the determinant of a square matrix. So it is safe to assume that it will have a similar effect on singular values. However, we konw that both will have 3 singular values since \\(A\\) and \\(B\\) have the same rank. A = rbind(c(1,3,-3,1,0),c(2,1,0,6,5),c(3,3,-3,6,3),c(-1,4,-3,-3,-1)) B = rref(A) A ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 -3 1 0 ## [2,] 2 1 0 6 5 ## [3,] 3 3 -3 6 3 ## [4,] -1 4 -3 -3 -1 B ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 2.5 1.5 ## [2,] 0 1 0 1.0 2.0 ## [3,] 0 0 1 1.5 2.5 ## [4,] 0 0 0 0.0 0.0 (Asvd = svd(A)) ## $d ## [1] 1.170522e+01 7.291802e+00 1.953827e+00 1.048298e-15 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] -0.2074163 -0.5081310 0.3142951 0.7745967 ## [2,] -0.6655245 0.2640904 -0.6485883 0.2581989 ## [3,] -0.7060981 -0.2378931 0.4220968 -0.5163978 ## [4,] 0.1244229 -0.7845164 -0.5498965 -0.2581989 ## ## $v ## [,1] [,2] [,3] [,4] ## [1,] -0.3230339 0.01246428 0.4264992 -0.6959338 ## [2,] -0.2484683 -0.70106777 -0.3270500 0.2609756 ## [3,] 0.2022409 0.62969638 -0.2863539 0.1988388 ## [4,] -0.7526916 0.27463749 0.3096671 0.5095232 ## [5,] -0.4758851 0.19080183 -0.7302360 -0.3852494 (Bsvd = svd(B)) ## $d ## [1] 4.649483 1.543473 1.000000 0.000000 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] 0.6088198 0.7877604 -0.09365858 0 ## [2,] 0.4779319 -0.4584513 -0.74926865 0 ## [3,] 0.6331821 -0.4114072 0.65561007 0 ## [4,] 0.0000000 0.0000000 0.00000000 1 ## ## $v ## [,1] [,2] [,3] [,4] ## [1,] 0.1309436 0.5103818 -9.365858e-02 -0.82024886 ## [2,] 0.1027925 -0.2970259 -7.492686e-01 0.05990659 ## [3,] 0.1361833 -0.2665465 6.556101e-01 -0.04871373 ## [4,] 0.6344264 0.5791090 6.938894e-17 0.49438789 ## [5,] 0.7424586 -0.4948451 -5.551115e-17 -0.27714724 # check that the singular values are different Asvd$d ## [1] 1.170522e+01 7.291802e+00 1.953827e+00 1.048298e-15 Bsvd$d ## [1] 4.649483 1.543473 1.000000 0.000000 Arow = Asvd$v[,1:3] Acol = Asvd$u[,1:3] Brow = Bsvd$v[,1:3] Bcol = Bsvd$u[,1:3] # check that the rowspaces are the same # this also means that the nullspaces are the same rref(cbind(Arow, Brow)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 -0.8711504 0 0.4023226 ## [2,] 0 1 0 0.3312215 0 0.9197649 ## [3,] 0 0 1 -0.3624766 0 -0.1264565 ## [4,] 0 0 0 0.0000000 1 0.1543445 ## [5,] 0 0 0 0.0000000 0 0.0000000 rref(cbind(Brow, Arow)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 -0.8711504 0.3312215 0 ## [2,] 0 1 0 -0.3453768 0.1113812 0 ## [3,] 0 0 1 0.3490156 0.9369560 0 ## [4,] 0 0 0 0.0000000 0.0000000 1 ## [5,] 0 0 0 0.0000000 0.0000000 0 # check that the columnspaces are different rref(cbind(Acol, Bcol)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 2.77467300 -1.955659 ## [2,] 0 1 0 0 0.45357378 -1.059136 ## [3,] 0 0 1 0 -0.01928201 1.068529 ## [4,] 0 0 0 1 2.62771978 -2.255687 30.9.11 "]]
