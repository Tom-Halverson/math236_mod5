[["index.html", "MATH 236: Linear Algebra Preface", " MATH 236: Linear Algebra Preface This is the class handbook for Math 236 Linear Algebra at Macalester College. The content here was made by Andrew Beveridge and Tom Halverson and other faculty in the Department of Mathematics, Statistics and Computer Science at Macalester College. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["problem-set-1-a.html", "Section 1 Problem Set 1-A 1.1 Characterize the Solution Set 1.2 Find the General Solution 1.3 Elementary row operations are reversible 1.4 Designer Parabolas 1.5 Traffic Flow", " Section 1 Problem Set 1-A Due: Tuesday January 26 by noon CST. Week 1 and Week 8 are half weeks, so those two assignments will be split in two and called Problem Sets 1A and 1B. Upload your solutions to problems 1–4 by writing them out by hand, scanning them to pdf using a scanning software such as AdobeScan, assembling them into a single PDF, and uploading it to Moodle. Problem 1.5 is to be done using RStudio. To solve it, create an Rmarkdown file, knit it to .html, and upload the .html on Moodle along with the PDF for questions 1-4. 1.1 Characterize the Solution Set The following augmented matrices are in row echelon form. Decide whether the set of solutions is a point, line, plane, or the empty set in 3-space. Briefly justify your answer. \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 4 \\\\ 0 &amp; 1 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; -1 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 0 &amp; 1 &amp; 0 &amp; 6 \\\\ 0 &amp; 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.2 Find the General Solution Each of the following matrices is the reduced row echelon form of the augmented matrix of a system of linear equations. Give the general solution to each system. \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 3 &amp; 0 &amp; -2 &amp; 5\\\\ 0 &amp; 0 &amp; 1 &amp; 4 &amp; -2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; 4 &amp; 0 &amp; 3 &amp; 6\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -2&amp; -8 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 3 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 4 &amp; 0 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7 &amp; 6\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.3 Elementary row operations are reversible In each case below, an elementary row operation turns the matrix \\(A\\) into the matrix \\(B\\). For each of them, Describe the row operation that turns \\(A\\) into \\(B\\), and Describe the row operation that turns \\(B\\) into \\(A\\). Give your answers in the form: “scale \\(R_2\\) by 3” or “swap \\(R_1\\) and \\(R_4\\)” or “replace \\(R_3\\) with \\(R_3 + \\frac{1}{5} R_1\\).” \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 0 &amp; 7 &amp; 0 &amp; -4 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 4 &amp; 1 &amp; -2 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] 1.4 Designer Parabolas In each part below, set up and solve a linear system of equations to find all possible parabolas of the form \\[ f(x) = a + b x + c x^2 \\] that satisfy the given conditions. For full credit, please solve these by hand, doing all row reductions that bring the system of equations to Reduced Row Echelon Form. On future assignments, you can solve problems like this using either RStudio or WolframAlpha. You are welcome (and, in fact, encouraged) to check your answers using software. \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(2,4)\\). \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(3,10)\\). \\(f(x)\\) passes through the two points: \\((1,3)\\) and \\((3,11)\\). 1.5 Traffic Flow Below you find a section of one-way streets in downtown St Paul, where the arrows indicate traffic direction. The traffic control center has installed electronic sensors that count the numbers of vehicles passing through the 6 streets that lead into and out of this area. Assume that the total flow that enters each intersection equals the the total flow that leaves each intersection (we will ignore parking and staying). Create a system of linear equations to find the possible flow values for the inner streets \\(x_1, x_2, x_3, x_4\\). Using RStudio, enter the augmented matrix of this system, and solve it using the rref command. Type out the general solution to this system of equations. Your answer to part b should be an infinite solution set. Give two distinct solutions that are realistic in terms of traffic flow. Is it possible to close down the street labeled by \\(x_2\\) for road construction? That is, is it possible to have \\(x_2 = 0\\) and to meet the other conditions? "],["problem-set-2.html", "Section 2 Problem Set 2 2.1 Parametric Vector Form 2.2 RREF for a linear system 2.3 RREF for a set of vectors 2.4 Removing free variable columns from a matrix 2.5 A square matrix 2.6 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) 2.7 A Balanced Diet 2.8 Missing Column 2.9 Linear System", " Section 2 Problem Set 2 Due: Tuesday February 2 by 10:00am CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. In problems where you use RStudio for row reduction and are not asked to turn in an R markdown file, you can write something like this: You can download the Rmd source file for this problem set. The Problem Set covers sections 1.3, 1.4, 1.5, and 1.7. 2.1 Parametric Vector Form Here is the augmented matrix for a system of linear equations \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\), and its RREF. Give the general solution to this system in parametric vector form. \\[ \\left[ \\begin{array}{ccccc|c} 1 &amp; 1 &amp; -1 &amp; -1 &amp; 2 &amp; 1 \\\\ 1 &amp; 0 &amp; -2 &amp; 1 &amp; 1 &amp; 3 \\\\ -2 &amp; 1 &amp; 5 &amp; 1 &amp; -6 &amp; 2 \\\\ -3 &amp; 0 &amp; 6 &amp; 2 &amp; -8 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; -3 &amp; 6 \\\\ 1 &amp; 0 &amp; -2 &amp; -1 &amp; 3 &amp; -1 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; -2 &amp; 0 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] 2.2 RREF for a linear system Here is the reduced row echelon form of a matrix \\(\\mathsf{A}\\) (you are not given the matrix \\(\\mathsf{A}\\)). \\[ \\mathsf{A} \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 0 &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; -5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] Give the parametric equations of the general solution to the homogenous equation \\(\\mathsf{A} \\mathsf{x} = {\\bf 0}\\). Describe the geometric form of your answer to part (a). For example, you answer should be something like: “it is a plane in \\(\\mathbb{R}^3\\)” or “it is a line in \\(\\mathbb{R}^7\\)” or “it is a point in \\(\\mathbb{R}^4\\).” Suppose that we also know that \\(\\mathsf{A}\\begin{bmatrix} 4 \\\\ 1 \\\\ -3 \\\\ 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 22 \\\\ -13 \\\\ 7 \\\\ \\end{bmatrix}\\). Then give the general solution to \\(\\mathsf{A} \\mathsf{x}= \\begin{bmatrix} 22 \\\\ -13\\\\ 7 \\\\ \\end{bmatrix}\\) in parametric form. 2.3 RREF for a set of vectors Suppose that we have five vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4,\\mathsf{v}_5\\) in \\(\\mathbb{R}^4\\) and that the matrix \\[ A = \\left[ \\begin{array}{ccc} \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\\\ \\mathsf{v}_1 &amp; \\mathsf{v}_2 &amp; \\mathsf{v}_3 &amp;\\mathsf{v}_4 &amp;\\mathsf{v}_5 \\\\ \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\end{array} \\right] \\] has reduced row echelon form \\[ \\begin{bmatrix} 1 &amp; 0 &amp; -3 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 4 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}. \\] Do the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\) span \\(\\mathbb{R}^4\\)? Justify your answer. Is the vector \\(\\mathsf{v}_3\\) in \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2)\\)? Justify your answer. Pick any \\(\\mathsf{b}\\) in \\(\\mathrm{span}(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5)\\). Is there always a unique way to write \\(\\mathsf{b}\\) as a linear combination of \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\)? Justify your answer. 2.4 Removing free variable columns from a matrix Consider the matrix \\[ A =\\left[ \\begin{array}{cccccc} 6 &amp; 5 &amp; -3 &amp; 4 &amp; 2 &amp; -9 \\\\ -7 &amp; -6 &amp; 4 &amp; -5 &amp; -7 &amp; 16 \\\\ -4 &amp; -3 &amp; -1 &amp; 0 &amp; -8 &amp; 9 \\\\ 8 &amp; 7 &amp; -5 &amp; 6 &amp; 1 &amp; -12 \\end{array} \\right]. \\] Use RStudio to show that the columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^4\\). You don’t need to turn in an R file here, just report the reduced row echelon form that you get. Write down the new matrix \\(\\mathsf{A}&#39;\\) gotten by removing the free variable columns from \\(\\mathsf{A}\\). Without using additional calculations on RStudio, explain why the new system \\(\\mathsf{A}&#39; \\mathsf{x} = \\mathsf{b}\\) is consistent and has a unique solution for every choice of \\(\\mathsf{b} \\in \\mathbb{R}^4\\). 2.5 A square matrix Suppose that \\(A\\) is a \\(5\\times 5\\) matrix and \\(\\mathsf{b}\\) is a vector in \\(\\mathbb{R}^5\\) with the property that \\(A\\mathsf{x}=\\mathsf{b}\\) has a unique solution. Explain why the columns of \\(A\\) must span \\(\\mathbb{R}^5\\). Use the reduced row echelon form of \\(A\\) in your explanation. 2.6 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) Suppose that \\(\\mathsf{x}_1\\) and \\(\\mathsf{x}_2\\) are solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) (where \\(\\mathsf{b} \\not= \\mathsf{0}\\)). Decide if any of the following are also solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\). \\(\\mathsf{x}_1+ \\mathsf{x}_2\\) \\(\\mathsf{x}_1 - \\mathsf{x}_2\\) \\(\\frac{1}{2} ( \\mathsf{x}_1 + \\mathsf{x}_2)\\) \\(\\frac{5}{2} \\mathsf{x}_1 - \\frac{3}{2} \\mathsf{x}_2\\). Under what conditions on \\(c\\) and \\(d\\) is \\(\\mathsf{x} = c \\mathsf{x}_1 + d \\mathsf{x}_2\\) a solution to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\)? Justify your answer. Let \\(\\mathsf{u}\\) be the vector that points to \\(1/3\\) of the way from the tip of \\(\\mathsf{v}\\) to the tip of \\(\\mathsf{w}\\) as depicted below. Write \\(\\mathsf{u}\\) as a linear combination of \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) (hint: think about \\(\\mathsf{w} - \\mathsf{v}\\)) If \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) are solutions to \\(A x = \\mathsf{b}\\) then show that \\(\\mathsf{u}\\) is also a solution to \\(A \\mathsf{x} = \\mathsf{b}\\). 2.7 A Balanced Diet An athlete wants to consume a daily diet of 200 grams of carbohydrates, 60 grams of fats and 160 grams of proteins. Here are some of their favorite foods. Table 2.1: Food Carb/Fat/Protein (grams) food carbs fats proteins almonds 3 8 5 avocado 15 31 4 beans 20 1 8 bread 12 1 2 cheese 1 5 3 chicken 0 13 50 egg 1 5 6 milk 12 8 8 zucchini 6 0 2 Answer the following questions, using RStudio for your calculations. Each response must use two or more of the following terms: linear combination, span, linearly dependent, linearly independent. Explain why they cannot achieve their daily goal by eating only almonds, milk and zucchini. Explain why they cannot achieve their daily goal by eating only almonds, beans and cheese. Find a valid one-day diet consisting of almonds, chicken, and zucchini. 2.8 Missing Column The matrices below are supposed to be \\(3 \\times 3\\) but in each case the third column was accdentally deleted. In each case, add a third column, that has no 0s in it and is different from either the first or second column, so that the columns of \\(A\\) are linearly dependent and so that the columns of \\(B\\) are linearly independent. Briefly describe your strategy. \\[ A=\\left[ \\begin{matrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{matrix}\\right] \\qquad\\qquad B=\\left[ \\begin{matrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{matrix}\\right] \\] 2.9 Linear System Use R to solve this problem. Do your computations in an R markdown file. Knit the file to HTML and include it with your homework. Here you can download a template for doing this problem (including the matrix typed out for you!). \\[ A =\\left[ \\begin{array}{cccccc} 12 &amp; 10 &amp; -6 &amp; 8 &amp; 4 &amp; -18 \\\\ -7 &amp; -6 &amp; 4 &amp; -5 &amp; -7 &amp; 16 \\\\ 9 &amp; 9 &amp; -9 &amp; 9 &amp; 9 &amp; -27 \\\\ -4 &amp; -3 &amp; -1 &amp; 0 &amp; -8 &amp; 9 \\\\ 8 &amp; 7 &amp; -5 &amp; 6 &amp; 1 &amp; -12 \\\\ \\end{array} \\right] \\quad b = \\begin{bmatrix} 14 \\\\ -12 \\\\ 9\\\\ -15 \\\\6 \\end{bmatrix} \\] Show that the columns of \\(A\\) are linearly dependent by finding two different dependency relations among them. You can write your answer in a form like \\(5 a1+ 4 a2 + 3 a3 + 2 a4 + a5 = 0\\), where \\(a1, a2,\\) etc are the columns of \\(A\\). Augment \\(A\\) with \\(b\\) and show that \\(A x = b\\) is consistent and has infinitely many solutions. Remove the free-variable columns from \\(A\\) to get a new, smaller matrix \\(A&#39;\\). Show that \\(A&#39; x = b\\) has a unique solution and say what that solution is. "],["problem-set-3.html", "Section 3 Problem Set 3 3.1 Properties of Linear Transformations 3.2 Partial Information about a Linear Transformation 3.3 House Renovations 3.4 Matrix of a Nonlinear Transformation? 3.5 A Proof 3.6 Inner and Outer Products 3.7 Archaeological Seriation 3.8 Rental Car 3.9 Adjacency Matrix", " Section 3 Problem Set 3 Due: Tuesday February 9 by noon CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 1.8, 1.9, 2.1, 2.2. 3.1 Properties of Linear Transformations Here are the row reductions pf 4 matrices into reduced row echelon form. \\[ \\begin{array}{ll} A \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 5 &amp; -3 &amp; 0\\\\ 0 &amp; 1 &amp; -2 &amp; 8 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\qquad &amp; B \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\\\ \\\\ C \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} &amp; D \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{bmatrix} \\end{array} \\] In each case, if \\(T_M\\) is the linear transformation given by the matrix product \\(T_M(x) = M x\\), where \\(M\\) is the given matrix, then \\(T_M: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a transformation from domain \\(\\mathbb{R}^n\\) to codomain (aka target) \\(\\mathbb{R}^m\\). Determine the appropriate values for \\(n\\) and \\(m\\), and decide whether \\(T_M\\) is one-to-one and/or onto. Submit your answers in table form, as shown below. \\[ \\begin{array} {|c|c|c|c|c|} \\hline \\text{transformation} &amp; n &amp; m &amp; \\text{one-to-one?} &amp; \\text{onto?} \\\\ \\hline T_A &amp;\\phantom{\\Big\\vert XX}&amp;\\phantom{\\Big\\vert XX}&amp;&amp; \\\\ \\hline T_B &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline T_C &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline T_D &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline \\end{array} \\hskip5in \\] 3.2 Partial Information about a Linear Transformation We are given that \\(T: \\mathbb{R}^4 \\rightarrow \\mathbb{R}^3\\) is a linear transformation such that: \\[ T\\left(\\begin{bmatrix} 3 \\\\ ~2~ \\\\ 1 \\\\ 2 \\end{bmatrix} \\right)=\\begin{bmatrix} ~2~ \\\\ 3 \\\\ 6 \\end{bmatrix} \\qquad\\hbox{and}\\qquad T\\left(\\begin{bmatrix}~~2 \\\\ -1 \\\\ 0 \\\\ -1 \\end{bmatrix} \\right)=\\begin{bmatrix} 2 \\\\ ~0~ \\\\ 1 \\end{bmatrix}. \\] If that is all we know about \\(T\\), then do we have enough information to compute the value of \\(T\\) below? \\[T\\left(\\begin{bmatrix} 5 \\\\ 8 \\\\ ~3~ \\\\ 8 \\end{bmatrix} \\right) = \\hskip5in\\] If yes, then compute it (showing how you do so). If no, then explain why not. Hint: try to write the third input vector as a linear combination of the first two. 3.3 House Renovations Find the matrix of a linear transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) that performs the given transformation of my house. (Hint: use the base, the doorway and the peak of the roof as a guide.) Transformation #1 \\(\\qquad \\qquad\\) Transformation #2 \\(\\qquad \\qquad\\) 3.4 Matrix of a Nonlinear Transformation? This problem illustrates what happens if you try to make the matrix of a transformation that is not linear. Consider the transformation \\(T\\) defined by \\[ T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right) = \\begin{bmatrix} x_1 + x_2^2 + x_3 \\\\ 2 x_2 + x_1 x_3 + 1 \\\\ 2 x_1 + 3 x_2 + x_3 \\end{bmatrix} \\] This is not a linear transformation. Let’s see what happens if we compute its matrix anyway. Compute \\(T(\\mathbf{e}_1)\\), \\(T(\\mathbf{e}_2)\\), and \\(T(\\mathbf{e}_3)\\), and put the vectors you get in the columns of a matrix \\(A\\). Then compute the product below: \\[ \\underbrace{\\begin{bmatrix} \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\end{bmatrix}}_{A} \\begin{bmatrix} x_1 \\\\ x_ 2 \\\\ x_3 \\end{bmatrix} = \\] Explain how the result of this computation demonstrates that \\(T\\) is not linear. 3.5 A Proof Let \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) be a linear transformation. Suppose that \\(\\{v_1, v_2, v_3, v_4\\}\\) is a linearly independent set of vectors in \\(\\mathbb{R}^n\\) but the set of images \\(\\{T(v_1), T(v_2), T(v_3), T(v_4)\\}\\) is a linearly dependent set in \\(\\mathbb{R}^m\\). In the following steps, you will prove that \\(T\\) is not one-to-one. Write out clearly, using the definition, what it means for \\(\\{v_1, v_2, v_3, v_4\\}\\) to be linearly independent. Write out clearly, using the definition, what it means for \\(\\{T(v_1), T(v_2), T(v_3), T(v_4)\\}\\) to be linearly dependent. Use the definition of linear transformation and parts (a) and (b) above to argue that \\(T(x) = \\vec{0}\\) for some nonzero vector \\(x \\in \\mathbb{R}^n\\). Explain why this tells us that \\(T\\) is not one-to-one. 3.6 Inner and Outer Products We can also think of vectors as matrix. A column vector is an \\(n \\times 1\\) matrix and a row vector is a \\(1 \\times n\\) matrix. Compute the following products. These matrix products are called inner products (or dot products) of the vectors. \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\1 \\\\1 \\\\\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 2 \\\\ 5 \\\\ 0 \\\\ -1 \\\\\\end{bmatrix} = \\hskip3in \\] b. Now compute the following products. These are called outer products. \\[ \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} \\begin{bmatrix} 1 &amp; -5 &amp; 2 &amp; 3\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1\\end{bmatrix} =\\hskip3in \\] Row reduce both of the matrices that you get in part b (this should be easy to do by hand,but you can use R if you want to). How many pivots do you get? Explain why you always get this number of pivots when you row reduce an outer product. 3.7 Archaeological Seriation The matrix \\(A\\) below is used in archaeological dating. Its rows correspond to four different grave sites \\(G_1, G_2, G_3, G_4\\) and its columns correspond to five types of pottery\\(P_1, P_2, P_3, P_4, P_5\\). There is a 1 in position \\(i\\)-\\(j\\) if pottery type \\(P_j\\) is found in grave \\(G_i\\) (and a 0 otherwise). \\[ A=\\begin{array}{c|ccccc} &amp; P_1 &amp; P_2 &amp; P_3 &amp; P_4 &amp; P_5 \\\\ \\hline G_1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ G_2 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ G_3 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ G_4 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ \\end{array} \\] Compute the matrix \\(\\mathbf{G} = A A^T\\), where \\(A^T\\) is the transpose of \\(A\\), meaning that the rows and columns have been interchanged. Give the meaning of the \\(i\\)-\\(j\\) entry of \\(\\mathbf{G}\\) (the entry in row \\(i\\) and column \\(j\\)). State clearly the meaning of this entry using complete sentences (or sentence) and explain why it has this meaning. 3.8 Rental Car Solve this problem using R and turn in a markdown file knitted to .html. A group of Macalester alumni open a rental car company specializing in renting electric cars. As a start, they have opened offices in St. Paul, Rochester, and Duluth. Through market research they find that of the cars rented in St. Paul, 85% will get returned in St. Paul, 9% will get returned in Rochester, and 6% will get returned in Duluth. Of the cars rented in Rochester, 30% will get returned in St. Paul, 60% will get returned in Rochester, and 10% in Duluth. Of the cars rented in Duluth, 35% will get returned in St. Paul, 5% in Rochester, and 60% in Duluth. This information is represented in the matrix below. StP = c(.85,.09,.06) Roch = c(.30,.60,.10) Dul = c(.35,.05,.60) M = cbind(StP,Roch,Dul) M ## StP Roch Dul ## [1,] 0.85 0.3 0.35 ## [2,] 0.09 0.6 0.05 ## [3,] 0.06 0.1 0.60 Such a matrix is called a probability matrix or a stochastic matrix because it contains numbers between 0 and 1 and each of its columns sum to 1. The owners are trying to use this data to estimate how much of their fleet will be at each location on average in the long run. Assume that initially they locate 20 cars in each city. This can be recorded by the vector v0 = c(20,20,20). Apply, M to v0, call this vector v1, and explain, using how the matrix-vector product works, why v1 represents the number of cars at each location one day later (for simplicity, we assume that each rental is for 1 day). Now apply M to v1 and call it v2. This should represent the number of cars at each location 2 days later. Also compute the square of the matrix M and call it M2. Confirm that M2 times v0 is the same as M times v1. Write a for loop that applies M over and over again to see what happens to the distribution of cars in the long-run (we will learn how to do this in class but you can also probably just google it). Does this sequence stabilize or does it keep changing after each application? If it does stabilize, how long does it take to stabilize (to within 0.1 cars at each location). Does the starting distribution matter? Try 4 different starting distributions (with a total of 60 cars) and see what the final distribution looks like in each case. For one of your 4 starting distributions, try all 60 cars at one of the locations. 3.9 Adjacency Matrix You can do this problem in R or by hand. Consider the matrix \\(A\\) defined here A = rbind(c( 0 , 1 , 0 , 1 , 1 ,0), c(1 , 0 , 1 , 1 , 0, 0 ),c( 0 , 1 , 0 , 1 , 1, 0 ), c( 1 , 1 , 1 , 0 , 1, 0 ),c( 1 , 0 , 1 , 1 , 0, 1 ), c(0, 0, 0, 0, 1, 0)) A ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0 1 0 1 1 0 ## [2,] 1 0 1 1 0 0 ## [3,] 0 1 0 1 1 0 ## [4,] 1 1 1 0 1 0 ## [5,] 1 0 1 1 0 1 ## [6,] 0 0 0 0 1 0 This matrix represents the connections in the network diagram below. There is a 1 in position \\((i,j)\\) of the matrix if there is a connection (an edge) between vertex \\(i\\) and vertex \\(j\\) and there is a 0 if there is not. Compute \\(A v\\) where \\(v\\) is the vector of all 1’s. Explain what this new vector tells us about the graph. Compute \\(A^2 = A A\\), the square of the matrix \\(A\\). Look at the \\((2,5)\\) entry of \\(A^2\\). Explain what this entry says about connections in the network. Do the same for the \\((2,3)\\) and the \\((2,6)\\) entry of \\(A^2\\). "],["problem-set-4.html", "Section 4 Problem Set 4 4.1 Rainy Day in LA 4.2 Fibonacci Vectors 4.3 Vectors Rescaled 4.4 Polynomial Vector Spaces 4.5 Column and Null Space 4.6 Extend to a basis 4.7 Getting Into a Subspace 4.8 A Vector in Both Col(A) and Nul(A)", " Section 4 Problem Set 4 Due: Wednesday February 17 by noon CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 2.2-2.3 on Matrix Inverses and 4.1-4.3 on Subspaces and Bases. 4.1 Rainy Day in LA In Los Angeles if it rains today, there is a 50% chance it will rain tomorrow, but it if is sunny today, there is a 90% chance it will be sunny tomorrow. This is modeled in the rain-sunshine probability matrix P. \\[ P = \\begin{array}{c|cc|} &amp;\\text{rain}&amp;\\text{sun}\\\\ \\hline \\text{rain}&amp;1/2&amp;1/10\\\\ \\text{sun}&amp;1/2&amp;9/10\\\\ \\hline \\end{array} \\] This matrix works as follows: if the rain-sunshine probability today is (40, 60) (that is, 40% chance rain and 60% chance sunshine), then the rain-sunshine probability tomorrow is (26, 74) as seen by the calculation below. \\[ \\begin{bmatrix} 1/2 &amp; 1/10 \\\\ 1/2 &amp; 9/10 \\\\ \\end{bmatrix} \\begin{bmatrix} 40 \\\\ 60 \\end{bmatrix} = \\begin{bmatrix} 26 \\\\ 74 \\end{bmatrix} \\] Find the rain-sunshine probability the day after tomorrow. Compute \\(P^2\\) and explain the meaning of each of the four entries in the matrix. Find \\(P^{-1}\\) and and use it find the rain-sunshine probability yesterday if the rain-sunshine probability today is (40, 60). 4.2 Fibonacci Vectors The Fibonacci vectors \\(F\\) in \\(\\mathbb{R}^5\\) are defined below: \\[ F = \\left\\{ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} ~\\Bigg\\vert~ \\ x_3 = x_1 + x_2, x_4 = x_2 + x_3, x_5 = x_3 + x_4 \\right\\} \\subseteq \\mathbb{R}^5. \\hskip5in \\] Find a basis for \\(F\\). Be sure to show that your vectors span \\(F\\) and are linearly independent. 4.3 Vectors Rescaled If the function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^n\\) is a linear transformation, then show that the set below is a subspace of \\(\\mathbb{R}^n\\) \\[ E_2 = \\{\\ \\vec{x} \\in \\mathbb{R}^n \\mid T(\\vec{x}) = 2 \\vec{x} \\}. \\] Is there anything special about 2 in the definition? If it were replaced by another scalar, would it still be a subspace? 4.4 Polynomial Vector Spaces This problem refers to the information about the vector space of polynomials \\(\\mathcal{P}_n\\) found in the Day 13 class notes. Here are three subsets of \\(\\mathcal{P}_4\\). Decide if they are subspaces. In each case, if it is not a subspace, give examples using specific polynomials to show that one of the rules is broken, and if it is a subspace, show that the subspace rule holds for any two polynomials \\(p(x), q(x)\\) and any constant \\(c \\in \\mathbb{R}\\). \\(U = \\{ p(x) \\in \\mathcal{P}_4 \\mid p(1) = 0 \\}\\) \\(V = \\{ p(x) \\in \\mathcal{P}_4 \\mid p(0) = 1 \\}\\) \\(W= \\{ p(x) \\in \\mathcal{P}_4 \\mid p&#39;(1) = 0 \\}\\) 4.5 Column and Null Space Find a basis for the column space \\(Col(A)\\) and the null space \\(Nul(A)\\) of the following matrix \\(A\\) below (A = rbind(c(1, 2, 0, 2, 0, -1),c(1, 2, 1, 1, 0, -2), c(2, 4, -2, 6, 1, 2),c(1, 2, 0, 2, -1, -3 ))) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 0 2 0 -1 ## [2,] 1 2 1 1 0 -2 ## [3,] 2 4 -2 6 1 2 ## [4,] 1 2 0 2 -1 -3 4.6 Extend to a basis I am interested in the vectors below. I know that they do not span \\(\\mathbb{R}^5\\), because there are not enough of them, but I want to extend this set to a basis of \\(\\mathbb{R}^5\\) by adding some vectors to the set. \\[ \\begin{bmatrix} 5\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 4\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 1\\\\ 1\\\\ 1\\\\ 1\\\\ 1\\end{bmatrix}. \\] I searched online for ideas and one suggested that I make the matrix below and row reduce it. (A = cbind(c(5,4,3,1,2),c(4,4,3,1,2),c(1,1,1,1,1),diag(5))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 5 4 1 1 0 0 0 0 ## [2,] 4 4 1 0 1 0 0 0 ## [3,] 3 3 1 0 0 1 0 0 ## [4,] 1 1 1 0 0 0 1 0 ## [5,] 2 2 1 0 0 0 0 1 Row reduce this matrix. Use the result to come up with a basis for \\(\\mathbb{R}^5\\) that includes my original 3 vectors Explain why this method works. 4.7 Getting Into a Subspace Let \\(S \\subset \\mathbb{R}^n\\) be a subspace and let \\(\\mathsf{v}, \\mathsf{w} \\in \\mathbb{R}^n\\). For each of the following statements, either give a specific example or explain why it cannot happen. If \\(\\mathsf{v}\\) is in \\(S\\) but \\(\\mathsf{w}\\) is not in \\(S\\), can \\(\\mathsf{v} + \\mathsf{w}\\) be in \\(S\\)? If \\(\\mathsf{v}\\) is not in \\(S\\) and \\(\\mathsf{w}\\) is not in \\(S\\), can \\(\\mathsf{v} + \\mathsf{w}\\) be in \\(S\\)? If \\(\\mathsf{v}\\) is not in \\(S\\) and \\(c\\) is a nonzero constant, can \\(c\\mathsf{v}\\) be in \\(S\\)? 4.8 A Vector in Both Col(A) and Nul(A) Give a \\(3 \\times 3\\) matrix \\(A\\) for which the vector \\(\\mathsf{v} = \\begin{bmatrix}3 \\\\ -2 \\\\ 5 \\end{bmatrix}\\) is in both \\(\\mathrm{Col}(A)\\) and \\(\\mathrm{Nul}(A)\\). Be sure to demonstrate that \\(\\mathsf{v} \\in \\mathrm{Col}(A)\\) and \\(\\mathsf{v} \\in \\mathrm{Nul}(A)\\). "],["problem-set-5.html", "Section 5 Problem Set 5 5.1 A Tale of Two Bases 5.2 Dimension 5.3 Determinant Properties 5.4 Matrix Rank 5.5 A Tetrahedral Basis 5.6 Eigenbasis 5.7 House Party", " Section 5 Problem Set 5 Due: Tuesday February 23 by 11:59am CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 4.4, 4.5, 3.1, 5.1 5.1 A Tale of Two Bases I recommend using R on this problem. Consider the subspace \\(S\\) of \\(\\mathbb{R}^5\\) below. \\[ S = \\textsf{span}\\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 2 \\\\ \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ -1 \\\\ \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 1 \\\\ 3 \\\\ 0 \\\\ -2 \\\\ \\end{bmatrix}, \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 3 \\\\ \\end{bmatrix} \\right) \\] A = cbind(c(1,1,1,1,2),c(1, 2, 3, 0, -1),c(0, 0, 0, 1, 2), c(-1, 1, 3, 0, -2),c(2, 1, 0, 1, 3)) Give a basis of \\(S\\) consisting of some or all of the vectors used to define \\(S\\) above. Give a basis of \\(S\\) that has the nice standard basis property (i.e., the 0s and 1s property). For the two vectors below, decide if they are in \\(S\\). If the vector is in \\(S\\) then give its coordinates in each of your bases from part (b). If you can do one of these “by hand” then explain how. \\[ \\mathbf{w} = \\begin{bmatrix} 8 \\\\ 11 \\\\ 14 \\\\ 7 \\\\ 11 \\end{bmatrix}, \\qquad \\mathbf{v} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\\\ 1 \\\\ 1 \\end{bmatrix}. \\] 5.2 Dimension Find the dimension of the subspace \\(Z\\) of \\(\\mathbb{R}^5\\) of zero-sum vectors below \\[ Z = \\left\\{ \\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} \\ \\Bigg\\vert\\ \\ x_1 + x_2 + x_3 + x_4 + x_5 = 0\\ \\right\\}. \\] 5.3 Determinant Properties Turn in an R-markdown file with your solution to this problem. You can compute determinants in R using the det command. Here you will explore some properties of determinants. (A = rbind(c(3, 0, -1, 1, 2), c(1, 1, -1, 1, 1), c(-2, -3, -2, 3, 1), c(1, 3, 1, 3, 0), c(1, 3, 0, -2, 0))) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 3 0 -1 1 2 ## [2,] 1 1 -1 1 1 ## [3,] -2 -3 -2 3 1 ## [4,] 1 3 1 3 0 ## [5,] 1 3 0 -2 0 Compute the determinant of \\(A\\). Compute the determinant of \\(A^2\\). How does it compare to \\(det(A)\\)? Compute the determinant of \\(A^{-1}\\). How does it compare to \\(det(A)\\)? Swap two rows of \\(A\\) and then compute the determinant of the matrix that you get. Multiply the 4th row of the original matrix \\(A\\) by 7 and then compute the determinant of the matrix you get. How does it compare to \\(det(A)\\)? Compute the determinant of \\(7A\\). How does it compare to \\(det(A)\\)? Let \\(B\\) be the matrix below, and compute \\(det(B)\\), \\(det(A B)\\), and \\(det(A) det(B)\\). (B = rbind(c(1, 2,1, 1, 1), c(1, 2, 0, 1, -1), c(-2, -1, -2,0, 1), c(1, 0, 1, 3, 0), c(1, 0, 0, 1,1))) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 1 1 1 ## [2,] 1 2 0 1 -1 ## [3,] -2 -1 -2 0 1 ## [4,] 1 0 1 3 0 ## [5,] 1 0 0 1 1 5.4 Matrix Rank Fill in the entries of the table with T = true or F = false or I = not enough information to know. (Hint: draw a “picture” of the RREF of \\(\\mathsf{A}\\) in each case). .col2 { columns: 2 200px; /* number of columns and width in pixels*/ -webkit-columns: 2 200px; /* chrome, safari */ -moz-columns: 2 200px; /* firefox */ } .col3 { columns: 3 100px; -webkit-columns: 3 100px; -moz-columns: 3 100px; } (a) \\(\\mathsf{A}\\) is invertible (b) \\(\\mathsf{rref}(\\mathsf{A}) = I\\) (c) \\(\\mathsf{A}\\) has 8 pivots (d) \\(\\mathsf{A} \\mathbf{0} = \\mathbf{0}\\) (e) \\(\\mathsf{A} \\mathsf{x} = \\mathbf{0}\\) has more than one solution. (f) \\(T\\) is one-to-one (g) \\(T\\) is onto (h) \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has at least one solution for all \\(\\mathsf{b} \\in \\mathbb{R}^8\\). (i) The columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^8\\). (j) There is a vector \\(\\mathsf{b} \\in \\mathbb{R}^8\\) such that \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has no solutions. (k) There is a vector \\(\\mathsf{b} \\in \\mathbb{R}^8\\) such that \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has infinitely many solutions. (l) There is a vector \\(\\mathsf{b} \\in \\mathbb{R}^8\\) such that \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) has exactly 17 solutions. (m) There is a vector \\(b \\in \\mathbb{R}^8\\) that can be written as a linear combination of the columns of \\(\\mathsf{A}\\) in more than one way. (n) The rows of \\(\\mathsf{A}\\) span a 7 dimensional subspace of \\(\\mathbb{R}^8\\). (o) The columns of \\(\\mathsf{A}\\) are linearly independent. (p) The rows of \\(\\mathsf{A}\\) are linearly independent \\[ \\begin{array}{|c|c|c|c|c|} \\hline &amp; T: \\mathbb{R}^8 \\to \\mathbb{R}^8 &amp; T: \\mathbb{R}^8 \\to \\mathbb{R}^8 &amp; T: \\mathbb{R}^7 \\to \\mathbb{R}^8 &amp; T: \\mathbb{R}^9 \\to \\mathbb{R}^8 \\\\ &amp; \\text{$\\mathsf{A}$ has rank 7} &amp; \\text{$\\mathsf{A}$ has rank 8} &amp; \\text{$\\mathsf{A}$ has rank 7} &amp; \\text{$\\mathsf{A}$ has rank 8} \\\\ \\hline (a) &amp; &amp; &amp; &amp; \\\\ \\hline (b) &amp; &amp; &amp; &amp; \\\\ \\hline (c) &amp; &amp; &amp; &amp; \\\\ \\hline (d) &amp; &amp; &amp; &amp; \\\\ \\hline (e) &amp; &amp; &amp; &amp; \\\\ \\hline (f) &amp; &amp; &amp; &amp; \\\\ \\hline (g) &amp; &amp; &amp; &amp; \\\\ \\hline (h) &amp; &amp; &amp; &amp; \\\\ \\hline (i) &amp; &amp; &amp; &amp; \\\\ \\hline (j) &amp; &amp; &amp; &amp; \\\\ \\hline (k) &amp; &amp; &amp; &amp; \\\\ \\hline (l) &amp; &amp; &amp; &amp; \\\\ \\hline (m) &amp; &amp; &amp; &amp; \\\\ \\hline (n) &amp; &amp; &amp; &amp; \\\\ \\hline (o) &amp; &amp; &amp; &amp; \\\\ \\hline (p) &amp; &amp; &amp; &amp; \\\\ \\hline \\end{array} \\] 5.5 A Tetrahedral Basis In practice, we change bases because problems are computationally easier in another coordinate system or because we learn something by looking at a problem from the point of view of a different coordinate system. The following example illustrates this with ideas that arises both in chemistry and computer graphics. Below is the tetrahedral molecule methane, \\(\\mathsf{CH}_4\\). Its coordinates can be described in 3-dimensional space by the vectors below. \\[ \\mathsf{C}=\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{H}_1=\\begin{bmatrix} 0 \\\\ 0 \\\\ \\frac{3}{2\\sqrt{6}} \\end{bmatrix}, \\mathsf{H}_2=\\begin{bmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ -\\frac{1}{2} \\\\ -\\frac{1}{2 \\sqrt{6}} \\end{bmatrix}, \\mathsf{H}_3=\\begin{bmatrix} -\\frac{1}{2 \\sqrt{3}} \\\\ \\frac{1}{2} \\\\ -\\frac{1}{2 \\sqrt{6}} \\end{bmatrix}, \\mathsf{H}_4=\\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\0 \\\\-\\frac{1}{2 \\sqrt{6}} \\end{bmatrix} \\] Let \\(\\mathcal{M} = \\{ \\mathsf{H}_1, \\mathsf{H}_2, \\mathsf{H_3} \\}\\). Then \\(\\mathcal{M}\\) is a basis of \\(\\mathbb{R}^3\\), which we will call the tetrahedral basis. You can see from the plot that these vectors are linearly independent (not all on the same plane) Express \\(\\mathsf{H}_4\\) in the tetrahedral basis. Hint: first compute \\(\\mathsf{H}_1+\\mathsf{H}_2+\\mathsf{H}_3+\\mathsf{H}_4\\) using the coordinates above. You do not need to do any row reductions for this question. Give the change of basis matrix \\(T\\) that converts from the tetrahedral basis \\(\\mathcal{M}\\) to the standard basis \\(\\mathcal{S}\\) and compute its inverse that converts from the standard basis back to \\(\\mathcal{M}\\). Chemists are interested in symmetry operations. These are linear transformations such that the atom looks the same after the transformation as it did before. For example one such operation is rotation \\(r_4\\) by 120\\(^o\\) around the \\(\\mathsf{H}_4\\) axis. This rotation sends \\(\\mathsf{H}_1\\) to \\(\\mathsf{H}_3\\), \\(\\mathsf{H}_3\\) to \\(\\mathsf{H}_2\\), and \\(\\mathsf{H}_2\\) to \\(\\mathsf{H}_1\\). Give the matrix of \\(r_4\\) in the \\(\\mathcal{M}\\) basis. r4.M = cbind(c(0,0,0),c(0,0,0),c(0,0,0)) rownames(r4.M) &lt;- c(&quot;H1&quot;,&quot;H2&quot;,&quot;H3&quot;) colnames(r4.M) &lt;- c(&quot;H1&quot;,&quot;H2&quot;,&quot;H3&quot;) r4.M ## H1 H2 H3 ## H1 0 0 0 ## H2 0 0 0 ## H3 0 0 0 It is a pain to describe these transformations in the standard basis, but it is easy and elegant to do so in the methane basis. We can now use the change-of-basis matrix to get the matrix in the standard basis. Compute the rotation in the standard basis by multiplying out these matrices in R. Use your matrices from parts b and c. Give the matrix in the \\(\\mathcal{M}\\)-basis for the following symmetry transformations: (ii) The rotation \\(r_2\\) around the \\(\\mathsf{H}_2\\) axis sending \\(\\mathsf{H}_1\\) to \\(\\mathsf{H}_3\\), \\(\\mathsf{H}_3\\) to \\(\\mathsf{H}_4\\), and \\(\\mathsf{H}_4\\) to \\(\\mathsf{H}_1\\). (iii) The reflection \\(\\sigma_{1,2}\\) across the plane containing \\(\\mathsf{H}_1, \\mathsf{H}_2,\\) and \\(\\mathsf{C}\\). You do not need to give these in standard coordinates. 5.6 Eigenbasis Consider the matrix \\(A\\) below \\[ A = \\frac{1}{3} \\begin{bmatrix} -14 &amp; 13 &amp; -2 \\\\ -20 &amp; 19 &amp; -2 \\\\ -23 &amp; 19 &amp; 1 \\\\ \\end{bmatrix} \\] and consider the following set basis of \\(\\mathbb{R}^3\\) (you don’t have to check that it is a basis). \\[ \\mathcal{B} = \\left\\{ v_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, v_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, v_3 = \\begin{bmatrix} -1 \\\\ -1 \\\\ 2 \\end{bmatrix} \\right\\} \\] By hand, compute \\(A v_1\\), \\(A v_2\\), and \\(A v_3\\) and see that each of these vectors is an eigenvector. From this, find the corresponding eigenvalues \\(\\lambda_1, \\lambda_2, \\lambda_3\\). A basis consisting of eigenvectors of \\(A\\) is called an eigenbasis. We will see that they often, but not always, exist. Let \\(B\\) be the change of basis matrix from \\(\\mathcal{B}\\) to \\(\\mathcal{S}\\) (the standard basis), and compute the product \\(B^{-1} A B\\). You can use R. Just report the matrix \\(B\\) and the matrix \\(B^{-1} A B\\). If you look closely at your answer, you should see something nice. Report what you see. 5.7 House Party Here is a plot of the grey house and four other houses, colored cyan, red, gold, and purple Reproduce this image using homogeneous coordinates. See Homogeneous Coordinates. ############# # your code for 3x3 matrices that create the transformed houses goes here A.red = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.purple = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.gold = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.cyan = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) #################### # you do not need to change this code house = cbind(c(0,0,1), c(0,3/4,1), c(2/4,3/4,1), c(2/4,0,1), c(4/4,0,1), c(4/4,4/4,1), c(5/4,4/4,1), c(0,8/4,1), c(-5/4,4/4,1), c(-4/4,4/4,1), c(-4/4,0,1), c(0,0,1)); plot(house[1,], house[2,], type = &quot;n&quot;, xlim=c(-2.5,2.5),ylim=c(-2.0,3.0),,xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-4:4, v=-4:4, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house.gold = A.gold %*% house polygon(house.gold[1,], house.gold[2,], col = &quot;gold&quot;, border = &quot;blue&quot;) house.cyan = A.cyan %*% house polygon(house.cyan[1,], house.cyan[2,], col = &quot;cyan&quot;, border = &quot;blue&quot;) house.red = A.red %*% house polygon(house.red[1,], house.red[2,], col = &quot;red&quot;, border = &quot;blue&quot;) house.purple= A.purple %*% house polygon(house.purple[1,], house.purple[2,], col = &quot;purple&quot;, border = &quot;blue&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) "],["problem-set-6.html", "Section 6 Problem Set 6 6.1 Rain and Sunshine Revisited 6.2 The Square Root of a Matrix? 6.3 Matrix Reconstruction 6.4 Coyotes and Roadrunners 6.5 Same Eigenvectors 6.6 Hunt Creek 6.7 Glucose-Insulin", " Section 6 Problem Set 6 Due: Wednesday March 03 by 11:59am CST (since Monday is a wellness day). Upload your solutions to Moodle in a PDF. You can download the Rmd source file for this problem set. The Problem Set covers sections 5.1, 5.2, 5.3, 5.6. 6.1 Rain and Sunshine Revisited On PS4, we encountered the rain-sunshine matrix \\(A\\) below \\[ A = \\begin{bmatrix} 1/2 &amp; 1/10 \\\\ 1/2 &amp; 9/10 \\\\ \\end{bmatrix}. \\] Perform the following calculations by hand and show your work. Find the characteristic polynomial of \\(A\\) and find its eigenvalues. Find an eigenvector for each eigenvalue and describe the eigenspaces. Diagonalize \\(A\\). Use your answer to (c) to give a formula for \\(A^n\\) and use this formula to compute \\(\\displaystyle{\\lim_{n\\to \\infty}} A^n\\). Write a loop in R that starts with the vector v = c(1,0) (i.e., a rainy day vector) and applies the matrix A = cbind(c(1/2,1/2),c(1/10,9/10)) over and over again (100 times). Explain how your answer compares to the answer to the previous problem. 6.2 The Square Root of a Matrix? The matrix \\(A =\\begin{bmatrix} 7 &amp; 2 \\\\ -4 &amp; 1 \\end{bmatrix}\\) has characteristic polynomial \\(c(\\lambda) = \\lambda^2 - 8 \\lambda + 15 = (\\lambda -3)(\\lambda - 5).\\) Describe the eigenspaces of \\(A\\). Diagonalize \\(A\\). Find a matrix that makes sense to call \\(\\sqrt{A}\\). Then show that when you square this matrix, you really do get matrix \\(A\\). 6.3 Matrix Reconstruction An unknown \\(3 \\times 3\\) matrix \\(M\\) has eigenvectors and corresponding eigenvalues: \\[ \\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\ \\lambda_1 = 1; \\qquad \\mathsf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix},\\ \\lambda_2 = \\frac{9}{10}; \\qquad \\mathsf{v}_3 = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix},\\ \\lambda_3 = 0. \\] Without using the matrix \\(M\\), compute \\(M^{10} \\mathsf{v}\\) where \\(\\mathsf{v} = \\begin{bmatrix}7\\\\3\\\\4\\end{bmatrix}\\). (That is, use only the eigen-information.) Describe all vectors \\(\\mathsf{v}\\), if there are any, such that \\(M^{n} \\mathsf{v} \\to {\\bf 0}\\) as \\(n \\to \\infty\\). Is it possible to reconstruct \\(M\\) from the evidence given? If so, then do it! If not, explain what further information is needed. 6.4 Coyotes and Roadrunners This summer, Macalester’s Ordway Natural History Study Area will be stocked with a population of coyotes and roadrunners so that Math 236 students can study real-life predator-prey dynamics. From similar experiments, we expect the predator-prey dynamics to be governed by linear model below. The eigenvalues of the matrix are also given. \\[ \\begin{bmatrix} \\phantom{\\Big\\vert} r_{t+1}\\phantom{\\Big\\vert} \\\\ \\phantom{\\Big\\vert} c_{t+1}\\phantom{\\Big\\vert} \\phantom{\\Big\\vert} \\end{bmatrix} =\\left[ \\begin{array}{cc} \\phantom{\\Big\\vert} \\frac{57}{50} &amp; -\\frac{6}{50} \\\\ \\phantom{\\Big\\vert} \\frac{4}{50} &amp; \\frac{43}{50} \\\\ \\end{array} \\right] \\begin{bmatrix} \\phantom{\\Big\\vert} r_t \\phantom{\\Big\\vert} \\\\ \\phantom{\\Big\\vert} c_t \\phantom{\\Big\\vert} \\end{bmatrix} = \\begin{bmatrix} \\phantom{\\Big\\vert} \\frac{57}{50} r_t - \\frac{6}{50} c_t \\\\ \\phantom{\\Big\\vert} \\frac{4}{50} r_t + \\frac{43}{50} c_t \\end{bmatrix}, \\] The eigenvalues and eigenvectors of this matrix are: \\[ \\begin{array}{lcl} \\lambda_1 = \\frac{11}{10} = 1.1, &amp; \\qquad &amp; \\lambda_2 = \\frac{9}{10} = 0.9 \\\\ \\mathsf{v}_1 = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} &amp;&amp; \\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\end{array} \\] A = cbind(c(57/50,4/50),c(-6/50,43/50)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] 0.9486833 0.4472136 ## [2,] 0.3162278 0.8944272 If \\(r_0 = 10\\) roadrunners and \\(c_0 = 15\\) coyotes are introduced to the area, then give for the population of coyotes \\(c_t\\) and roadrunners \\(r_t\\) after \\(t\\) years. In the long-term, in this model, what is the ratio of roadrunners to coyotes? When another college tried the same experiment in their Arboretum, they introduced \\(r_0 = 5\\) roadrunners and \\(c_0 = 10\\) coyotes and both populations died off (as is verified in the computation below). Explain why this happens using the eigenvalues and eigenvectors. A = cbind(c(57/50,4/50),c(-6/50,43/50)) v = c(5,10) for (i in 1:100) {v = A %*% v} v ## [,1] ## [1,] 0.0001328069 ## [2,] 0.0002656140 6.5 Same Eigenvectors Here are two matrices (A = cbind(c(-8, 3, 29), c(-40, 24, 46), c(10, 3, 11))) ## [,1] [,2] [,3] ## [1,] -8 -40 10 ## [2,] 3 24 3 ## [3,] 29 46 11 (B = cbind(c(4, 3, 35), c(-49, 42, 55), c(13, 3, 26))) ## [,1] [,2] [,3] ## [1,] 4 -49 13 ## [2,] 3 42 3 ## [3,] 35 55 26 Use R to show that they have the same eigenvectors but different eigenvalues. Show that \\(A B = B A\\) (even though we know that, in general, matrices do not commute). Now let \\(A\\) and \\(B\\) be any \\(n \\times n\\) matrices which have the same eigenvectors. Prove that \\(AB = BA\\). Hint: use the diagonalization of these two matrices. 6.6 Hunt Creek Age-structured population models like we saw in the Spotted Owl Example are often called Leslie Matrices, named after the British ecologist P.H. Leslie. Here is the Leslie Matrix of a population of brook trout in Hunt Creek in Michigan. The population is categorized into 5 age categories: fingerlings (0,1), yearlings (1-2), young adults (2-3), adults (3-4), and adults (4-5). Right now the population is seen to be dying off. The vector \\(p(t)\\) denotes the population at year \\(t\\) broken into the 5 age categories: \\[p(t) = (f (t), y(t), ya(t), a_1(t), a_2(t))^T\\] and the matrix \\(L\\) gives next year’s population from this year’s population: \\(p_{t+1} = L p_t\\). Below is the Leslie matrix for this example. \\[ \\begin{bmatrix} f (t+1) \\\\ y(t+1) \\\\ ya(t+1) \\\\ a_1(t+1) \\\\ a_2(t+1) \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 37 &amp; 64 &amp; 82 \\\\ 0.06 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0&amp;0.28 &amp; 0 &amp; 0 &amp; 0 \\\\ 0&amp;0&amp;0.16&amp; 0 &amp; 0 \\\\ 0&amp;0&amp;0&amp;0.08&amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} f (t) \\\\ y(t) \\\\ ya(t) \\\\ a_1(t) \\\\ a_2(t) \\end{bmatrix} = \\begin{bmatrix} 37 ya(t) + 64 a_1(t) + 82 a_2(t) \\\\ 0.06 f(t) \\\\ 0.28y(t) \\\\ 0.16 ya(t) \\\\ 0.08 a_1(t) \\end{bmatrix} \\] L = cbind(c(0,.06,0,0,0),c(0,0,.28,0,0),c(37,0,0,.16,0),c(64,0,0,0,.08),c(82,0,0,0,0.00)) The trout population in the creek is known to be dying off largely due to poisoning by the insecticide rotenone. The model demonstrates this behavior here, as can be seen in the folowing plot, which starts with 200 trout in each age group. You shouldn’t need to edit this plot. start = c(200,200,200,200,200) # the starting distribution N = 35 # N is the number of iterations X = matrix(0,nrow=5,ncol=N) # Store the results in a 3 x N matrix called X X[,1] = start # put start in the first column of X # loop N times and put your results in X for (i in 2:N) {X[,i] = L %*% X[,i-1]} # Then plot the results t = seq(1,N) # time plot(t,X[1,],type=&#39;l&#39;,col=1,ylim=c(0,8000),ylab=&quot;population&quot;,xlab=&quot;time (year)&quot;, main=&quot;Population in Age Group&quot;) for (i in 1:5) { lines(t,X[i,],col=i) points(t,X[i,],col=i,pch=20,cex=.8)} legend(22, 7600, legend=c(&quot;Fingerlings (0-1)&quot;, &quot;Yearlings (1-2)&quot;, &quot;Young Adults (2-3)&quot;,&quot;Adults (3-4)&quot;,&quot;Adults (4-5)&quot;), col=1:5, lty=1) Give the meaning of the values 37, 64, 82, 0.06, 0.28, 0.16, 0.08 that appear in this matrix. Compute the eigenvectors and eigenvalues of \\(L\\) and relate what you find to population dynamics. In particular, use the eigen-information to Give the overall population growth rate. Give the limiting age distribution: that is, the long-run distribution of the population into the different age categories. Give your answer as proportions which sum to 1. You are seeking funding from the Michigan DNR to support a cleanup effort. As part of your proposal, you argue that you believe that such a cleanup will most impact the youngest fish and will improve the survival rate of fingerlings to yearlings. Figure out (by trial and error) how high this survival rate will need to grow in order for the population to stop dying off. Justify your answer with eigenvalues and a plot. You should just be able to duplicate the code for the plot above (after changing the matrix). 6.7 Glucose-Insulin The hormone insulin helps regulate glucose metabolism in your blood. The presene of insulin helps your body absorb excess glucose. Here \\(G_t\\) (glucose) and \\(H_t\\) (insulin) are measued as excess values (in mg per 100 ml of blood) above the steady state. \\[ \\begin{bmatrix} G_{t+1} \\\\ H_{t+1} \\end{bmatrix} = \\begin{bmatrix} 0.9 &amp; -0.4 \\\\ 0.1 &amp; 0.9 \\\\ \\end{bmatrix} \\begin{bmatrix} G_t \\\\ H_t \\end{bmatrix}= \\begin{bmatrix} 0.9 G_t - 0.4 H_t \\\\ 0.1 G_t + 0.9 H_t \\end{bmatrix} \\] Here is what happens if we start at \\((1,0)\\) and iterate. That is we start with 1 unit excess glucose. Observe that the system spirals back to the steady state of \\((0,0)\\). We can plot the indivdual glucose and insulin coordinates over time. These are the x and y coordinates of the points in the above plot. You see the insulin responding to the excess glucose, and then the glucose being absorbed by the presence of insulin, and so on … The key point here is that the spiraling in the (x,y) plane or oscillating in the (x,t) plane corresponds to the presence of complex eigenvalues. Your job is to perform an eigen-analysis of this problem: Give a trajectory plot of this matrix using the trajectory_plot command from Dynamical Systems in 2D. Use R to find the eigenvalues and eigenvectors. Write out the eigenvalues in the form \\(\\lambda = a \\pm b i\\) and the eigenvectors in the form \\(\\vec{v} = \\vec{u} \\pm \\vec{w} i.\\) Use this information to find the scaling factor \\(|\\lambda|\\) for this matrix and the angle of rotation \\(\\arctan(b/a)\\). Give your answer in degrees. Compare your answers from part (d) to the plots above to confirm that the system is doing what the eigenvalues predict. "],["important-definitions.html", "Section 7 Important Definitions 7.1 Vector Spaces 7.2 Matrices 7.3 Orthogonality 7.4 Spectral Decompostion", " Section 7 Important Definitions 7.1 Vector Spaces span A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span a vector space \\(V\\) if for every \\(\\mathsf{v} \\in V\\) there exist a set of scalars (weights) \\(c_1, c_2, \\ldots, c_n \\in \\mathbb{R}\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, \\ldots, c_n]^{\\top}\\) is a solution to \\(A x = \\mathsf{v}\\). linear independence A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\ldots, \\mathsf{v}_n\\) are linearly independent if the only way to write \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] is with \\(c_1 = c_2 = \\cdots = c_n = 0\\). Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(A x = \\mathsf{0}\\) has only the trivial solution. linear dependence Conversely, a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly dependent if there exist scalars \\(c_1, c_2,\\ldots, c_n \\in \\mathbb{R}\\) that are not all equal to 0 such that \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] This is called a dependence relation among the vectors. Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, c_2, \\ldots, c_n]^{\\top}\\) is a nontrivial solution to \\(A \\mathsf{x} = \\mathsf{0}\\). linear transformation A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation when: \\(T(\\mathsf{u} + \\mathsf{v}) = T(\\mathsf{u}) + T(\\mathsf{v})\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\) (preserves addition) \\(T(c \\mathsf{u} ) = c T(\\mathsf{u})\\) for all \\(\\mathsf{u} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\) (preserves scalar multiplication). It follows from these that also \\(T(\\mathsf{0}) = \\mathsf{0}\\). one-to-one A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a one-to-one when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at most one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). onto A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a onto when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at least one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). subspace A subset \\(S \\subseteq \\mathbb{R}^n\\) is a subspace when: \\(\\mathsf{u} + \\mathsf{v} \\in S\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in S\\) (closed under addition) \\(c \\mathsf{u} \\in S\\) for all \\(\\mathsf{u}\\in S\\) and \\(c \\in \\mathbb{R}\\) (closed under scalar multiplication) It follows from these that also \\(\\mathsf{0} \\in S\\). basis A basis of a vector space (or subspace) \\(V\\) is a set of vectors \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) in \\(V\\) such that \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span \\(V\\) \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly independent Equivalently, one can say that \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) is a basis of \\(V\\) if for every vector \\(\\mathsf{v} \\in V\\) there is a unique set of scalars \\(c_1, \\ldots, c_n\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] (The fact that there is a set of vectors comes from the span; the fact that they are unique comes from linear independence). dimension The dimension of a subspace \\(W\\) is the number of vectors in any basis of \\(W\\). This is also the fewest number of vectors required to span the subspace. 7.2 Matrices invertible The square \\(n \\times n\\) matrix \\(A\\) is invertible when there exists an \\(n \\times n\\) matrix \\(A^{-1}\\) such that \\(A A^{-1} = I = A^{-1} A\\). The Invertible Matrix Theorem collects over two dozen equivalent conditions, each of which guarantees that \\(A\\) is invertible. null space The null space \\(\\mbox{Nul}(A) \\subset \\mathbb{R}^n\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of solutions to the homogeneous equation \\(A \\mathsf{x} = \\mathbf{0}\\)&gt; We also write this as \\[ \\mbox{Nul}(A) = \\{ \\mathsf{x} \\in \\mathbb{R}^n : A \\mathsf{x} = \\mathbf{0} \\} \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the kernel of \\(T\\) is the null space of matrix \\(A\\). column space The column space \\(\\mbox{Col}(A) \\subset \\mathbb{R}^m\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of all linear combinations of the columns of \\(A\\). For \\(A = \\begin{bmatrix} \\mathsf{a}_1 &amp; \\mathsf{a}_2 &amp; \\cdots &amp; \\mathsf{a}_n \\end{bmatrix}\\), we have \\[ \\mbox{Col}(A) = \\mbox{span} ( \\mathsf{a}_1, \\mathsf{a}_2, \\ldots , \\mathsf{a}_n ) \\] We also write this as \\[ \\mbox{Col}(A) = \\{ \\mathsf{b} \\in \\mathbb{R}^m : \\mathsf{b} = A \\mathsf{x} \\mbox{ for some } \\mathsf{x} \\in \\mathbb{R}^n \\}. \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the range (also called the image) of \\(T\\) is the column space of matrix \\(A\\). rank The rank of the \\(m \\times n\\) matrix \\(A\\) is the dimension of the column space of \\(A\\). This is also the number of pivot columns of the matrix. eigenvalue and eigenvector For a square \\(n \\times n\\) matrix \\(A\\), the scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue for \\(A\\) when there exists a nonzero vector \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(A \\mathsf{x} = \\lambda \\mathsf{x}\\). The nonzero vector \\(\\mathsf{x}\\) is the eigenvector for eigenvalue \\(\\lambda\\). The collection of all of these eigenvalues and eigenvectors is called the eigensystem of A. diagonalization A square \\(n \\times n\\) matrix is diagonalizable when \\(A = P D P^{-1}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an invertible matrix. In this case, the eigenvalues of \\(A\\) are the diagonal entries of \\(D\\) and their corresponding eigenvectors are the columns of \\(P\\). dominant eigenvalue The eigenvalue \\(\\lambda\\) of the square matrix \\(A\\) is the dominant eigenvalue when \\(| \\lambda | &gt; | \\mu |\\) where \\(\\mu\\) is any other eigenvalue of \\(A\\). The dominant eigenvalue determines the long-term behavior of \\(A^t\\) as \\(t \\rightarrow \\infty\\). 7.3 Orthogonality length The length of a vector \\(\\mathsf{v}\\) is \\[ \\| \\mathsf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}. \\] distance and angle The distance between vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) is \\[ \\mbox{dist}(\\mathsf{u},\\mathsf{v}) = \\| \\mathsf{u} - \\mathsf{v} \\|. \\] The angle \\(\\theta\\) between these vectors is determined by \\[ \\cos \\theta = \\frac{\\mathsf{u} \\cdot \\mathsf{v}}{ \\| \\mathsf{u} \\| \\, \\| \\mathsf{v} \\|}. \\] orthogonal The vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are orthogonal when \\(\\mathsf{u} \\cdot \\mathsf{v} = 0\\). This means that either one of them is the zero vector, or they are perpendicular to one another. orthogonal complement If \\(W \\subset \\mathbb{R}^n\\) is a subspace, then its orthogonal complement \\(W^{\\perp}\\) is the set of all vectors in \\(\\mathsf{R}^n\\) that are orthogonal to \\(W\\). We also write \\[ W^{\\perp} = \\{ \\mathsf{v} \\in \\mathbb{R}^n : \\mathsf{v} \\cdot \\mathsf{w} \\mbox{ for all } \\mathsf{w} \\in W \\}. \\] orthonormal set A collection of vectors \\(\\mathsf{u}_1, \\mathsf{u}_2, \\ldots, \\mathsf{u}_k\\) are an orthonormal set when every vector has length 1 and the vectors are pairwise orthogonal. orthogonal matrix orthogonal matrix A square \\(n \\times n\\) matrix \\(P\\) is an orthogonal matrix when its columns are an orthonormal set. As a result, we have \\(P^{-1} = P^{\\top}\\). projection and residual The orthogonal projection of vector \\(\\mathsf{y}\\) into a subspace \\(W\\) is the unique vector \\(\\hat{\\mathsf{y}} \\in W\\) such that \\(\\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}} \\in W^{\\perp}\\). The vector \\(\\mathsf{z}\\) is called the residual vector for the projection. 7.4 Spectral Decompostion orthogonal diagonalization Every symmetric \\(n \\times n\\) matrix is orthogonally diagonalizable, meaning that we have \\(A = P D P^{\\top}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix. The diagonal entries of \\(D\\) are the eigenvalues of \\(A\\) and the columns of \\(P\\) are the corresponding orthonormal eigenvectors. Furthermore, the eigenvalues of \\(A\\) are nonnegative. spectral decomposition A symmetric matrix \\(A\\) can be written as a linear combination of rank 1 matrices derived from the orthonormal eigensystem of \\(A\\). In particular, we have \\[ A = \\lambda_1 \\mathsf{u}_1 \\mathsf{u}_1^{\\top} + \\lambda_2 \\mathsf{u}_2 \\mathsf{u}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{u}_n \\mathsf{u}_n^{\\top}. \\] This linear combination of rank 1 vectors is called the spectral decomposition of \\(A\\). singular value decomposition (SVD) Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be factored into its singular value decomposition \\(U \\Sigma V^{\\top}\\) where \\(U\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\Sigma\\) is a matrix whose nonzero entries are the positive numbers \\(\\sigma_1, \\ldots , \\sigma_r\\), which appear in decreasing order on the diagonal, and \\(V\\) is an \\(n \\times n\\) orthogonal matrix. The nonzero entries of \\(\\Sigma\\) are called the singular values of \\(A\\). The columns of \\(U\\) are the left singular vectors and the rows of \\(V^{\\top}\\) are the right singular vectors. SVD spectral decomposition Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be written as a linear combination of rank 1 matrices derived from the singular value decomposition of \\(A\\). In particular, we have \\[ A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\sigma_r \\mathsf{u}_r \\mathsf{v}_r^{\\top}. \\] This linear combination of rank 1 vectors is called the (SVD) spectral decomposition of \\(A\\). "],["week-1-learning-goals.html", "Section 8 Week 1 Learning Goals 8.1 Solving Linear Equations 8.2 RStudio 8.3 Vocabulary 8.4 Conceptual Thinking", " Section 8 Week 1 Learning Goals Here are the knowledge and skills you should master by the end of this first, shorter week. 8.1 Solving Linear Equations I should be able to do the following tasks: Identify linear systems from nonlinear systems Create a linear system to solve a variety of applied scenarios Convert between a linear system and an augmented matrix Row reduce an augmented matrix into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Use REF to determine whether a linear system is consistent or inconsistent Use REF to determine whether a consistent system has a unique solution or an infinite number of solutions Use RREF to find explicit equations for the solution set of a consistent system 8.2 RStudio I should be able to do the following tasks: Log in to Macalester’s RStudio server Upload R Markdown files to RStudio Knit R Markdown to produce HTML Use RStudio to create vectors and matrices Use the rref command from pracma to solve a linear system 8.3 Vocabulary I should know and be able to use and explain the following terms: elementary row operation (and be able to state them) augmented matrix REF and RREF pivot position basic variable (pivot variable) free variable consistent system and inconsistent system 8.4 Conceptual Thinking I should understand and be able to perform the following conceptual tasks: Model 2-dimensional linear systems as the intersections of lines Model 3-dimensional linear systems as the intersections of planes "],["week-2-learning-goals.html", "Section 9 Week 2 Learning Goals 9.1 Solution Sets, Span and Linear Independence 9.2 Vocabulary 9.3 Conceptual Thinking", " Section 9 Week 2 Learning Goals Here are the knowledge and skills you should master by the end the second week. 9.1 Solution Sets, Span and Linear Independence I should be able to do the following tasks: Go back and forth between (i) systems of equations, (ii) vector equations, and (iii) the matrix equation \\(Ax = b\\). Compute and understand the matrix-vector product \\(A x\\) both as a linear combination of the columns of A and as the dot product of \\(x\\) with the rows of \\(A\\). Write the solution set to \\(Ax=b\\) as a parametric vector equation. Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) 9.2 Vocabulary I should know and be able to use and explain the following terms or properties. \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) homogeneous and nonhomogeneous equations parametric vector equations linear independence and linear dependence 9.3 Conceptual Thinking I should understand and be able to explain the following concepts: Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b \\in \\mathbb{R}^m\\), the system \\(A x = b\\) has at least one solution Each \\(b \\in \\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. Understand the relation between homogeneous solutions and nonhomogeneous solutions. Linear independence Span More than \\(n\\) vectors in \\(\\mathbb{R}^n\\) must be linearly dependent. "],["week-3-learning-goals.html", "Section 10 Week 3 Learning Goals 10.1 Linear Transformations and Matrix Inverses 10.2 Vocabulary 10.3 Conceptual Thinking", " Section 10 Week 3 Learning Goals Here are the knowledge and skills you should master by the end the third week. 10.1 Linear Transformations and Matrix Inverses I should be able to do the following tasks: Determine whether a mapping from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) is a linear transformation. Use the RREF of the corresponding matrix to determine whether \\(T(\\mathsf{x})\\) is one-to-one and/or onto. Describe 2D linear transformations as a mixture of geometric operations, including expansion, contraction, reflection, rotation, shearing and dimension reduction. Perform a 2D translation using 3D homogeneous coordinates. Multiply an \\(m \\times n\\) matrix with an \\(n \\times p\\) matrix to get an \\(m \\times p\\) matrix. Determine whether a \\(2 \\times 2\\) matrix is invertible. Find the inverse of a \\(2 \\times 2\\) matrix by hand. Use RStudio to check for invertiblity and to find the inverse of an \\(n \\times n\\) square matrix. Explain the connection between Gaussian Elimination, elementary matrices, and the matrix inverse. 10.2 Vocabulary I should know and be able to use and explain the following terms or properties. linear transformation: \\(T(a \\mathsf{u} + b \\mathsf{v}) = a T(\\mathsf{u}) + b T(\\mathsf{v})\\) domain, codomain (aka target) and range (aka image) \\(T\\) maps vector \\(\\mathsf{x}\\) to its image \\(T(\\mathsf{x})\\) one-to-one onto standard matrix for a linear transformation homogeneous coordinates transpose of a matix invertible matrix elementary matrices 10.3 Conceptual Thinking I should understand and be able to explain the following concepts: A linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) corresponds to multiplication by an \\(m \\times n\\) matrix \\(A\\). \\(T(\\mathsf{x})=\\mathsf{A} \\mathsf{x}\\) is a one-to-one linear transformations if and only \\(\\mathsf{A}\\) has linearly independent columns \\(T(\\mathsf{x})=\\mathsf{A} \\mathsf{x}\\) is an onto linear transformations if and only if the columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^m\\). The Invertible Matrix Theorem (Section 2.3, Theorem 8, page 112) is one of the highlights of the course! It gives 12 different conditions that all equivalent! You should think deeply about why everything comes together like this for square matrices. "],["week-4-learning-goals.html", "Section 11 Week 4 Learning Goals 11.1 Vector Spaces and the Determinant 11.2 Vocabulary 11.3 Conceptual Thinking", " Section 11 Week 4 Learning Goals Here are the knowledge and skills you should master by the end of the fourth week. 11.1 Vector Spaces and the Determinant I should be able to do the following tasks: Prove/disprove that a subset of a vector space is a subspace. Prove/disprove that a set of vectors is linearly dependent. Prove/disprove that a set of vectors span a vector space (or a subspace). Find the kernel and image of \\(T(\\mathsf{x}) = Ax\\). Determine whether a set of vectors is a basis. Find a basis for \\(\\mathrm{Nul}(A)\\) and a basis for \\(\\mathrm{Col}(A)\\). Find the change-of-coordinate matrix \\(P_{\\mathcal{B}}\\) from basis \\({\\mathcal{B}}\\) to the standard basis \\(\\mathcal{S}\\). Use matrix inverses (and RStudio) to find the change-of-coordinate matrix \\(P_{\\mathcal{B}}^{-1}\\) from basis \\({\\mathcal{S}}\\) to the standard basis \\(\\mathcal{B}\\). Find the coordinate vector with respect to a given basis. Find the dimension of a vector space (or subspace) by finding or verifying a basis. Find the determinant of a \\(2 \\times 2\\) matrix by hand. Find the determinant of a \\(3 \\times 3\\) matrix by using row operations/cofactor expansion/permutation method. Use RStudio to calculate the determinant of a square matrix. Use \\(\\det(A)\\) to decide whether the square matrix \\(A\\) is invertible. 11.2 Vocabulary I should know and be able to use and explain the following terms or properties. every one of these Important Definitions subspace null space and column space of a matrix kernel and image of a linear transformation basis coordinate vector with respect to a basis change-of-coordinates matrix the coordinate vector with respect to a basis the dimension of a vector space (or a subspace) determinant 11.3 Conceptual Thinking I should understand and be able to explain the following concepts: A vector space consists of a collection of vectors and all of their linear combinations. A subspace is a subset of a vector space that is also a vector space by itself (closed under linear combinations). The solutions to \\(A \\mathsf{x} = \\mathbb{0}\\) form a subspace. The span of the columns of \\(A\\) form a subspace. How the kernel and image of \\(T(\\mathsf{x}) = Ax\\) correspond to the nullspace and columnspace of \\(A\\). Every basis of a given vector space (or subspace) contains the same number of vectors. Why every vector in a vector space has a unique representation as a linear combination of a given basis \\({\\mathcal{B}}\\). How dimension relates to span and linear independence. Interpret \\(\\det(A)\\) as a measure the expansion/contraction of “volumes” in \\(\\mathbb{R}^n\\) under the linear transformation \\(T(\\mathsf{x})=A\\mathsf{x}\\). "],["week-5-6-learning-goals.html", "Section 12 Week 5-6 Learning Goals 12.1 Eigensystems 12.2 Vocabulary 12.3 Conceptual Thinking", " Section 12 Week 5-6 Learning Goals Here are the knowledge and skills you should master by the end of the fifth and sixth weeks. 12.1 Eigensystems I should be able to do the following tasks: Check whether a given vector \\(\\mathsf{v}\\) is an eigenvector for square matrix \\(A\\). Find the eigenvalues of a matrix \\(2 \\times 2\\) matrix by hand, using the characteristic equation Find the eigenvalues of a triangular matix by inspection. Given the eigenvalues of matrix \\(A\\), find the eigenvectors by solving \\((A \\lambda I) = \\mathbf{0}\\). Find the eigenvalues and (“human readable”) eigenvectors of an \\(n \\times n\\) matrix \\(A\\) using eigen(A) on RStudio. Determine whether a matrix is diagonalizable. Factor a diagonalizable \\(n \\times n\\) matrix as \\(A = PDP^{-1}\\) where \\(D\\) is a diagonal matrix of eigenvalues and \\(P\\) is the matrix whose columns are the corresponding eignvectors. Use RStudio to find complex eigenvalues and (“human readable”) eigenvectors of a square matrix. Factor a \\(2 \\times 2\\) scaling-rotation matrix as \\(A = P C P^{-1}\\) where \\(C\\) is a scaling-rotation matrix \\(\\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix}\\) and \\(P = [ \\mathsf{w}, \\mathsf{u}]\\) where \\(\\mathsf{v} = \\mathsf{u} + i \\mathsf{w}\\) is the eigenvector for \\(\\lambda = a + b i\\). Use RStudio to create a 2D plot of pairs of eigenvalues of a square matrix Use the dominant eigenvalue and dominant eigenvector to determine the long-term behavior of a dynamical system Create a trajectory of a \\(2 \\times 2\\) dynamical system (either using RStudio or by using a given vector field plot) and then relate the trajectory to the eigenvectors and eigenvalues Use RStudio to investigate the animal population modeled with a Leslie matrix. 12.2 Vocabulary I should know and be able to use and explain the following terms or properties. eigenvalue, eigenvector and eigenspace characteristic equation diagonalizable matrix similar matrices algebraic multiplicity of an eigenvalue geometric multiplicity of an eigenvalue scaling-rotation matrix discrete dynamical system trajectory dominant eigenvalue and dominant eigenvector population model Leslie matrix 12.3 Conceptual Thinking I should understand and be able to explain the following concepts: An eigenspace of \\(A\\) is a subspace that is fixed under the linear transformation \\(T(\\mathsf{x}) = A \\mathsf{x}\\). An eigenvalue \\(\\lambda\\) with \\(1 &lt;| \\lambda |\\) corresponds to expansion. An eigenvalue \\(\\lambda\\) with \\(0 &lt; | \\lambda | &lt; 1\\) corresponds to contraction. A complex eigenvalue corresponds to a rotation in a 2D subspace. The eigenspace for \\(\\lambda\\) is the subspace \\(\\mathrm{Nul}(A - \\lambda I)\\). A matrix is not diagonalizable when it has complex eigenvalues. A matrix is not diagonalizable when it has an eigenvalue whose algebraic mutiplicity is strictly larger than its geometrix multiplicity. The long-term behavior of a dynamical system is determined by its dominant eigenvalue and eigenvector. Population model predicts one of: long term growth, extinction, convergence to a stable population. "],["week-7-8-learning-goals.html", "Section 13 Week 7-8 Learning Goals 13.1 Orthogonality and SVD 13.2 Vocabulary 13.3 Conceptual Thinking", " Section 13 Week 7-8 Learning Goals Here are the knowledge and skills you should master by the end of the seventh and eighth weeks. 13.1 Orthogonality and SVD I should be able to do the following tasks: Find the length of a vector Find the distance between two vectors Normalize a vector Find the cosine of the angle between two vectors Find the orthogonal projection of one vector onto another Find the orthogonal projection of one vector onto a subspace (using an orthogonal basis) Find the orthogonal complement of a subspace Use the Gram-Schmidt process to create an orthonormal basis (starting from a given basis) Find the least squares approximation for an inconsistent system Formulate a curve fitting problem as an inconsistent linear system \\(A \\mathsf{x} = \\mathsf{b}\\) Orthogonally diagonalize a symmetric matrix as \\(A=PDP^{\\top}\\). Find the spectral decomposition \\(A = \\lambda_1 \\mathsf{v}_1 \\mathsf{v}_1^{\\top} + \\lambda_2 \\mathsf{v}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{v}_n \\mathsf{v}_n^{\\top}\\) of a symmetric matrix \\(A\\) Use an orthogonal diagonalization to find the best rank \\(k\\) approximation of symmetric matrix \\(A\\) Find the singular value decomposition \\(A = U \\Sigma V^{\\top}\\) of a rectangular matrix \\(A\\) Use the singular value decomposition \\(A = U \\Sigma V^{\\top}\\) to find orthonormal bases for \\(\\mbox{Row}(A), \\mbox{Nul}(A), \\mbox{Col}(A), \\mbox{Nul}(A^{\\top})\\) Find the SVD spectral decomposition \\(A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{u}_r \\mathsf{v}_r^{\\top}\\) of a rank \\(r\\) matrix \\(A\\) Use SVD to find the best rank \\(k\\) approximation of a matrix \\(A\\) 13.2 Vocabulary I should know and be able to use and explain the following terms or properties. dot product of two vectors \\(\\mathsf{v} \\cdot \\mathsf{w} = \\mathsf{v}^{\\top} \\mathsf{w}\\) (aka scalar product, inner product) length (magnitude) of a vector angle between vectors normalize unit vector orthogonal vectors orthogonal complement of a subspace orthogonal projection orthogonal basis orthonormal basis Gram-Schmidt process normal equations for a least squares approximation least squares solution residual vector symmetric matrix orthogonally diagonalizable outer product of two vectors \\(\\mathsf{v} \\, \\mathsf{w}^{\\top}\\) spectral decomposition of a symmetric matrix singular value decomposition singular value, left singular vector, right singular vector SVD spectral decomposition of a rectangular matrix image compression 13.3 Conceptual Thinking I should understand and be able to explain the following concepts: The dot product gives an algebraic encoding of the geometry (lengths and angles) of \\(\\mathbb{R}^n\\) If two vectors are orthogonal, then they are perpendicular, or one of them is the zero vector An orthogonal projection is a linear transformation The row space of a matrix is orthogonal to its nullspace The inverse of orthogonal matrix \\(A\\) is the transpose \\(A^{\\top}\\) Cosine similarity is a useful way to compare vectors, especially in high-dimensional vector spaces. The residual vector measures the quality of fit of a least squares solution The outer product \\(\\mathsf{v}\\, \\mathsf{w}^{\\top}\\) is a square matrix with rank 1 Singular value decomposition is a generalization of orthogonal diagonalization The singular values of \\(A\\) are the square roots of the eigenvalues of \\(A^{\\top}A\\). The spectral decomposition of a matrix allows us to approximate a matrix with a linear combination of rank 1 matrices. The relative magnitudes of the eigenvectors/singular values indicate the quality of the spectral decomposition approximation. "],["linear-systems-in-r.html", "Section 14 Linear Systems in R 14.1 Getting started with R 14.2 Building Vectors and Matrices 14.3 Solving a Linear System 14.4 Solving another Linear System 14.5 Appendix: Dimensionless Vectors in R", " Section 14 Linear Systems in R 14.1 Getting started with R To use RStudio, you have two choices: Use the cloud version by logging in to Rstudio.macalester.edu. This is the easiest way to use RStudio and works great for our course. You can also download the free desktop version of RStudio. If you plan to go on to take more MSCS classes, especially in statistics and data science, you may want to use the desktop version. Download the desktop version following the instructions here: rstudio.com/products. Now, let’s learn how to use R to solve systems of linear equations! Download this Rmd file. First, we will create vectors and matrices Then we will see how to create an augmented matrix and then apply Gaussian Elimination to obtain is reduced row echelon form. Gaussian elimination is performed by the rref() command. However, this command is not loaded into R by default. So we have have to tell RStudio to use the practical math package, which is known as pracma. So we need to run the following command once at the beginning of our session. require(pracma) 14.2 Building Vectors and Matrices A vector in R is a list of data. The simplest way to create a vector is to use the c() command. The letter ‘c’ is short for ‘combine these values into a vector.’ For example, we can make a vector v for the numbers 1,2,3 as follows: v=c(1,2,3) v ## [1] 1 2 3 Note that we had to ask R to display the value of v. This is because the assignment of v doesn’t echo the value to the console. But can see the value of v in the Environment tab in the upper right panel of RStudio. For example, run this command and then check to see that the value of v gets updated in the environment. v=c(1,2,3,4,5,6) It is interesting to note that c() returns a dimensionless vector. So you can treat a vector c() as either a row or a column when you construct a matrix. For example, suppose that we want to make the matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] We could create this matrix by binding three row vectors: A = rbind(c(1,1,1), c(2,4,8), c(3,9,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 or we could bind three column vectors: A = cbind(c(1,2,3), c(1,4,9), c(1,8,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 14.3 Solving a Linear System Suppose that we want to solve the linear system \\[\\begin{aligned} x + y + z &amp;= 7 \\\\ 2x + 4y + 8z &amp;= 6 \\\\ 3x +9y+27z &amp;=12 \\end{aligned}\\] which has coefficient matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] and target (column) vector \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix}. \\] This is the same matrix A we defined above. Let’s define a vector b and use cbind() to create an augmented matrix which we will name Ab. (We could have just made the full augmented matrix from the start, but using cbind to add a column to a matrix is a skill we will use later in the course!) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 b = c(4,6,12) Ab = cbind(A,b) Ab ## b ## [1,] 1 1 1 4 ## [2,] 2 4 8 6 ## [3,] 3 9 27 12 Now we use the rref() command to apply Gaussian Elimination to produce the reduced row echelon form. (And remember: we had to load this function into R by using the require(pracma) command above.) rref(Ab) ## b ## [1,] 1 0 0 7 ## [2,] 0 1 0 -4 ## [3,] 0 0 1 1 We conclude that this is a consistent system no free variables. The unique solution is \\[\\begin{align} x&amp;=7\\\\ y&amp;=-4\\\\ z&amp;=1 \\end{align}\\] We can verify that our answer works by multiplying \\(A\\) by one of the solutions above. Matrix multiplication uses the funny operation %*%. A %*% c(7,-4,1) ## [,1] ## [1,] 4 ## [2,] 6 ## [3,] 12 #A %*% c(1,2,3,0,0) Which matches our target \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix} \\] just as we had hoped. 14.4 Solving another Linear System Now let’s find the solution set for the linear system \\[ \\begin{array}{rrrrrcr} x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ 2x_1 &amp; +x_2 &amp; +2x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; 4 \\\\ -x_1 &amp; +x_2 &amp; +x_3 &amp; &amp; &amp; = &amp; 10 \\\\ x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ \\end{array} \\] which corresponds to augmented matrix \\[ \\left[ \\begin{array}{rrrrr|r} 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ 2 &amp; +1 &amp; +2 &amp; -1 &amp; -1 &amp; 4 \\\\ -1 &amp; +1 &amp; +1 &amp; &amp; &amp; 10 \\\\ 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ \\end{array} \\right] \\] This time, let’s just construct the augmented matrix direclty. Then we define the coefficient matrix \\(A\\). Here we use cbind to combine the vectors into the columns of a matrix named \\(A\\). You can use rbind if you want to combine the vectors into the rows of a matrix. Ab = cbind(c(1,2,-1,1),c(0,1,1,0),c(-1,2,1,-1),c(-1,1,0,-1),c(-1,5,0,-1),c(-2,10,4,-2)) Ab ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 -1 -1 -1 -2 ## [2,] 2 1 2 1 5 10 ## [3,] -1 1 1 0 0 4 ## [4,] 1 0 -1 -1 -1 -2 And now let’s row reduce to get RREF. rref(Ab) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 1 1 ## [2,] 0 1 0 -1 -1 2 ## [3,] 0 0 1 1 2 3 ## [4,] 0 0 0 0 0 0 So the set of solutions in parametric form is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ 1 \\\\ -2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] and this is a “plane” in \\(\\mathbb{R}^5\\). It is in \\(\\mathbb{R}^5\\) because these vectors have 5 coordinates. It is a plane because it is spanned by two vectors that are not on the same line. 14.5 Appendix: Dimensionless Vectors in R Let’s revisit the vector constructed by cbind. Above we called this a “dimensionless” vector because it can be used as a column vector or a row vector. In general, R will do its best to make sense of a dimensionless vector. In other words, it will promote c() to make an expression valid. For example, let \\(A\\) be an \\(n \\times n\\) matrix, and let \\(b\\) be a vector. The expression \\(Av\\) is only defined when \\(v\\) is a \\(n \\times 1\\) column vector and that \\(wA\\) is only defined when \\(w\\) is a \\(1 \\times n\\) ** row vector**. But let’s look at what happens when we use a dimensionless vector instead. A = cbind(c(1,1,1),c(-1,0,1), c(0,1,-1)) A ## [,1] [,2] [,3] ## [1,] 1 -1 0 ## [2,] 1 0 1 ## [3,] 1 1 -1 b = c(2,5,11) b ## [1] 2 5 11 # A times b A %*% b ## [,1] ## [1,] -3 ## [2,] 13 ## [3,] -4 # b times A b %*% A ## [,1] [,2] [,3] ## [1,] 18 9 -6 Both of these multiplications worked! So R treated b as a column vector for the multiplicationA %*% b. And then R treated b as a row vector for the multiplication b %b% A. So how do you make a true column vector or a true row vector? The answer is to use cbind and rbind! Here are some examples: # dimensionless b = c(1,2,3,4) b ## [1] 1 2 3 4 # column vector b.col = cbind(b) b.col ## b ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 # row vector b.row = rbind(b) b.row ## [,1] [,2] [,3] [,4] ## b 1 2 3 4 "],["linear-dependence.html", "Section 15 Linear Dependence 15.1 Example 1: a 7x9 integer matrix 15.2 A 5 x 6 Numerical Matrix 15.3 Random Matrices", " Section 15 Linear Dependence In this activity, we will explore linear dependence and independence in the context of solving nonhomogeneous \\(A x = b\\) and homogeneous equations \\(A x = 0\\). Download this Rmd file. Remember that we will use the pracma package to get the rref function, so we first load it in: require(&quot;pracma&quot;) 15.1 Example 1: a 7x9 integer matrix Here is a 7 x 9 coeefficient matrix that we will use. These commands define it and echo it back. A = cbind( c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24)) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 And here is a vector b that we hope to use in solving A x = b. b = c(382, 51, -321, -314, -86, -170, -153) b ## [1] 382 51 -321 -314 -86 -170 -153 You can augment A with b, and call it Ab, using cbind: Ab = cbind(A,b) Ab ## b ## [1,] 3 5 3 4 0 -4 5 0 37 382 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 51 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 -321 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 -314 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 -86 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 -170 ## [7,] 1 4 5 -3 8 3 -1 0 -24 -153 And row reduce using rref rref(Ab) ## b ## [1,] 1 0 0 0 5 0 0 0 -2 8 ## [2,] 0 1 0 0 -2 0 0 0 2 10 ## [3,] 0 0 1 0 1 0 0 0 -3 -19 ## [4,] 0 0 0 1 -2 0 0 0 3 21 ## [5,] 0 0 0 0 0 1 0 0 0 6 ## [6,] 0 0 0 0 0 0 1 0 6 61 ## [7,] 0 0 0 0 0 0 0 1 0 8 15.1.1 Solution to the nonhomogeneous equations Ax = b Write out the solution to Ax=b in parametric form using the following formatting. You just need to fill in the correct values of the vectors: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 10 \\\\ -19 \\\\ 21 \\\\ 0 \\\\ 6 \\\\ 61 \\\\ 8 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} -5 \\\\ 2 \\\\ -1 \\\\ 2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} 2 \\\\ -2 \\\\ 3 \\\\ -3 \\\\ 0 \\\\ 0 \\\\ -6 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] Describe this solution space (by fixing up this sentence, which is incorrect right now): the set of solutions to A x= b is a plane in \\(\\mathbb{R}^9\\). 15.1.2 Solution to the nonhomogeneous equations Ax = 0 Now, describe the set of solutions to the homogeneous equations A x = 0. Again, you can just edit this: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} p1 \\\\ p2 \\\\ p3 \\\\ p4 \\\\ p5 \\\\ p6 \\\\ p7 \\\\ p8 \\\\ p9 \\end{bmatrix} + s \\begin{bmatrix} u1 \\\\ u2 \\\\ u3 \\\\ u4 \\\\ u5 \\\\ u6 \\\\ u7 \\\\ u8 \\\\ u9 \\end{bmatrix} + t \\begin{bmatrix} v1 \\\\ v2 \\\\ v3 \\\\ v4 \\\\ v5 \\\\ v6 \\\\ v7 \\\\ v8 \\\\ v9 \\end{bmatrix} \\] And describe, in words, the geometric relationship between the solutions to Ax=b and Ax=0. your answer here 15.1.3 Linearly dependent columns The columns of the matrix A are linearly dependent. You can see that in rref(A). rref(A) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Discuss in your group how you see it. Then write out a dependence relation among the columns by filling in numbers for the weights in this equation \\[ 0 = c_1 \\vec{a}_1 + c_2 \\vec{a}_2 + c_3 \\vec{a}_3 + c_4 \\vec{a}_4 + c_5 \\vec{a}_5 + c_6 \\vec{a}_6 + c_7 \\vec{a}_7 + c_8 \\vec{a}_8 + c_9 \\vec{a}_9. \\] Challenge: give a dependency relation that none of the other groups in the class have. This is telling us that there is some redundancy in the matrix A. Remove columns from A to get a new matrix M whose columns are linearly independent. You can do this by removing the appropriate columns from the code below: M = cbind( # you need to edit this matrix c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24) ) M ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 rref(M) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Your matrix should now be square (7x7) with linearly independent columns. R has a build in solve command, solve, that works for matrices of this form (i.e., square with linearly independent columns). You can try it here. First you need to un-comment-out the solve command. I have it commented out right now, because it does not work with the matrix M (above) until you remove its redundancies. # solve(M,b) Now, you should get a unique solution to the equation M x = b, since M has no free variables, and it should be one of the solutions to the original question A x = b. Which solution is it? That is, which of the many solutions to A x = b are you getting here (forw which values of the paramters?). Compare this with trying to use solve on the original equation A x = b with linearly dependent columns. The solve command in the next bit of code is commented out. Delete the comment command and try executing it. # solve(A,b) 15.2 A 5 x 6 Numerical Matrix So far, all of the matrices we’ve worked with in this class have integer values. This is only so that the calulations are nice to do by hand. All of our theory works over the real numbers. Here we will look at a real matrix with numerical values, something you might find when dealing with real-world data. B = cbind( c(0.717, -0.274, 0.365, 0.482, -0.362), c(0.587, -0.545, 0.5, -0.407, -0.597), c(-0.441, 0.886, 0.784, -0.831, -0.594), c(0.923, -0.466, 0.222, 0.867, 0.493), c(-0.42, -0.745, -0.02, -0.44, 0.209), c(0.621, 0.049, -0.134, -0.844, -0.31) ) B ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.717 0.587 -0.441 0.923 -0.420 0.621 ## [2,] -0.274 -0.545 0.886 -0.466 -0.745 0.049 ## [3,] 0.365 0.500 0.784 0.222 -0.020 -0.134 ## [4,] 0.482 -0.407 -0.831 0.867 -0.440 -0.844 ## [5,] -0.362 -0.597 -0.594 0.493 0.209 -0.310 and here is a vector d in \\(\\mathbb{R}^5\\). d = c(5.886, -4.001, 3.701, -6.621, -2.199) d ## [1] 5.886 -4.001 3.701 -6.621 -2.199 Try answering some of these questions: Are the columns of B linearly independent? Do the columns of B span \\(\\mathbb{R}^5\\)? Give the parametric solution to B x = d. What is the geometric form of this solution (e.g., a plane in \\(\\mathbb{R}^4\\))? Remove redundancies from the columns of B to get a new matrix B2 and use solve to solve the equation B2 x = d. Which of the parametric solutions to you get. 15.3 Random Matrices The following code generates a random 5 x 5 matrix. Every time you enter it, it will give you a new matrix. Use this to try to figure out how likely it is that a random square matrix has linearly dependent columns. R1 = matrix(runif(5*5), nrow = 5, ncol = 5) R1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.4017099 0.5925975 0.7029120 0.8834429 0.3389149 ## [2,] 0.9628397 0.6352139 0.8557174 0.5329521 0.7410602 ## [3,] 0.3693616 0.4929142 0.4230346 0.9889505 0.8122403 ## [4,] 0.1662429 0.7823658 0.6851453 0.6190364 0.5844715 ## [5,] 0.9431595 0.4429706 0.4531959 0.5591870 0.9528636 Try the same using the following code that generates a random 5 x 6 matrix. R2 = matrix(runif(5*6), nrow = 5, ncol = 6) R2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.5198429 0.6562038 0.6504391 0.54066058 0.4287286 0.2075979 ## [2,] 0.8769594 0.5819127 0.6828241 0.67957386 0.5456600 0.1647130 ## [3,] 0.8036129 0.5744560 0.7201219 0.38968260 0.9291340 0.4479268 ## [4,] 0.6081769 0.4650714 0.2288190 0.80555324 0.3196861 0.5886934 ## [5,] 0.6003209 0.7528477 0.2033427 0.05244186 0.8774471 0.3884654 Try the same using the following code that generates a random 5 x 4 matrix. R3 = matrix(runif(5*4), nrow = 5, ncol = 4) R3 ## [,1] [,2] [,3] [,4] ## [1,] 0.6565079 0.01589044 0.5003482 0.1097602 ## [2,] 0.2742474 0.99595055 0.3441915 0.3774686 ## [3,] 0.6881732 0.91399739 0.2653110 0.4607757 ## [4,] 0.1301978 0.44570673 0.5843880 0.2252262 ## [5,] 0.8516054 0.29114316 0.8274085 0.7488964 rref(R3) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## [5,] 0 0 0 0 In each of these cases, how likely is it that the columns of the matrix spans all of \\(\\mathbb{R}^4\\)? "],["matrix-multiplication.html", "Section 16 Matrix Multiplication", " Section 16 Matrix Multiplication Download this Rmd file from GitHub Here we will practice multiplying matrices in R. First, let’s define a few matrices.I’m using a trick here. By putting the assignment in parentheses, it assigns the matrix and displays it.s (A = cbind(c(1,2,3),c(4,5,6),c(1,1,-1))) ## [,1] [,2] [,3] ## [1,] 1 4 1 ## [2,] 2 5 1 ## [3,] 3 6 -1 (B = cbind(c(1,-1,1),c(1,1,1),c(0,2,1))) ## [,1] [,2] [,3] ## [1,] 1 1 0 ## [2,] -1 1 2 ## [3,] 1 1 1 (C = cbind(c(2,1,1),c(1,0,1),c(1,-3,1),c(3,2,1))) ## [,1] [,2] [,3] [,4] ## [1,] 2 1 1 3 ## [2,] 1 0 -3 2 ## [3,] 1 1 1 1 We multiply using the %*% command. As seen here: A %*% B ## [,1] [,2] [,3] ## [1,] -2 6 9 ## [2,] -2 8 11 ## [3,] -4 8 11 Note that B %*% A ## [,1] [,2] [,3] ## [1,] 3 9 2 ## [2,] 7 13 -2 ## [3,] 6 15 1 What do these last two multiplications say about the matrix product AB and BA? This is a very important property (or, perhaps, lack of property) of matrix multiplication. Try multiplying BC and CB. What happens? And why? The transpose of a matrix is computed by t(A). Compute the transpose of the matrices A, B, C and be sure that you all understand what it does. The command diag(n) gives the n x n identity matrix. This is denoted \\(I_n\\). For example, here is \\(I_3\\). diag(3) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Compute \\(I_2\\), \\(I_4\\), and \\(I_5\\) and be sure you all agree on what the identity matrix is. Multiply the matrices A and B by the appropriately-sized identity matrix. Multiply both ways, A I and I A, and agree upon what multiplying by the identity does. Multiply C by an identity matrix I C and C I. You might need a different size on the left and on the right. Our topic for Thursday (tomorrow) is the inverse of a matrix. You compute the inverse of the matrix A with solve(A). Try this solve(B) ## [,1] [,2] [,3] ## [1,] -0.5 -0.5 1 ## [2,] 1.5 0.5 -1 ## [3,] -1.0 0.0 1 B %*% solve(B) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Multiply A by its inverse and look closely at the answer you get. solve(A) ## [,1] [,2] [,3] ## [1,] -1.8333333 1.6666667 -0.1666667 ## [2,] 0.8333333 -0.6666667 0.1666667 ## [3,] -0.5000000 1.0000000 -0.5000000 A %*% solve(A) ## [,1] [,2] [,3] ## [1,] 1.000000e+00 0.000000e+00 0.000000e+00 ## [2,] 3.330669e-16 1.000000e+00 -1.110223e-16 ## [3,] 1.110223e-16 -2.220446e-16 1.000000e+00 Some matrices do not have inverses. Try computing the inverse of the following matrices. We will discuss this tomorrow! (M1 = cbind(c(3,5),c(-2,1))) ## [,1] [,2] ## [1,] 3 -2 ## [2,] 5 1 (M2 = cbind(c(4,3),c(5,4))) ## [,1] [,2] ## [1,] 4 5 ## [2,] 3 4 (M3 = cbind(c(4,2),c(10,5))) ## [,1] [,2] ## [1,] 4 10 ## [2,] 2 5 Enter the matrix A in problem 3.7 in the homework. Then compute the matrix G which is A times its transpose. Discuss its meaning. "],["linear-transformations-of-a-house.html", "Section 17 Linear Transformations of a House 17.1 Rotations 17.2 Expansion and contraction 17.3 Reflection 17.4 Shear Transformations 17.5 Dimension Reduction 17.6 Your Turn", " Section 17 Linear Transformations of a House Download this Rmd file from GitHub Here is a plot of my house. You will need to run this chunk of code each time you re-start R to get the house back in memory. house = cbind(c(0,0), c(0,3/4), c(1/2,3/4), c(1/2,0), c(1,0), c(1,1), c(5/4,1), c(0,2), c(-5/4,1), c(-1,1), c(-1,0), c(0,0)); plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) Here we explore linear transformations on the plane by looking at their effect on my house. We give a series of examples of 2D linear transformations. After each example, it’s your turn to play with variations from the same family of transformations. 17.1 Rotations Suppose that we wish to rotate my house by pi/3 radians. As we’ve seen, a 2D rotation matrix by \\(t\\) radians, counter-clockwise, is given by \\[A=\\displaystyle{ \\begin{bmatrix} \\cos(t) &amp; -\\sin(t) \\\\ \\sin(t) &amp; \\cos(t) \\end{bmatrix}}.\\] Here is the code to display this transformation. Observe that I apply the matrix A to the house, call it house2 and plot both the original house and the new house in the same plot. # define the matrix A. This is the only part you should need to edit. t = pi/3 A = cbind(c(cos(t),sin(t)),c(-sin(t),cos(t))) A # display the matrix A ## [,1] [,2] ## [1,] 0.5000000 -0.8660254 ## [2,] 0.8660254 0.5000000 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) Try changing the angle above to achieve different rotations. How can you rotate it clockwise? 17.2 Expansion and contraction Next, we scale the house by 2 in the \\(x\\)-direction and by 3 in the \\(y\\)-direction. # define the matrix A. This is the only part you should need to edit. A = cbind(c(2,0),c(0,3)) A # display the matrix A ## [,1] [,2] ## [1,] 2 0 ## [2,] 0 3 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) Your turn: try some different scale factors. What if you use negative scale factors. 17.3 Reflection Now we reflect over the line y = x. # define the matrix A. This is the only part you should need to edit. A = cbind(c(0,1),c(1,0)) A # display the matrix A ## [,1] [,2] ## [1,] 0 1 ## [2,] 1 0 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) Your turn: try the reflections (1) over the x-axis; (2) over the y-axis; and (3) through the origin, i.e., sending (x,y) to (-x,-y). 17.4 Shear Transformations A shear transformation is of the form \\[ A=\\displaystyle{ \\begin{bmatrix} a &amp; b \\\\ 0 &amp; c \\end{bmatrix}} \\quad \\mbox{and} \\quad A=\\displaystyle{ \\begin{bmatrix} a &amp; 0 \\\\ b &amp; c \\end{bmatrix}} \\] For example: # define the matrix A. This is the only part you should need to edit. A = cbind(c(1,0),c(1,1)) A # display the matrix A ## [,1] [,2] ## [1,] 1 1 ## [2,] 0 1 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) You try: try to get the house to slant in the other direction. 17.5 Dimension Reduction Here we perform the transformation that sends \\(\\mathsf{e}_1\\) to \\((-1,1/2)\\) and \\(\\mathsf{e}_2\\) to \\((2,-1)\\). Notice that they are the same line and the transformation projects the house onto this line. # define the matrix A. This is the only part you should need to edit. A = cbind(c(-1,1/2),c(2,-1)) A # display the matrix A ## [,1] [,2] ## [1,] -1.0 2 ## [2,] 0.5 -1 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 17.6 Your Turn See if you can do the transformations in problem 3.3. # define the matrix A. This is the only part you should need to edit. A = cbind(c(1,0),c(0,1)) A # display the matrix A ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) "],["homogeneous-coordinates.html", "Section 18 Homogeneous Coordinates 18.1 Translation 18.2 Rotation and then Translation 18.3 Your Turn", " Section 18 Homogeneous Coordinates Download this Rmd file A translation of the plane shifts every vector by a constant vector. For example, the mapping \\[ S \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\right) = \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} x +3 \\\\ y - 4 \\end{bmatrix} \\] translates every vector in the plane by \\(\\begin{bmatrix} 3 \\\\ -4~ \\end{bmatrix}\\). The bad news: This is a simple and natural mapping, but it is not a linear transformation! We know that a linear transformation must map \\(\\mathbb{0}\\) to \\(\\mathbb{0}\\), and that is certainly not the case when we translate! This restriction is rather limiting for computer graphics: we can never move our image away from the origin. The good news: We can work around this problem by creating a 3D linear transformation \\(T: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3\\) and then retricting our attention to a plane in this larger space. As discussed in the Homogeneous Coordinates video, we do the following: Embed the \\(xy\\)-plane \\(\\mathbb{R}^2\\) into the plane \\(z = 1\\) in \\(\\mathbb{R}^3\\). Translate in \\(\\mathbb{R}^3\\) using a mapping \\(T\\) that maps this horizontal plane to itself. That is: \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\right) = \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}. \\] When we create our plot, we use only the first two coordinates and ignore the third coordinate (which is still 1). In summary, during our calculations, we replace the vector \\(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\) in \\(\\mathbb{R}^2\\) with the homogeneous coordinate vector \\(\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\) in \\(\\mathbb{R}^3\\). 18.1 Translation Here is my house once again. Note that we have add \\(z=1\\) as the third coordinate to each point. However, when plotting, we only use the first two coordinates. # the third entry always = 1 house = cbind(c(0,0,1), c(0,3/4,1), c(1/2,3/4,1), c(1/2,0,1), c(1,0,1), c(1,1,1), c(5/4,1,1), c(0,2,1), c(-5/4,1,1), c(-1,1,1), c(-1,0,1), c(0,0,1)); # only plot the first two coordinates plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) Next, we translate by \\(\\begin{bmatrix} 3 \\\\ - 4 \\end{bmatrix}\\) by using the linear transformation \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\right) = \\begin{bmatrix} 1 &amp; 0 &amp; 3 \\\\ 0 &amp; 1 &amp; -4 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}= \\begin{bmatrix} x+3 \\\\ y - 4 \\\\ 1 \\end{bmatrix}. \\] Now, let’s do this calculation in R and plot the first two coordiantes: A = cbind(c(1,0,0),c(0,1,0),c(3,-4,1)) house2 = A %*% house # only plot the first two coordinates plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 18.2 Rotation and then Translation We know plenty of interesting 2D linear transformations, including rotation, reflection and shear mappings. We can turn any of them into a 3D transformation by appending a row and a column with a 1 in the lower right corner and zero everywhere else. For example, the 2D rotation \\[ \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta~ \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix} \\] becomes the 3D transformation \\[ \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta~ &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp;0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] This mapping rotates 3D space around the \\(z\\)-axis. So let’s combine two operations: a rotation and a translation First, let’s rotate counterclockwise by \\(2 \\pi/3\\) and then translate by \\(\\begin{bmatrix} -2 \\\\ 4 \\end{bmatrix}\\). And remember: the matrix closest to the vector acts first. So if we want to translate first, the translation matrix needs to be to the right of the rotation matrix: \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\right) = \\begin{bmatrix} 1 &amp; 0 &amp; -2 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\cos \\frac{2\\pi}{3} &amp; -\\sin\\frac{2\\pi}{3}~ &amp; 0 \\\\ \\sin\\frac{2\\pi}{3} &amp; \\cos\\frac{2\\pi}{3} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}. \\] t = 2*pi/3 rot = cbind(c(cos(t),sin(t),0),c(-sin(t),cos(t),0),c(0,0,1)) rot ## [,1] [,2] [,3] ## [1,] -0.5000000 -0.8660254 0 ## [2,] 0.8660254 -0.5000000 0 ## [3,] 0.0000000 0.0000000 1 translate = cbind(c(1,0,0),c(0,1,0),c(-2,4,1)) translate ## [,1] [,2] [,3] ## [1,] 1 0 -2 ## [2,] 0 1 4 ## [3,] 0 0 1 A = translate %*% rot A ## [,1] [,2] [,3] ## [1,] -0.5000000 -0.8660254 -2 ## [2,] 0.8660254 -0.5000000 4 ## [3,] 0.0000000 0.0000000 1 house3 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house3[1,], house3[2,], col = &quot;green&quot;, border = &quot;black&quot;) 18.3 Your Turn 18.3.1 Translation and then Rotation Let’s reverse the order of these matrices in the previous example and see that we get a different transformation. Remember, order matters: matrix multiplication is not commutative. So try changing the problem to first translating and then rotating. But first, in your group, try guessing where the house will go and then do it in R. Does the house end up where you expected it to be? 18.3.2 House of Orange Here is a picture of a gray house and a larger, upside-down orange house. Work as a group to reproduce this image using homogeneous coordinates. You will have to use a combination of translation, rotation, and expansion. You will do this by multiplying three matrices. Think carefully and experiment. Remember that the order of your matrices matters, and the rightmost one happens first. ############################# # your code defining the 3x3 matrices A1 and A2 A1 = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A2 = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A3 = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A = A3 %*% A2 %*% A1 ############################# # you do not need to change this code plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) house2 = A %*% house abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house2[1,], house2[2,], col = &quot;orange&quot;, border = &quot;green&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) 18.3.3 House Party This problem is on PS5. Here is a plot of the grey house and four other houses, colored cyan, red, gold, and purple Reproduce this image using homogeneous coordinates. ############# # your code for 3x3 matrices that create the transformed houses goes here A.red = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.purple = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.gold = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.cyan = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) #################### # you do not need to change this code house = cbind(c(0,0,1), c(0,3/4,1), c(2/4,3/4,1), c(2/4,0,1), c(4/4,0,1), c(4/4,4/4,1), c(5/4,4/4,1), c(0,8/4,1), c(-5/4,4/4,1), c(-4/4,4/4,1), c(-4/4,0,1), c(0,0,1)); plot(house[1,], house[2,], type = &quot;n&quot;, xlim=c(-2.5,2.5),ylim=c(-2.0,3.0),,xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-4:4, v=-4:4, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house.gold = A.gold %*% house polygon(house.gold[1,], house.gold[2,], col = &quot;gold&quot;, border = &quot;blue&quot;) house.cyan = A.cyan %*% house polygon(house.cyan[1,], house.cyan[2,], col = &quot;cyan&quot;, border = &quot;blue&quot;) house.red = A.red %*% house polygon(house.red[1,], house.red[2,], col = &quot;red&quot;, border = &quot;blue&quot;) house.purple= A.purple %*% house polygon(house.purple[1,], house.purple[2,], col = &quot;purple&quot;, border = &quot;blue&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) "],["eigenvectors.html", "Section 19 Eigenvectors 19.1 Computing Eigenvectors and Eigenvalues 19.2 Diagonalization 19.3 Rental Car Example 19.4 Northern Spotted Owl", " Section 19 Eigenvectors Download this Rmd file 19.1 Computing Eigenvectors and Eigenvalues To compute eigenvalues and eigenvectors in R we use the eigen command. For example if our matrix is (A = cbind(c(-14,-20,-23),c(13,19,19),c(-2,-2,1))) ## [,1] [,2] [,3] ## [1,] -14 13 -2 ## [2,] -20 19 -2 ## [3,] -23 19 1 Then we compute its eigenvalues and eigenvectors as eigen(A) ## eigen() decomposition ## $values ## [1] 6 3 -3 ## ## $vectors ## [,1] [,2] [,3] ## [1,] -0.2672612 -0.4082483 0.5773503 ## [2,] -0.5345225 -0.4082483 0.5773503 ## [3,] -0.8017837 0.8164966 0.5773503 One thing to notice about the eigenvectors is that they are scaled to have length one (they are unit vectors). So they often do not look like what we expect. Note for example that the first vector above is a multiple of \\((1,2,3)^T\\), the second is a multiple of \\((-1,-1,2)^T\\) and the third is a multiple of \\((1,1,1)^T\\). We can extract the eigenvectors and eigenvalues as follows vals = eigen(A)$values vecs = eigen(A)$vectors Then, for example, we can see if a vector is an eigenvector as follows. Here I will check the first eigenvalue and first eigenvector: lambda1 = vals[1] v1 = vecs[,1] A %*% v1 ## [,1] ## [1,] -1.603567 ## [2,] -3.207135 ## [3,] -4.810702 lambda1 * v1 ## [1] -1.603567 -3.207135 -4.810702 From this, we see that \\(A v_1 = \\lambda_1 v_1\\). Recall that every scalar multiple of an eigenvector is also an eigenvector of that same eigenvalue. The vectors are currently scaled to have length 1. Another useful scaling is to have them sum to 1. You can accomplish this by dividing them by the sum of their entries. For example, v1 = v1/sum(v1) v1 ## [1] 0.1666667 0.3333333 0.5000000 19.2 Diagonalization In class we diagonalized a few matrices. Here we show how to do this in R. Here is the first matrix from the checkpoint question CP-5.3. (A = cbind(c(-5,-6,-7),c(6,7,8),c(-2,-2,-2))) ## [,1] [,2] [,3] ## [1,] -5 6 -2 ## [2,] -6 7 -2 ## [3,] -7 8 -2 vals = eigen(A)$values vals ## [1] 1.000000e+00 -1.000000e+00 -3.766534e-15 vecs = eigen(A)$vectors vecs ## [,1] [,2] [,3] ## [1,] -0.2672612 0.5773503 -0.6666667 ## [2,] -0.5345225 0.5773503 -0.6666667 ## [3,] -0.8017837 0.5773503 -0.3333333 solve(vecs) %*% A %*% vecs ## [,1] [,2] [,3] ## [1,] 1.000000e+00 3.355686e-15 -6.554309e-16 ## [2,] 6.661338e-15 -1.000000e+00 8.881784e-15 ## [3,] 4.272762e-15 -3.076740e-15 1.776357e-15 Here, we are diagonalizing \\(A\\) by multiplying \\(P^{-1} A P = D\\) where \\(P\\) is the matrix of eigenvectors and \\(D\\) is the diagonal matrix of eigenvalues. We can use zapsmall to round or “zap” very small numbers to 0, and it then looks more like what we are expecting. zapsmall(solve(vecs) %*% A %*% vecs) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 -1 0 ## [3,] 0 0 0 Now we diagonalize the second matrix from CP-5.3 You will recall that this one has a repeated eigenvalue (algebraic multiplicity 2), but it has a 2-dimensional eigenspace (geometric multiplicity 2), so it is digonalizable. B = cbind(c(3,-1,2),c(-1,3,2),c(2,2,0)) eigen(B) ## eigen() decomposition ## $values ## [1] 4 4 -2 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.9128709 0.0000000 -0.4082483 ## [2,] -0.1825742 0.8944272 -0.4082483 ## [3,] 0.3651484 0.4472136 0.8164966 vals = eigen(B)$values vecs = eigen(B)$vectors zapsmall(solve(vecs) %*% B %*% vecs) ## [,1] [,2] [,3] ## [1,] 4 0 0 ## [2,] 0 4 0 ## [3,] 0 0 -2 The third matrix from CP-5.3 is not diagonalizable. It has an eigenvalue of algebraic multiplicity 2 and geometric multiplicity 1. Note that it gives the same two eigenvectors for \\(v_2\\) and \\(v_3\\), because the eigenspace \\(E_2\\) is only 1 dimensional. C = cbind(c(3,-1,1),c(2,2,1),c(1,1,2)) eigen(C) ## eigen() decomposition ## $values ## [1] 3 2 2 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.8017837 0.5773503 0.5773503 ## [2,] -0.2672612 -0.5773503 -0.5773503 ## [3,] 0.5345225 0.5773503 0.5773503 19.3 Rental Car Example In problem PS 3.8, you studied a transition matrix for where cars got returned if they are rented from one of three rental locations: St. Paul, Rochester, and Duluth. Note that the columns of this matrix are probablilities, and as such, the are nonnegative and sum to 1. Such a matrix is called a Markov or stochastic matrix. StP = c(.85,.09,.06) Roch = c(.30,.60,.10) Dul = c(.35,.05,.60) M = cbind(StP,Roch,Dul) rownames(M) &lt;- c(&quot;StP&quot;,&quot;Roch&quot;,&quot;Dul&quot;) M ## StP Roch Dul ## StP 0.85 0.3 0.35 ## Roch 0.09 0.6 0.05 ## Dul 0.06 0.1 0.60 In this assignment you imagined that there were 20 cars at each location, i.e., v = cbind(20,20,20) and you applied the matrix over and over again to this vector watching it converge to a steady state. v = c(20,20,20) for (i in 1: 100) { v = M %*% v } v ## [,1] ## StP 40.969163 ## Roch 10.308370 ## Dul 8.722467 Stochastic matrices always have eigenvalue \\(\\lambda = 1\\). As can be seen here: eigen(M) ## eigen() decomposition ## $values ## [1] 1.000+0.000000i 0.525+0.037081i 0.525-0.037081i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.9497414+0i 0.6201737+0.0000000i 0.6201737+0.0000000i ## [2,] 0.2389672+0i -0.3100868-0.4599331i -0.3100868+0.4599331i ## [3,] 0.2022030+0i -0.3100868+0.4599331i -0.3100868-0.4599331i Note that the first eigenvalue is 1, and that the second and third eigenvalues (and eigenvectors) are complex and have both real and imaginary parts. If some of the eigenvalues have imaginary parts, then it outputs them all in complex form. Here we extract just the real part of the first eigenvector (since its imaginary part is 0), and we rescale it both to sum to 1 and to sum to 60. v = Re(eigen(M)$vectors[,1]) (v = v /sum(v)) ## [1] 0.6828194 0.1718062 0.1453744 60*v ## [1] 40.969163 10.308370 8.722467 Notice that this is the exact same as the steady-state vector that we got by iterating. The steady-state that this system wants to be in — with 40.97 cars in St. Paul, 10.31 cars in Rochester, and 8.72 cars in Duluth. It makes sense that a steady-state vector is an eigenvector with eigenvalue \\(\\lambda = 1\\). That the system converges to this state is eigen-magic that we will learn about soon. 19.4 Northern Spotted Owl This is the opening example in Chapter 5 of the textbook on page 265. It comes from a 1992 study of the northern spotted owl, which was threatened with extinction due to the loss of forest habitat due to logging in the Pacific Northwest. This is currently a story featured in an NPR Podcast called Timber Wars. 19.4.1 The Dynamical System The vector \\[ x_n = \\begin{bmatrix} j_n \\\\ s_n \\\\ a_n \\end{bmatrix} \\] is an age-stage vector in which \\(j_n, s_n\\), and \\(a_n\\) are the number of female owls in the juvenile (up to 1 year), subadult (1-2 year), and adult (over 2 year) age groups in year \\(n\\). The dynamics that take us from one year to the next is given by, the recursive relation \\(x_{n+1} = A x+n\\), where \\(A\\) is the matrix shown here. This is an age-stage matrix model that was published in Conservation Biology. \\[ \\begin{bmatrix} j_{n+1} \\\\ s_{n+1} \\\\ a_{n+1} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 0.33 \\\\ 0.18 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.71 &amp; 0.94 \\end{bmatrix} \\begin{bmatrix} j_n \\\\ s_n \\\\ a_n \\end{bmatrix} \\] If we multiply this system out, we get \\[ \\begin{array} {rcl} j_{n+1} &amp;=&amp; 0.33 a_n \\\\ s_{n+1} &amp;=&amp; 0.18 j_n \\\\ a_{n+1} &amp;=&amp; 0.71 s_n + 0.94 a_n \\end{array} \\] We see that, in this model, 0.33 represents the fertility or fecundity rate. That is, it is the proportion of new juveniles next year to adults this year (the proportion of offspring the adult population is producing). The 0.18 is the survival rate from juvenile to subadult, 0.71 is the survival rate from subadult to adult, and 0.94 proportion of adults that survive from one year to the next. To see the dynamics play out over time, we will start with an original population of owls is distributed into age groups as follows. \\[ x_0 = \\begin{bmatrix} 100 \\\\ 76 \\\\ 502 \\end{bmatrix} \\] We will write a loop to apply the matrix \\(A\\) over and over again. This time we will make a table and store each value in the table. A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix x0 = c(100,76,502) # the inital value N = 10 # iterate N=10 times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } X # display the table ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 100 165.66 173.5272 167.3330 164.27953 161.74110 159.0937 156.47643 153.90912 151.38419 ## [2,] 76 18.00 29.8188 31.2349 30.11993 29.57032 29.1134 28.63687 28.16576 27.70364 ## [3,] 502 525.84 507.0696 497.8168 490.12454 482.10222 474.1710 466.39127 458.73997 451.21326 ## [,11] ## [1,] 148.90038 ## [2,] 27.24915 ## [3,] 443.81005 Having saved the information, we can now plot the data, Note that it appears to support the claim the claim that the owls are threatened with extinction. tot = X[1,] + X[2,] + X[3,] t = seq(1,N+1) plot(t,X[1,],type=&#39;l&#39;,col=&#39;blue&#39;,ylim=c(0,1000),ylab=&quot;population&quot;,xlab=&quot;year&quot;,main=&quot;Spotted Owl Population&quot;) points(t,X[1,],col=&#39;blue&#39;,pch=20,cex=.8) lines(t,X[2,],col=&#39;orange&#39;) points(t,X[2,],col=&#39;orange&#39;,pch=20,cex=.8) lines(t,X[3,],col=&#39;red&#39;) points(t,X[3,],col=&#39;red&#39;,pch=20,cex=.8) points(t,tot,col=&#39;black&#39;,pch=20,cex=.8) lines(t,tot,col=&#39;black&#39;) legend(8, 1050, legend=c(&quot;juvenile&quot;, &quot;subadults&quot;, &quot;adults&quot;,&quot;total&quot;), col=c(&#39;blue&#39;,&#39;orange&#39;,&#39;red&#39;,&#39;black&#39;), lty=1) Let’s run the iteration further. This time, we won’t display the table (gets too big), and we will just show the plot of 100 iterations A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix x0 = c(100,76,502) # the inital value N = 100 # iterate N=10 times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } tot = X[1,] + X[2,] + X[3,] t = seq(1,N+1) plot(t,X[1,],type=&#39;l&#39;,col=&#39;blue&#39;,ylim=c(0,1000),ylab=&quot;population&quot;,xlab=&quot;year&quot;,main=&quot;Spotted Owl Population&quot;) points(t,X[1,],col=&#39;blue&#39;,pch=20,cex=.8) lines(t,X[2,],col=&#39;orange&#39;) points(t,X[2,],col=&#39;orange&#39;,pch=20,cex=.8) lines(t,X[3,],col=&#39;red&#39;) points(t,X[3,],col=&#39;red&#39;,pch=20,cex=.8) points(t,tot,col=&#39;black&#39;,pch=20,cex=.8) lines(t,tot,col=&#39;black&#39;) legend(8, 1050, legend=c(&quot;juvenile&quot;, &quot;subadults&quot;, &quot;adults&quot;,&quot;total&quot;), col=c(&#39;blue&#39;,&#39;orange&#39;,&#39;red&#39;,&#39;black&#39;), lty=1) They do seem to be dying out. 19.4.2 Eigenanalysis Now we check the eigenvectors and eigenvalues to see if they help us understand what is going on. eigen(A) ## eigen() decomposition ## $values ## [1] 0.9835927+0.0000000i -0.0217964+0.2059185i -0.0217964-0.2059185i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.31754239+0i 0.6820937+0.0000000i 0.6820937+0.0000000i ## [2,] 0.05811107+0i -0.0624124-0.5896338i -0.0624124+0.5896338i ## [3,] 0.94646180+0i -0.0450520+0.4256233i -0.0450520-0.4256233i The first eigenvalue is \\(\\lambda_1 = 0.98\\), and the other two are complex. R always lists the eigenvalues from largest to smallest, so in this case the largets eigenvalue is less than one. That means that in that direction, the population is dying off by 2% each year. If we extract the corresponding eigenvector, and scale it to sum to 1, we get v1 = eigen(A)$vectors[,1] # get the first eigenvector v1 = Re(v1) # drop the imaginary part v1/sum(v1) # scale it to sum to 1 ## [1] 0.24017754 0.04395311 0.71586935 What this is telling us that as the population dies off, it does so in this eigenvector direction with 24.0% of the population being juveniles, 4.4% subadults, and 71.5% adults. The owls were going extinct because of the logging in the Pacific Northwest. Suppose that we make the case that by stopping logging we will increase the survival rate from juvenile to subadult from 0.18 to 0.26 (by improving the habititat the juvinile owls have a better chance of surviving the first year). In this case, the eigenvalues and eigenvectors becomes: A = cbind(c(0,0.26,0),c(0,0,.71),c(0.33,0,0.94)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.0008184+0.0000000i -0.0304092+0.2448335i -0.0304092-0.2448335i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.3121152+0i -0.0754384+0.6073766i -0.0754384-0.6073766i ## [2,] 0.0810836+0i 0.6450012+0.0000000i 0.6450012+0.0000000i ## [3,] 0.9465778+0i -0.4436732-0.1119384i -0.4436732+0.1119384i Notice that the largest eigenvalue now becomes 1. And if we iterate, we see that the population does not die off (it even grows slightly). Finally, we go back to the original system, which is dying out, but start with a totally different age distribution. You can see the non-dominant eigevectors dying out quickly at the beginning and the dominant eigenvector, of eigenvalue 0.98, taking over. A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix x0 = c(70,600,8) # the inital value N = 20 # iterate N=100 times X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in X[,1] = x0 # the first column is the initial population for (i in 2:(N+1)) { # loopn from 2 to N+1 X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i } tot = X[1,] + X[2,] + X[3,] t = seq(1,N+1) plot(t,X[1,],type=&#39;l&#39;,col=&#39;blue&#39;,ylim=c(0,1000),ylab=&quot;population&quot;,xlab=&quot;year&quot;,main=&quot;Spotted Owl Population&quot;) points(t,X[1,],col=&#39;blue&#39;,pch=20,cex=.8) lines(t,X[2,],col=&#39;orange&#39;) points(t,X[2,],col=&#39;orange&#39;,pch=20,cex=.8) lines(t,X[3,],col=&#39;red&#39;) points(t,X[3,],col=&#39;red&#39;,pch=20,cex=.8) points(t,tot,col=&#39;black&#39;,pch=20,cex=.8) lines(t,tot,col=&#39;black&#39;) legend(8, 1050, legend=c(&quot;juvenile&quot;, &quot;subadults&quot;, &quot;adults&quot;,&quot;total&quot;), col=c(&#39;blue&#39;,&#39;orange&#39;,&#39;red&#39;,&#39;black&#39;), lty=1) s "],["dynamical-systems-in-2d.html", "Section 20 Dynamical Systems in 2D 20.1 Helper Function to Plot Dynamical Systems 20.2 Our first example 20.3 CheckPoint Question for today 20.4 Discussion Question 1 20.5 Discussion Question 2 20.6 Discussion Question 3 20.7 Discussion Question 4 20.8 Your Turn", " Section 20 Dynamical Systems in 2D Download this Rmd file Let \\(A\\) be a square \\(n \\times n\\) matrix and let \\(\\mathsf{x}_0 \\in \\mathbb{R}^n\\). A dynamical system is a sequence of vectors \\(\\mathsf{x}_0,\\mathsf{x}_1,\\mathsf{x}_2, \\ldots, \\mathsf{x}_t, \\ldots\\) where \\[ \\mathsf{x}_{t} = A \\mathsf{x}_{t-1} = A^t \\mathsf{x}_0 \\quad \\mbox{for} \\quad t \\geq 1. \\] The sequence \\(\\mathsf{x}_0,\\mathsf{x}_1,\\mathsf{x}_2, \\ldots, \\mathsf{x}_t, \\ldots\\) is called the trajectory for initial vector \\(\\mathsf{x}_0\\). A dynamical system evolves over time. The long-term behavior is governed by the eigenvalues of matrix \\(A\\). We will look at visualizations of some \\(2 \\times 2\\) dynamical systems to develop some intuition about eigensystems. 20.1 Helper Function to Plot Dynamical Systems Here is some special code, written by Professor Beveridge, that makes helpful plots. You need to execute this code chunk before the others. get_traj &lt;- function(mat, x0, num) { traj = cbind(x0) num for (i in 1:num) { traj = cbind(traj, mat %*% traj[,dim(traj)[2]]) traj } return(traj) } plot_traj &lt;- function(mat, x0, num) { traj = get_traj(mat,x0,num) points(traj[1,],traj[2,], pch=20, col=rainbow(length(traj))) } trajectory_plot &lt;- function(mat, t=20, datamax=5, plotmax=10, numpoints=10, showEigenspaces=TRUE) { # initialize plot par(pty = &quot;s&quot;) plot(c(0),c(0),type=&quot;n&quot;, xlim=c(-plotmax,plotmax),ylim=c(-plotmax,plotmax), xlab=&#39;x&#39;, ylab=&#39;y&#39;) abline(h=-plotmax:plotmax, v=-plotmax:plotmax, col=&quot;gray&quot;) mygrid &lt;- expand.grid(x=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1), y=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1)) for (i in 1:dim(mygrid)[1]) { plot_traj(mat,c(mygrid[i,1],mygrid[i,2]),t) } if (showEigenspaces) { eigen = eigen(mat) #mylabel = cat(&#39;lambda=&#39;, eigen$values[1], &#39;and lambda=&#39;, eigen$values[2]) #title(xlab=mylabel) v1 = zapsmall(eigen$vectors[,1]) v2 = zapsmall(eigen$vectors[,2]) if (! class(v1[1]) == &quot;complex&quot;) { if (v1[1] == 0) { abline(v=0) } else { abline(a=0,b=v1[2]/v1[1], col=&quot;blue&quot;) } if (v2[1] == 0) { abline(v=0) } else { abline(a=0,b=v2[2]/v2[1], col=&quot;blue&quot;) } } } } 20.2 Our first example Let’s start by looking at the example from the video \\[ A = \\frac{1}{30} \\begin{bmatrix} 31 &amp; 4 \\\\ 2 &amp; 29 \\end{bmatrix}. \\] We get the most complete picture when we plot multiple trajectories at once. So we use the helper function trajectory_plot to plot the trajectories of a grid of points. It also plots the eigenspaces for the matrix. You can specify the matrix A the number of iterations the size of the square where the initial points lie the size of the plot the number of points along the side of the grid A = 1/30 * cbind(c(31,2),c(4,29)) trajectory_plot(A, t=30, datamax=5, plotmax=15, numpoints=5) This system is best understood by comparing what we see with the eigenvector and eigenvalues. eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] 0.8944272 -0.7071068 ## [2,] 0.4472136 0.7071068 We can see that we have slight expansion along \\([ 2, 1]^{\\top}\\) and slight contraction along \\([-1,1]\\). The long term behavior is an expansion in the direction of \\([2, 1]^{\\top}\\). 20.3 CheckPoint Question for today Here is the checkpoint question for today (for which you found a closed-form solution). A = 1/110 * cbind(c(97,-8),c(6,123)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] -0.2425356 -0.9486833 ## [2,] -0.9701425 -0.3162278 And here is the corresponding plot of the dynamical system: A = 1/110 * cbind(c(97,-8),c(6,123)) trajectory_plot(A, t=10, datamax=15, plotmax=25, numpoints=5) 20.4 Discussion Question 1 In class, we looked at a matrix with eigenvalues 1 and 1/2, and we plotted a trajectory starting at \\((8,7)\\) by hand. Its eigensystem is shown here: A = rbind(c(0.4, 0.4), c(-0.15, 1.1)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.0 0.5 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 And here is a trajectory plot trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5) 20.5 Discussion Question 2 Here the matrix has the same eigenvectors, but now the eigenvalues are 1.0 and 0.9. It’s a little easier to see when the smaller eigenvalue converges more slowly. A = rbind(c(0.88, 0.08), c(-0.03, 1.02)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.0 0.9 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5) 20.6 Discussion Question 3 Here the matrix has the same eigenvectors, but now the eigenvalues are 1.1 and 0.9. A = rbind(c(0.86, 0.16), c(-0.06, 1.14)) eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5) 20.7 Discussion Question 4 Finally, here again the matrix has the same eigenvectors, but now the eigenvalues are 0.99 and 0.9. A = rbind(c(0.882, 0.072), c(-0.027, 1.008)) eigen(A) ## eigen() decomposition ## $values ## [1] 0.99 0.90 ## ## $vectors ## [,1] [,2] ## [1,] -0.5547002 -0.9701425 ## [2,] -0.8320503 -0.2425356 trajectory_plot(A, t=50, datamax=10, plotmax=15, numpoints=5) 20.8 Your Turn Now it’s your turn to explore some dynamical systems. Create trajectory plots for each of these dynamical systems. Characterize the long-term behavior. What direction to vectors converge to? Do magnitudes increase? decrease? stabilize? Calculate the eigenvectors and eigenvalues and compare them to your plot. The eigensystem should tell the same story as your plot. If your original plot is confusing, try changing the parameters (initial square size, plot size, number of grid points). Here is some code for you to adapt for the examples. A = cbind(c(1,0),c(0,1)) trajectory_plot(A, t=30, datamax=5, plotmax=10, numpoints=10) eigen(A) ## eigen() decomposition ## $values ## [1] 1 1 ## ## $vectors ## [,1] [,2] ## [1,] 0 -1 ## [2,] 1 0 20.8.1 Example 1 \\[ A = \\frac{1}{60} \\begin{bmatrix} 55&amp; -8 \\\\ -1 &amp; 53 \\end{bmatrix} \\] 20.8.2 Example 2 \\[ A = \\frac{1}{20} \\begin{bmatrix} 24&amp; -6 \\\\ 1 &amp; 19 \\end{bmatrix} \\] 20.8.3 Example 3 \\[ A = \\frac{1}{110} \\begin{bmatrix} 106&amp; 12 \\\\ 6 &amp; 92 \\end{bmatrix} \\] 20.8.4 Example 4 \\[ A = \\frac{1}{16} \\begin{bmatrix} 17&amp; -15 \\\\ 15 &amp; 17 \\end{bmatrix} \\] "],["complex-eigenvalues.html", "Section 21 Complex Eigenvalues 21.1 Rotation-Dilation 21.2 General 2x2 Matrices with Complex Eigenvalues 21.3 A 3D example", " Section 21 Complex Eigenvalues Download this Rmd file Now we will explore what happens if the matrix has complex eigenvalues. 21.1 Rotation-Dilation First we explore a special case of 2x2 matrices with complex eigenvalues of the following form: \\[ R=\\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\\\ \\end{bmatrix} \\] As we see in the image below, this matrix rotates by angle of \\(\\theta\\) and expands (dilates) or contracts by a factor of \\(r\\) where \\[ \\begin{align} \\theta &amp;= \\arctan(b/a) \\\\ r &amp;= \\sqrt{a^2 + b^2} \\end{align} \\] Furthermore the eigenvalues of this matrix are the complex values \\[ \\lambda_1 = a + b i \\qquad \\lambda_2 = a - b i \\] where \\(i = \\sqrt{-1}\\). These eigenvalues are conjugate pairs and are often written as \\(\\lambda = a \\pm b i\\). They come from applying the quadratic formula to the characteristic polynomial and getting a negative discriminant under the square root. It is important to note that both the angle of rotation \\(\\theta\\) and the dilation factor \\(r\\) are contained in the eigenvalues. The fact that these are the eigenvalues is derived in the video. We will illustrate it here in three examples. 21.1.1 Example 1 Our first example has \\(a = .9\\) and \\(b = .2\\). ## [,1] [,2] ## [1,] 0.9 -0.2 ## [2,] 0.2 0.9 We look at its eigenvectors and eigenvalues and see that \\(\\lambda = .9 \\pm .2 i\\): eigen(A) ## eigen() decomposition ## $values ## [1] 0.9+0.2i 0.9-0.2i ## ## $vectors ## [,1] [,2] ## [1,] 0.7071068+0.0000000i 0.7071068+0.0000000i ## [2,] 0.0000000-0.7071068i 0.0000000+0.7071068i Notics that the eigenvectors also come in conjugate pairs, with a real and a complex part. This always happens. \\[ \\vec{\\mathsf{v}} = \\begin{bmatrix}0.707 \\\\ 0.000 \\end{bmatrix} \\pm \\begin{bmatrix} 0.000 \\\\ .707 \\end{bmatrix} i \\] Now, let’s find the angle of rotation. We will use the Arg command which finds the angle (in radians) of a complex number. We also convert it to degrees here. vals = eigen(A)$values v1 = vals[1] Arg(v1) # gives the argument, or angle, of a complex number (in radians) ## [1] 0.2186689 Arg(v1) / (2*pi) * 360 # convert to degrees ## [1] 12.52881 For good measure, we can compare with using the arctan function. atan(.2/.9) ## [1] 0.2186689 Next we find the dilation/contraction factor. We can do so using the Mod command, which finds the “modulus” or absolute value or length of a complex number. Mod(v1) # gives the length of a complex number ## [1] 0.9219544 And, again for good measure, we compare with using the Pythagorean theorem: sqrt(.9^2 + .2^2) ## [1] 0.9219544 Now, we observe the trajectory of a single point \\((0,1)^T\\) under this matrix. In this picture you can see that it is contracting and rotating by 12.5 degrees. Note that 360/12.5 is about 29, and it takes 29 applications to go once around the circle. You can count them in the plot below. Furthermore, \\((0.9219544)^29 =0.095\\) and after 29 applications the vector is about 1/10 of its original length. We can also view this by looking at a plot of the x and y coordinates over time as the point (x,y) circles around in the xy-plane. Key point: complex eigenvalues lead to oscillating values of the individual coordinates We can also use trajectory_plot from Dynamical Systems in 2D to watch what happens to a whole grid of points under this transformation. It is beautiful! trajectory_plot(A, t=30, datamax=5, plotmax=5, numpoints=10) 21.1.2 Example 2 Here is a second example of a rotation-dilation matrix, this time with \\(a = .96\\) and \\(b = .28\\). (A = cbind(c(.96,.28),c(-.28,.96))) ## [,1] [,2] ## [1,] 0.96 -0.28 ## [2,] 0.28 0.96 eigen(A) ## eigen() decomposition ## $values ## [1] 0.96+0.28i 0.96-0.28i ## ## $vectors ## [,1] [,2] ## [1,] 0.7071068+0.0000000i 0.7071068+0.0000000i ## [2,] 0.0000000-0.7071068i 0.0000000+0.7071068i We check the angle of rotation and the dilation factor vals = eigen(A)$values v1 = vals[1] Arg(v1) # gives the argument, or angle, of a complex number (in radians) ## [1] 0.2837941 Arg(v1) / (2*pi) * 360 # convert to degrees ## [1] 16.2602 Mod(v1) # gives the length of a complex number ## [1] 1 Notice that the dilation factor is 1, which is seen in the following plots. Here are 21 iterations: And here are 200 iterations And a trajectory plot: 21.1.3 Example 3 A third and final example. (A = cbind(c(.99,.16),c(-.16,.99))) ## [,1] [,2] ## [1,] 0.99 -0.16 ## [2,] 0.16 0.99 eigen(A) ## eigen() decomposition ## $values ## [1] 0.99+0.16i 0.99-0.16i ## ## $vectors ## [,1] [,2] ## [1,] 0.0000000-0.7071068i 0.0000000+0.7071068i ## [2,] -0.7071068+0.0000000i -0.7071068+0.0000000i vals = eigen(A)$values v1 = vals[1] Arg(v1) / (2*pi) * 360 # convert to degrees ## [1] 9.180542 Mod(v1) # gives the length of a complex number ## [1] 1.002846 We see that the dilation factor is \\(r = 1.0028\\) and the angle of rotation is \\(9.18\\) degrees. Here are 100 iterations. And a trajectory plot: 21.2 General 2x2 Matrices with Complex Eigenvalues Now suppose we have a 2x2 matrix with complex eigenvalues \\(\\lambda = a \\pm b i\\) and complex eigenvectors \\(\\mathsf{v} = \\mathsf{u } \\pm \\mathsf{w} i\\) that is not in rotation-dilation form. Here is an example: \\[ A = \\begin{bmatrix} 1.19 &amp; -0.38 \\\\ 0.29 &amp; 0.78 \\end{bmatrix} \\] It has eigenvalues and eigenvectors \\[ \\lambda = 0.985 \\pm 0.261 i \\qquad \\mathsf{v} = \\begin{bmatrix} 0.753 \\\\ 0.406 \\end{bmatrix} \\pm \\begin{bmatrix} 0.000 \\\\ -0.517 \\end{bmatrix} i \\] as seen here: ## [,1] [,2] ## [1,] 1.19 -0.38 ## [2,] 0.29 0.78 ## eigen() decomposition ## $values ## [1] 0.985+0.2611034i 0.985-0.2611034i ## ## $vectors ## [,1] [,2] ## [1,] 0.7531030+0.0000000i 0.7531030+0.0000000i ## [2,] 0.4062793-0.5174679i 0.4062793+0.5174679i The angle of rotation and factor of dilation are \\(\\theta = 14.8\\) degrees and \\(r = 1.019\\) as we see from these computations: ## [1] 14.84649 ## [1] 1.019019 A trajectory plot shows us that it is still rotating by 14.8 degrees and dilating by 1.019, but it is taking more of an elliptical pattern. To see precisely what happens, we change to basis \\(\\{\\mathsf{w}, \\mathsf{u}\\}\\) where \\(\\mathsf{w}\\) and \\(\\mathsf{u}\\) are the imaginary and real parts of the eigenvector \\(\\mathsf{v} =\\mathsf{u} + \\mathsf{w} i\\). In this case the eigenvalues and eigenvectors are \\[ \\lambda = 0.985 \\pm 0.261 i \\qquad \\mathsf{v} = \\begin{bmatrix} 0.753 \\\\ 0.406 \\end{bmatrix} \\pm \\begin{bmatrix} 0.000 \\\\ -0.517 \\end{bmatrix} i \\] So if we make the change of basis matrix \\(P = [\\mathsf{u},\\mathsf{w}]\\) \\[ P = \\begin{bmatrix} 0.000 &amp; 0.753 \\\\ -0.517 &amp; 0.406 \\end{bmatrix} \\] then we can factor \\(A\\) as \\[ A = \\begin{bmatrix} 1.19 &amp; -0.38 \\\\ 0.29 &amp; 0.78 \\end{bmatrix} = \\underbrace{\\begin{bmatrix}0.000 &amp; 0.753 \\\\-0.517 &amp; 0.406 \\end{bmatrix}}_P \\underbrace{\\begin{bmatrix} 0.985 &amp; - 0.261 \\\\ 0.261 &amp; 0.985 \\end{bmatrix}}_R \\underbrace{\\begin{bmatrix}0.000 &amp; 0.753 \\\\-0.517 &amp; 0.406 \\end{bmatrix}^{-1}}_{P^{-1}} \\] We have not diagonalized \\(A\\). Rather we have rotation-dilationalized (made up term) the matrix \\(A\\). At is core \\(A\\) is a rotation-dilation matrix whose angle and dilation factor come from the eigenvalue. The matrix \\(P\\) is a change of basis matrix. It is rotating and dilating in this new coordinate system, which are the vectors in the plot above. If we multiply the other way, we get \\[ P^{-1} A P = \\begin{bmatrix} 0.985 &amp; - 0.261 \\\\ 0.261 &amp; 0.985 \\end{bmatrix} = R \\] Which we can see using R ## [,1] [,2] ## [1,] 0.0000000 0.7531030 ## [2,] -0.5174679 0.4062793 ## [,1] [,2] ## [1,] 0.9850000 -0.2611034 ## [2,] 0.2611034 0.9850000 21.3 A 3D example Here is the Northern Spotted Owl matrices from the [Eigenvalues] examples. We saw that it has one real and two complex eigenvalues: ## eigen() decomposition ## $values ## [1] 0.9835927+0.0000000i -0.0217964+0.2059185i -0.0217964-0.2059185i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.31754239+0i 0.6820937+0.0000000i 0.6820937+0.0000000i ## [2,] 0.05811107+0i -0.0624124-0.5896338i -0.0624124+0.5896338i ## [3,] 0.94646180+0i -0.0450520+0.4256233i -0.0450520-0.4256233i The eigenvalues are always listed in descending order by magnitude as we see here when we compute their modulus and arguments ## [1] 0.9835927 0.2070688 0.2070688 ## [1] 0.00000 96.04223 -96.04223 We can diagonalize this over the complex numbers. ## [,1] [,2] [,3] ## [1,] 0.9835927+0i 0.0000000+0.0000000i 0.0000000+0.0000000i ## [2,] 0.0000000+0i -0.0217964+0.2059185i 0.0000000+0.0000000i ## [3,] 0.0000000+0i 0.0000000+0.0000000i -0.0217964-0.2059185i It has complex eigenvalues on the diagonal and requires using the complex vectors in our basis. Often, this is not what we want to do if our matrix has real entries and comes from a real-valued problem. Instead, let’s rotation-dilationalize this matrix. The block diagonalization consists of a 1x1 block of the real eigenvalue and a 2x2 block of the rotation-dilation part of the complex eigenvalues. ## [,1] [,2] [,3] ## [1,] 0.31754239 0.0000000 0.68209367 ## [2,] 0.05811107 -0.5896338 -0.06241245 ## [3,] 0.94646180 0.4256233 -0.04505202 ## [,1] [,2] [,3] ## [1,] 0.9835927 0.0000000 0.0000000 ## [2,] 0.0000000 -0.0217964 -0.2059185 ## [3,] 0.0000000 0.2059185 -0.0217964 In 3 dimensions, there the geometry is as follows: * there is one basis vector, corresponding to the real eigenvalue \\(\\lambda = 0.985\\) * there is a plane in which the system rotates by \\(\\theta = 96\\) degrees and contracts by \\(r = 0.207\\). * thus, the complex part dies off pretty quickly (\\(r = 0.207\\)), and it converges to the dominant real eigenvector, which is also dying off but more slowly (\\(r = 0.983\\)) We can see this in the traectory plot below. The oscillating part at the beginning comes from the complex eigenvalues as they quickly die off and the system converges to the dominant real eigenvector. "],["quiz-1-review.html", "Section 22 Quiz 1 Review 22.1 Overview 22.2 Practice Problems 22.3 Solutions to Practice Problems", " Section 22 Quiz 1 Review 22.1 Overview Our first quiz covers sections 1.1 - 1.8 and part of 1.9 in Lay’s book. This corresponds to Problem Sets 1 and 2. In 1.8 and 1.9, it will cover the definition of a linear transformation as well as one-to-one and onto, but it will not cover the matrix of a linear transformation. 22.1.1 Vocabulary and Concepts You should understand these concepts and be able to read and use these terms correctly: elementary row operations REF and RREF pivot position linear combination span linear independence homogeneous and nonhomogeneous equations Understand the geometric relationship between the solutions to \\(Ax = 0\\) and \\(Ax=b\\) Understand Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b\\) in \\(\\mathbb{R}^m\\), \\(A x = b\\) has a solution Each \\(b\\) in \\(\\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. The columns of \\(A\\) are linearly independent if and only if \\(Ax=0\\) only has the trivial solution Understand Theorem 8 in Section 1.7: if you have more than \\(n\\) vectors in \\(\\mathbb{R}^n\\) they must be linearly dependent. linear transformation one-to-one and onto Understand Theorem 12 in Section 1.9 which states that \\(T(x)=Ax\\) is onto if and only if the column of \\(A\\) span \\(\\mathbb{R}^m\\) \\(T(x)=Ax\\) is one-to-one if and only if the columns of \\(A\\) are linearly independent. 22.1.2 Skills You should be able to perform these linear algebra tasks. Identify linear systems from nonlinear systems Make the augmented matrix from a set of equations Row reduce a system of equations into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Write the solution set to \\(Ax=b\\) as a parametric vector equation. Convert back and forth between systems of equations, vector equations, and matrix equations. Compute the matrix-vector product \\(Ax\\) Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) Manipulate matrix vector products using: \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) Determine whether a linear transformation is one-to-one and/or onto 22.2 Practice Problems 22.2.1 I have performed some row operations below for you on a matrix \\(A\\). Write out the complete set of solutions to \\(A \\mathsf{x} = {\\bf 0}\\). \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1 \\\\ 1&amp; 2&amp; 1&amp; 1&amp; 0&amp; -2 \\\\ 2&amp; 4&amp; -2&amp; 6&amp; 1&amp; 2 \\\\ 1&amp; 2&amp; 0&amp; 2&amp; -1&amp; -3 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 1&amp; -1&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 1&amp; 2\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ \\end{bmatrix} \\] 22.2.2 I have performed some row operations below for you on a matrix \\(B\\). \\[ B= \\begin{bmatrix} 1&amp; 1&amp; 0 \\\\ 0&amp; 1&amp; 1 \\\\ 2&amp; 1&amp; 2 \\\\ 1&amp; -1&amp; 1 \\\\ 2&amp; 3&amp; 1 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 0&amp; 0 \\\\ 0&amp; 1&amp; 0 \\\\ 0&amp; 0&amp; 1 \\\\ 0&amp;0&amp;0 \\\\ 0&amp;0&amp;0 \\\\ \\end{bmatrix} \\] a. Describe the solutions to the equation \\(B \\mathsf{x} = {\\bf 0}\\). Fill in the boxes: the transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{\\square}\\) to \\(\\mathbb{R}^{\\square}\\). 22.2.3 I want to know if it is possible to write \\(\\mathsf{w}\\) as a linear combination of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) below. Write down, but do not solve, a matrix equation that would solve this problem. Your answer should be of the form \\(A \\mathsf{x} = \\mathsf{b}\\), where I can clearly see what \\(A, \\mathsf{x}\\), and \\(\\mathsf{b}\\) are. I should also be able to tell how many unknowns there are. \\[ \\mathsf{v}_1 = \\left[ \\begin{matrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_2 = \\left[ \\begin{matrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_3 = \\left[ \\begin{matrix} 1 \\\\ 1 \\\\ 0 \\\\ -2 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{w} = \\left[ \\begin{matrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\\\ \\end{matrix}\\right] . \\] 22.2.4 Describe all vectors that are not in the span of the columns of the matrix \\(A\\) below: \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 4 \\\\ -3&amp; -5&amp; -11\\\\ 1&amp; 1&amp; 3 \\\\ \\end{bmatrix} \\] 22.2.5 The matrix below is \\(3 \\times 3\\) but the third column is missing. Add a nonzero third column so that the columns of \\(A\\) are linearly dependent and add a 3rd column so that the columns of \\(A\\) are linearly independent. Briefly describe your strategy. \\[ \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\qquad\\qquad \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\] 22.2.6 In each case below, find the matrix of the linear transformation that is described, if you believe that the matrix exists. Otherwise, demonstrate that the transformation is not linear. The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix}\\right) = \\begin{bmatrix} x_1 + x_2 \\\\ 2 x_1 \\\\ -x_2 \\\\\\end{bmatrix}. \\] The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)= \\begin{bmatrix} x_1 + x_2 + x_3 \\\\ x_1 x_2 \\\\ -x_2 + 2 x_3 \\end{bmatrix}. \\] The transformation \\(L: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) The transformation \\(R: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) 22.2.7 Write the following systems of equations in vector and matrix form. \\[ \\begin{array} {ccccccccccc} 5 x_1 &amp;+&amp; 3 x_2 &amp;+&amp; x_3 &amp;+&amp; 11 x_4 &amp;-&amp; x_5 &amp;=&amp; 10 \\\\ 4 x_1 &amp;+&amp; x_2 &amp;+&amp; 3 x_3 &amp;+&amp; 2 x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 11 \\\\ - x_1 &amp;+&amp; 3 x_2 &amp;-&amp; 2 x_3 &amp;+&amp; x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 12 \\\\ \\end{array} \\] 22.2.8 Let \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) be the vectors in the columns of the matrix \\(A\\) below. \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 3 &amp; 1 \\\\ 2 &amp; 0 &amp; 2 &amp; 3 \\\\ 1 &amp; 1 &amp; 3 &amp; 1 \\\\ -1 &amp; 0 &amp; -1 &amp; 0 \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right] \\] a. Are the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) linear independent or dependent? If they are linearly dependent, please give a dependence relation among them. b. Describe the span of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) inside of \\(\\mathbb{R}^4\\)? 22.2.9 Find a solution to \\(A \\mathsf{x}=0\\) that no one else in the class has. \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 2 &amp; 0 &amp; 4 &amp; 1 &amp; 4 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 1 &amp; 0 &amp; 2 &amp; 1 &amp; 3 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\] 22.3 Solutions to Practice Problems 22.3.1 The parametric vector form of the solution is \\[\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\\\ 1 \\\\0 \\\\ 0 \\end{bmatrix} u \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\-2 \\\\ 1 \\end{bmatrix}\\] 22.3.2 There is one solution: \\(\\mathsf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\). The transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{3}\\) to \\(\\mathbb{R}^{5}\\). 22.3.3 \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 0 &amp; 1 \\\\ 3 &amp; 1 &amp; 0 \\\\ 4 &amp; 0 &amp; -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\end{bmatrix} \\] 22.3.4 We want to find all target vectors \\(\\mathsf{b}\\) such that \\(A \\mathsf{x} = \\mathsf{b}\\) is inconsistent. So we want the augmented matrix \\(\\begin{bmatrix} A \\,| \\, b \\end{bmatrix}\\) to have a pivot in the last column. \\[ \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ -3&amp; -5&amp; -11 &amp; b_2\\\\ 1&amp; 1&amp; 3 &amp; b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; -1&amp; -1 &amp; -b_1+b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; 0&amp; 0 &amp; 2b_1+b_2+b_3 \\\\ \\end{array} \\right] \\] So the set of target vectors that are not in the span of the columns of \\(A\\) are the vectors \\[ \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\qquad \\mbox{where} \\qquad 2b_1 + b_2 + b_3 \\neq 0. \\] 22.3.5 This is on PS2. 22.3.6 This is a linear transformation with \\[A = \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}.\\] This is not a linear transformation because \\[ 2 \\, T \\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 2 \\\\ 2 \\end{bmatrix} \\quad \\mbox{while} \\quad T \\left( \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 6 \\\\ 4 \\\\ 2 \\end{bmatrix}. \\] \\(A= \\begin{bmatrix} 1/2 &amp; 1/2 \\\\ 1/2 &amp; 1/2 \\end{bmatrix}\\) \\(A= \\begin{bmatrix} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{bmatrix}\\) 22.3.7 Vector Form: \\[ x_1 \\begin{bmatrix} 5 \\\\ 4 \\\\ -1 \\end{bmatrix} + x_2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 3 \\end{bmatrix} + x_3 \\begin{bmatrix} 1 \\\\ 3 \\\\ -2 \\end{bmatrix} + x_4 \\begin{bmatrix} 11 \\\\ 2 \\\\ 1 \\end{bmatrix} + x_5 \\begin{bmatrix} -1 \\\\ 6 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] Matrix Form: \\[ \\begin{bmatrix} 5 &amp; 3 &amp; 1 &amp; 11 &amp; -1 \\\\ 4 &amp; 1 &amp; 3 &amp; 2 &amp; 6 \\\\ -1 &amp; 3 &amp; -2&amp; 1 &amp; 6 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] 22.3.8 \\(-\\mathsf{v}_1 - 2\\mathsf{v}_2 + \\mathsf{v}_3 + 0 \\mathsf{v}_4 = 0\\). \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4)\\) looks like a copy of \\(\\mathbb{R}^3\\) sitting inside \\(\\mathbb{R}^4\\). In other words, is 3-dimensional subset of \\(\\mathbb{R}^4\\). 22.3.9 The general solution is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] My solution is \\[77,083,679 \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - 72,159,215 \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] "],["quiz-2-review.html", "Section 23 Quiz 2 Review 23.1 Overview 23.2 Practice Problems 23.3 Solutions to Practice Problems", " Section 23 Quiz 2 Review 23.1 Overview Our second quiz covers the following sections: Linear Transformations 2.1: Matrix Multiplication and the Matrix of a Linear Transformation 2.2: Matrix Inverses 2.3: The Invertible Matrix Theorem Subspaces 4.1: Subspaces of \\(\\mathbb{R}^n\\) 4.2: Null Space and Column Space 4.3: Bases 4.4: Coordinates This corresponds to Problem Sets 3 and 4. The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Look over the Edfinity homework assingments Do practice problems from the Edfinity Practice assignments. These allow you to “Practice Similar” by generating new variations of the same problem. Redo the Jamboard problems Try to resolve the Problem Sets and compare your answers to the solutions. Do the practice problems below. Compare your answers to the solutions. 23.1.1 Vocabulary and Concepts You should understand these concepts and be able to read and use these terms correctly: all of the Important Definitions found here. matrix multiplication matrix inverses the Invertible Matrix Theorem subspaces null space and column space of a matrix kernel and image of a linear transformation basis (span and linearly independent) coordinate vector with respect to a basis \\(\\mathcal{B}\\) change-of-coordinates matrix dimension 23.1.2 Skills You should be able to perform these linear algebra tasks. solve matrix algebra equations find a matrix inverse show that a subset is a subspace or demonstrate that it is not a subspace describe the null space and the column space determine if a vector is in a null space or column space find a basis of a subspace answer questions about the connections between all these ideas write short proofs of basic statements using the Important Definitions 23.2 Practice Problems 23.2.1 In each case below, find the matrix of the linear transformation that is described, if you believe that the matrix exists. Otherwise, demonstrate that the transformation is not linear. The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix}\\right) = \\begin{bmatrix} x_1 + x_2 \\\\ 2 x_1 \\\\ -x_2 \\\\\\end{bmatrix}. \\] The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)= \\begin{bmatrix} x_1 + x_2 + x_3 \\\\ x_1 x_2 \\\\ -x_2 + 2 x_3 \\end{bmatrix}. \\] The transformation \\(L: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) The transformation \\(R: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) 23.2.2 Find the inverse of the matrix \\[ \\left[ \\begin{array}{rrr} 1 &amp; -2 &amp; 2 \\\\ 1 &amp; 0 &amp; 0 \\\\ 2 &amp;-4 &amp; 5 \\end{array} \\right] \\] 23.2.3 Suppose that a linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) has the property that \\(T(\\mathsf{u}) = T(\\mathsf{v})\\) for some pair of distinct vectors \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\). Can \\(T\\) map \\(\\mathbb{R}^n\\) onto \\(\\mathbb{R}^n\\)? Why or why not? 23.2.4 Let \\(U\\) and \\(W\\) be subspaces of a vector space \\(\\mathbb{R}^n\\). Prove or disprove the following statements. Prove them by showing that the conditions are being a subspace are satisfied. Disprove them with a specific counter example. \\(U \\cap W = \\{ \\mathsf{v} \\in \\mathbb{R}^n \\mid \\mathsf{v} \\in U \\mbox{ and } \\mathsf{v} \\in W \\}\\) is a subspace \\(U \\cup W = \\{ \\mathsf{v} \\in \\mathbb{R}^n \\mid \\mathsf{v} \\in U \\mbox{ or } \\mathsf{v} \\in W \\}\\) is a subspace \\(U+W = \\{\\mathsf{u} + \\mathsf{w} \\mid \\mathsf{u} \\in U \\mbox{ and } \\mathsf{w} \\in W \\}\\) is a subspace 23.2.5 Let \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) be a linear transformation. Suppose that \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is one-to-one. Prove that if \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) are linearly independent, then \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) are linearly independent. Suppose that \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is onto. Prove that if \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) span \\(\\mathbb{R}^n\\) then \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) span \\(\\mathbb{R}^m\\). 23.2.6 I have performed some row operations below for you on a matrix \\(A\\). Find a basis for the column space and the null space of \\(A\\). \\[ A= \\left[ \\begin{matrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1 \\\\ 1&amp; 2&amp; 1&amp; 1&amp; 0&amp; -2 \\\\ 2&amp; 4&amp; -2&amp; 6&amp; 1&amp; 2 \\\\ 1&amp; 2&amp; 0&amp; 2&amp; -1&amp; -3 \\\\ \\end{matrix}\\right] \\longrightarrow \\left[ \\begin{matrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 1&amp; -1&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 1&amp; 2\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ \\end{matrix}\\right] \\] 23.2.7 Consider the matrix \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 5 &amp; 2 &amp; -4 \\\\ 3 &amp; 10 &amp; 2 &amp; 8 \\\\ 4 &amp; 15 &amp; 4 &amp; 4 \\end{array} \\right] \\] Find a basis for \\(\\mathrm{Col}(A)\\). Find a basis for \\(\\mathrm{Nul}(A)\\). 23.2.8 Are the vectors in \\({\\mathcal B}\\) a basis of \\(\\mathbb{R}^3\\)? If not, find a basis of \\(\\mathbb{R}^3\\) that consists of as many of the vectors from \\({\\mathcal B}\\) as is possible. Explain your reasoning. \\[ \\mathcal{B}=\\left\\{ \\begin{bmatrix} 1 \\\\ -1 \\\\ -2 \\end{bmatrix},\\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix},\\begin{bmatrix} -1 \\\\ -1 \\\\ -8 \\end{bmatrix} \\right\\} \\] 23.2.9 Find the coordinates of \\(\\mathsf{w}\\) in the standard basis and of \\(\\mathsf{v}\\) in the \\(\\mathcal{B}\\)-basis. \\[ \\mathcal{B} = \\left\\{ \\mathsf{v}_1=\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_2=\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_3=\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_4=\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right\\}, \\] \\[ \\mathsf{w} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\\\ -1 \\end{bmatrix}_{\\mathcal{B}}, \\qquad \\mathsf{v} = \\begin{bmatrix} 10 \\\\ 9 \\\\ 7 \\\\ 4 \\end{bmatrix}_{\\mathcal{S}} \\] 23.2.10 The subspace \\(S \\subset \\mathbb{R}^5\\) is given by \\[ \\mathsf{S} = \\mathsf{span} \\left( \\begin{bmatrix}1\\\\ 1\\\\ 0\\\\ -1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\ 1\\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 3\\\\ 1\\\\ -2\\\\ -5\\\\ 4 \\end{bmatrix}, \\begin{bmatrix} 1\\\\ 0\\\\ 1\\\\ 0\\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 2\\\\ -1\\\\ -1\\\\ -3\\\\ 1 \\end{bmatrix}, \\right)\\] Use the following matrix to find a basis for \\(S\\). What is the dimension of \\(S\\)? \\[ A=\\left[ \\begin{array}{ccccc} 1 &amp; 0 &amp; 3 &amp; 1 &amp; 2 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 &amp; -1 \\\\ -1 &amp; 1 &amp; -5 &amp; 0 &amp; -3 \\\\ 2 &amp; 1 &amp; 4 &amp; 1 &amp; 1 \\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{ccccc} 1 &amp; 0 &amp; 3 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -2 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] Find a basis for \\(\\mathrm{Nul}(A)\\). What is the dimension of this nullspace? 23.2.11 A \\(6 \\times 8\\) matrix \\(A\\) contains 5 pivots. For each of \\(\\mathrm{Col}(A)\\) and \\(\\mathrm{Nul}(A)\\), Determine the dimension of the subspace, Indicate whether it is subspace of \\(\\mathbb{R}^6\\) or \\(\\mathbb{R}^8\\), and Decide how you would find a basis of the subspace. 23.3 Solutions to Practice Problems 23.3.1 This is a linear transformation with \\[A = \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}.\\] This is not a linear transformation because, for example, \\[ 2 \\, T \\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 2 \\\\ 2 \\end{bmatrix} \\quad \\mbox{while} \\quad T \\left( \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 6 \\\\ 4 \\\\ 2 \\end{bmatrix}. \\] \\(A= \\begin{bmatrix} 1/2 &amp; 1/2 \\\\ 1/2 &amp; 1/2 \\end{bmatrix}\\) \\(A= \\begin{bmatrix} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{bmatrix}\\) 23.3.2 The inverse is \\[ \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ -5/2 &amp; 1/2 &amp;1 \\\\ -2 &amp; 0 &amp; 1 \\end{bmatrix} \\] 23.3.3 No \\(T\\) cannot be an onto mapping by the Invertible Matrix Theorem. Since \\(T\\) is not one-to-one, the mapping cannot be onto. 23.3.4 True Since \\(U\\) and \\(W\\) are subspaces, we know that \\(\\mathbb{0} \\in U\\) and \\(\\mathbb{0} \\in W\\). Therefore \\(\\mathbb{0} \\in U \\cap W\\). Let \\(\\mathsf{v}_1 \\in U \\cap W\\) and \\(\\mathsf{v}_2 \\in U \\cap W\\). We know that \\(\\mathsf{v}_1 \\in U\\) and \\(\\mathsf{v}_2 \\in U\\). Since \\(U\\) is a subspace, we have \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in U\\). We know that \\(\\mathsf{v}_1 \\in W\\) and \\(\\mathsf{v}_2 \\in W\\). Since \\(W\\) is a subspace, we have \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in W\\). Therefore \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in U \\cap W\\). Let \\(\\mathsf{v} \\in U \\cap W\\) and \\(c \\in \\mathbb{R}\\). We know that \\(\\mathsf{v} \\in U\\) and \\(c \\in R\\). Since \\(U\\) is a subspace, we have \\(c \\mathsf{v} \\in U\\). We know that \\(\\mathsf{v} \\in W\\) and \\(c \\in R\\). Since \\(W\\) is a subspace, we have \\(c \\mathsf{v} \\in W\\). Therefore \\(c \\mathsf{v} \\in U \\cap W\\). False. Here is an example that shows this is not always true. Let \\(V= \\mathbb{R}^2\\), \\(U = \\{ { x \\choose 0} \\mid x \\in \\mathbb{R} \\}\\) and \\(W= \\{ { 0 \\choose y} \\mid y \\in \\mathbb{R} \\}\\). The set \\(U \\cup W\\) is not closed under addition. For example, \\({1 \\choose 0} + {0 \\choose 1} = { 1 \\choose 1} \\notin U \\cup W\\). True. Since \\(U\\) and \\(W\\) are subspaces, we know that \\(\\mathbb{0} \\in U\\) and \\(\\mathbb{0} \\in W\\). Therefore \\(\\mathbb{0} = \\mathbb{0} + \\mathbb{0} \\in U + W\\). Let \\(\\mathsf{u}_1 + \\mathsf{w}_1 \\in U + W\\) and \\(\\mathsf{u}_1 + \\mathsf{w}_2 \\in U + W\\), where \\(\\mathsf{u}_1, \\mathsf{u}_2 \\in U\\) and \\(\\mathsf{w}_1, \\mathsf{w}_2 \\in W\\). Then \\[ (\\mathsf{u}_1 + \\mathsf{w}_1) + (\\mathsf{u}_2 + \\mathsf{w}_2) = (\\mathsf{u}_1 + \\mathsf{u}_2) + (\\mathsf{w}_1 + \\mathsf{w}_2) \\] and \\(\\mathsf{u}_3 = (\\mathsf{u}_1 + \\mathsf{u}_2) \\in U\\) (because \\(U\\) is a subspace) and \\(\\mathsf{w}_3 = (\\mathsf{w}_1 + \\mathsf{w}_2) \\in W\\) (because \\(W\\) is a subspace). Therefore \\((\\mathsf{u}_1 + \\mathsf{v}_1) + (\\mathsf{u}_2 + \\mathsf{w}_2) = \\mathsf{u}_3 + \\mathsf{w}_3 \\in U + W\\). Let \\(\\mathsf{u} + \\mathsf{w} \\in U + W\\) and \\(c \\in \\mathbb{R}\\). Then \\(c(\\mathsf{u} + \\mathsf{w}) = c \\mathsf{u} + c \\mathsf{w}\\). We know that \\(c \\mathsf{u} \\in U\\) (since \\(U\\) is a subspace) and \\(c \\mathsf{w} \\in W\\) (since \\(W\\) is a subspace). Therefore \\(c(\\mathsf{u} + \\mathsf{w}) = c \\mathsf{u} + c \\mathsf{w} \\in U+W\\). 23.3.5 Suppose that \\(c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_3) = 0\\). We must show that \\(c_1 = c_2 = c_3 = 0\\). Since \\(T\\) is a linear transformation, this means that \\(T(c_1 \\mathsf{v}_1+ c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_3 )= 0\\). Since \\(T\\) is one-to-one and \\(T(\\mathbf{0}) = \\mathbf{0}\\), we must have \\(c_1 \\mathsf{v}_1+ c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_3 = \\mathbf{0}\\). Because \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) are linearly independent, this means that \\(c_1 = c_2 = c_3 = 0\\). This proves that \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) are linearly independent. Given \\(\\mathsf{w} \\in W\\). We must show that there exist constants \\(c_1, c_2, c_3\\) such that \\(\\mathsf{w} = c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_3)\\). Here we go! Since \\(T\\) is onto, we know that there exists \\(\\mathsf{v} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{v}) = \\mathsf{w}\\). Since \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) span \\(\\mathbb{R}^n\\), we know that there exist constants \\(c_1, c_2, c_3\\) such that \\(\\mathsf{v}= c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_k\\) Therefore \\[ \\mathsf{w} = T(\\mathsf{v})= T(c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_k) = c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_k) \\] because \\(T\\) is a linear transformation. This proves that \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) span \\(W\\). 23.3.6 A basis for \\(\\mathrm{Col}(A)\\) is \\[ \\begin{bmatrix} 1 \\\\1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\1 \\\\ -2 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\0 \\\\ 1 \\\\ -1 \\end{bmatrix} \\] and a basis for \\(\\mathrm{Nul}(A)\\) is \\[ \\begin{bmatrix} -2 \\\\1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} -2 \\\\0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\0 \\\\ 1\\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] 23.3.7 Using RStudio we find: ## [,1] [,2] [,3] [,4] ## [1,] 1 5 2 -4 ## [2,] 3 10 2 8 ## [3,] 4 15 4 4 ## [,1] [,2] [,3] [,4] ## [1,] 1 0 -2.0 16 ## [2,] 0 1 0.8 -4 ## [3,] 0 0 0.0 0 A basis for \\(\\mathrm{Col}(A)\\) is \\[ \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\quad \\begin{bmatrix} 5 \\\\ 10 \\\\ 15 \\end{bmatrix}. \\] A basis for \\(\\mathrm{Nul}(A)\\) is \\[ \\begin{bmatrix} 2 \\\\ -0.8 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} -16 \\\\ 4 \\\\ 0 \\\\ 1 \\end{bmatrix}. \\] 23.3.8 A = cbind(c(1,-1,-2),c(2,-1,1),c(-1,-1,-8)) A ## [,1] [,2] [,3] ## [1,] 1 2 -1 ## [2,] -1 -1 -1 ## [3,] -2 1 -8 rref(A) ## [,1] [,2] [,3] ## [1,] 1 0 3 ## [2,] 0 1 -2 ## [3,] 0 0 0 No they are not a basis. The corresponding matrix only has two pivots. Let’s add the three elementary vectors to create matrix \\(B\\) and then row reduce. B = cbind(A, c(1,0,0),c(0,1,0),c(0,0,1)) B ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 -1 1 0 0 ## [2,] -1 -1 -1 0 1 0 ## [3,] -2 1 -8 0 0 1 rref(B) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 3 0 -0.3333333 -0.3333333 ## [2,] 0 1 -2 0 -0.6666667 0.3333333 ## [3,] 0 0 0 1 1.6666667 -0.3333333 From this matrix, we can see that the vectors \\[ \\begin{bmatrix} 1 \\\\ -1 \\\\ -2 \\end{bmatrix}, \\quad \\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}. \\] are linearly independent because they correspond to the basis columns of \\(B\\). 23.3.9 We use the change of basis matrix. \\[ P_{\\cal B} = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Then, the desired coordinate vectors are \\[ \\mathsf{w} = \\begin{bmatrix} 0 \\\\ -3 \\\\ -1 \\\\ -1 \\end{bmatrix}_{\\mathcal{S}}, \\qquad \\mathsf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}_{\\mathcal{B}} \\] You can find these vectors by multiplying by \\(P_\\mathcal{B}\\) and by augmenting and row reducing as seen here. A = cbind(c(1,0,0,0),c(1,1,0,0),c(1,1,1,0),c(1,1,1,1)) w = c(3,-2,0,-1) v = c(10,9,7,4) A %*% w ## [,1] ## [1,] 0 ## [2,] -3 ## [3,] -1 ## [4,] -1 Av = cbind(A,v) rref(Av) ## v ## [1,] 1 0 0 0 1 ## [2,] 0 1 0 0 2 ## [3,] 0 0 1 0 3 ## [4,] 0 0 0 1 4 Or we can use the inverse of \\(P_\\mathcal{B}\\). \\[ P_{\\cal B}^{-1} = \\begin{bmatrix} 1 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp;-1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Ainv = solve(A) Ainv %*% v ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 23.3.10 \\(\\dim(S) = 3\\) and a basis for \\(S\\) is \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ -1 \\\\2 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\1 \\end{bmatrix}. \\] \\(\\dim(\\mathrm{Nul}(A))=2\\) and a basis is \\[ \\begin{bmatrix} -3 \\\\ 2 \\\\ 1 \\\\ 0 \\\\0\\end{bmatrix}, \\quad \\begin{bmatrix} -1 \\\\ 2 \\\\ 0 \\\\ -1 \\\\1 \\end{bmatrix}. \\] 23.3.11 \\(\\mathrm{Col}(A)\\) has dimension 5, and it is a subspace of \\(\\mathbb{R}^6\\). You would find a basis by taking the pivot columns of \\(A\\). \\(\\mathrm{Nul}(A)\\) has dimension 3, and it is a subspace of \\(\\mathbb{R}^8\\). You would find a basis by finding the parametric solution to \\(A \\mathsf{x}= \\mathbb{0}\\). "]]
