[["index.html", "MATH 236: Linear Algebra Preface", " MATH 236: Linear Algebra Preface This is the class handbook for Math 236 Linear Algebra at Macalester College. The content here was made by Andrew Beveridge and Tom Halverson and other faculty in the Department of Mathematics, Statistics and Computer Science at Macalester College. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["problem-set-1-a.html", "Section 1 Problem Set 1-A 1.1 Characterize the Solution Set 1.2 Find the General Solution 1.3 Elementary row operations are reversible 1.4 Designer Parabolas 1.5 Traffic Flow", " Section 1 Problem Set 1-A Due: Tuesday January 26 by noon CST. Week 1 and Week 8 are half weeks, so those two assignments will be split in two and called Problem Sets 1A and 1B. Upload your solutions to problems 1–4 by writing them out by hand, scanning them to pdf using a scanning software such as AdobeScan, assembling them into a single PDF, and uploading it to Moodle. Problem 1.5 is to be done using RStudio. To solve it, create an Rmarkdown file, knit it to .html, and upload the .html on Moodle along with the PDF for questions 1-4. 1.1 Characterize the Solution Set The following augmented matrices are in row echelon form. Decide whether the set of solutions is a point, line, plane, or the empty set in 3-space. Briefly justify your answer. \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 4 \\\\ 0 &amp; 1 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; -1 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 0 &amp; 1 &amp; 0 &amp; 6 \\\\ 0 &amp; 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.2 Find the General Solution Each of the following matrices is the reduced row echelon form of the augmented matrix of a system of linear equations. Give the general solution to each system. \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 3 &amp; 0 &amp; -2 &amp; 5\\\\ 0 &amp; 0 &amp; 1 &amp; 4 &amp; -2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; 4 &amp; 0 &amp; 3 &amp; 6\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -2&amp; -8 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 3 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 4 &amp; 0 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7 &amp; 6\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.3 Elementary row operations are reversible In each case below, an elementary row operation turns the matrix \\(A\\) into the matrix \\(B\\). For each of them, Describe the row operation that turns \\(A\\) into \\(B\\), and Describe the row operation that turns \\(B\\) into \\(A\\). Give your answers in the form: “scale \\(R_2\\) by 3” or “swap \\(R_1\\) and \\(R_4\\)” or “replace \\(R_3\\) with \\(R_3 + \\frac{1}{5} R_1\\).” \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 0 &amp; 7 &amp; 0 &amp; -4 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 4 &amp; 1 &amp; -2 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] 1.4 Designer Parabolas In each part below, set up and solve a linear system of equations to find all possible parabolas of the form \\[ f(x) = a + b x + c x^2 \\] that satisfy the given conditions. For full credit, please solve these by hand, doing all row reductions that bring the system of equations to Reduced Row Echelon Form. On future assignments, you can solve problems like this using either RStudio or WolframAlpha. You are welcome (and, in fact, encouraged) to check your answers using software. \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(2,4)\\). \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(3,10)\\). \\(f(x)\\) passes through the two points: \\((1,3)\\) and \\((3,11)\\). 1.5 Traffic Flow Below you find a section of one-way streets in downtown St Paul, where the arrows indicate traffic direction. The traffic control center has installed electronic sensors that count the numbers of vehicles passing through the 6 streets that lead into and out of this area. Assume that the total flow that enters each intersection equals the the total flow that leaves each intersection (we will ignore parking and staying). Create a system of linear equations to find the possible flow values for the inner streets \\(x_1, x_2, x_3, x_4\\). Using RStudio, enter the augmented matrix of this system, and solve it using the rref command. Type out the general solution to this system of equations. Your answer to part b should be an infinite solution set. Give two distinct solutions that are realistic in terms of traffic flow. Is it possible to close down the street labeled by \\(x_2\\) for road construction? That is, is it possible to have \\(x_2 = 0\\) and to meet the other conditions? "],["problem-set-2.html", "Section 2 Problem Set 2 2.1 Parametric Vector Form 2.2 RREF for a linear system 2.3 RREF for a set of vectors 2.4 Removing free variable columns from a matrix 2.5 A square matrix 2.6 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) 2.7 A Balanced Diet 2.8 Missing Column 2.9 Linear System", " Section 2 Problem Set 2 Due: Tuesday February 2 by 10:00am CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. In problems where you use RStudio for row reduction and are not asked to turn in an R markdown file, you can write something like this: ## Loading required package: pracma You can download the Rmd source file for this problem set. The Problem Set covers sections 1.3, 1.4, 1.5, and 1.7. 2.1 Parametric Vector Form Here is the augmented matrix for a system of linear equations \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\), and its RREF. Give the general solution to this system in parametric vector form. \\[ \\left[ \\begin{array}{ccccc|c} 1 &amp; 1 &amp; -1 &amp; -1 &amp; 2 &amp; 1 \\\\ 1 &amp; 0 &amp; -2 &amp; 1 &amp; 1 &amp; 3 \\\\ -2 &amp; 1 &amp; 5 &amp; 1 &amp; -6 &amp; 2 \\\\ -3 &amp; 0 &amp; 6 &amp; 2 &amp; -8 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; -3 &amp; 6 \\\\ 1 &amp; 0 &amp; -2 &amp; -1 &amp; 3 &amp; -1 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; -2 &amp; 0 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] 2.2 RREF for a linear system Here is the reduced row echelon form of a matrix \\(\\mathsf{A}\\) (you are not given the matrix \\(\\mathsf{A}\\)). \\[ \\mathsf{A} \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 0 &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; -5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] Give the parametric equations of the general solution to the homogenous equation \\(\\mathsf{A} \\mathsf{x} = {\\bf 0}\\). Describe the geometric form of your answer to part (a). For example, you answer should be something like: “it is a plane in \\(\\mathbb{R}^3\\)” or “it is a line in \\(\\mathbb{R}^7\\)” or “it is a point in \\(\\mathbb{R}^4\\).” Suppose that we also know that \\(\\mathsf{A}\\begin{bmatrix} 4 \\\\ 1 \\\\ -3 \\\\ 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 22 \\\\ -13 \\\\ 7 \\\\ \\end{bmatrix}\\). Then give the general solution to \\(\\mathsf{A} \\mathsf{x}= \\begin{bmatrix} 22 \\\\ -13\\\\ 7 \\\\ \\end{bmatrix}\\) in parametric form. 2.3 RREF for a set of vectors Suppose that we have five vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4,\\mathsf{v}_5\\) in \\(\\mathbb{R}^4\\) and that the matrix \\[ A = \\left[ \\begin{array}{ccc} \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\\\ \\mathsf{v}_1 &amp; \\mathsf{v}_2 &amp; \\mathsf{v}_3 &amp;\\mathsf{v}_4 &amp;\\mathsf{v}_5 \\\\ \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\end{array} \\right] \\] has reduced row echelon form \\[ \\begin{bmatrix} 1 &amp; 0 &amp; -3 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 4 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}. \\] Do the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\) span \\(\\mathbb{R}^4\\)? Justify your answer. Is the vector \\(\\mathsf{v}_3\\) in \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2)\\)? Justify your answer. Pick any \\(\\mathsf{b}\\) in \\(\\mathrm{span}(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5)\\). Is there always a unique way to write \\(\\mathsf{b}\\) as a linear combination of \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\)? Justify your answer. 2.4 Removing free variable columns from a matrix Consider the matrix \\[ A =\\left[ \\begin{array}{cccccc} 6 &amp; 5 &amp; -3 &amp; 4 &amp; 2 &amp; -9 \\\\ -7 &amp; -6 &amp; 4 &amp; -5 &amp; -7 &amp; 16 \\\\ -4 &amp; -3 &amp; -1 &amp; 0 &amp; -8 &amp; 9 \\\\ 8 &amp; 7 &amp; -5 &amp; 6 &amp; 1 &amp; -12 \\end{array} \\right]. \\] Use RStudio to show that the columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^4\\). You don’t need to turn in an R file here, just report the reduced row echelon form that you get. Write down the new matrix \\(\\mathsf{A}&#39;\\) gotten by removing the free variable columns from \\(\\mathsf{A}\\). Without using additional calculations on RStudio, explain why the new system \\(\\mathsf{A}&#39; \\mathsf{x} = \\mathsf{b}\\) is consistent and has a unique solution for every choice of \\(\\mathsf{b} \\in \\mathbb{R}^4\\). 2.5 A square matrix Suppose that \\(A\\) is a \\(5\\times 5\\) matrix and \\(\\mathsf{b}\\) is a vector in \\(\\mathbb{R}^5\\) with the property that \\(A\\mathsf{x}=\\mathsf{b}\\) has a unique solution. Explain why the columns of \\(A\\) must span \\(\\mathbb{R}^5\\). Use the reduced row echelon form of \\(A\\) in your explanation. 2.6 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) Suppose that \\(\\mathsf{x}_1\\) and \\(\\mathsf{x}_2\\) are solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) (where \\(\\mathsf{b} \\not= \\mathsf{0}\\)). Decide if any of the following are also solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\). \\(\\mathsf{x}_1+ \\mathsf{x}_2\\) \\(\\mathsf{x}_1 - \\mathsf{x}_2\\) \\(\\frac{1}{2} ( \\mathsf{x}_1 + \\mathsf{x}_2)\\) \\(\\frac{5}{2} \\mathsf{x}_1 - \\frac{3}{2} \\mathsf{x}_2\\). Under what conditions on \\(c\\) and \\(d\\) is \\(\\mathsf{x} = c \\mathsf{x}_1 + d \\mathsf{x}_2\\) a solution to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\)? Justify your answer. Let \\(\\mathsf{u}\\) be the vector that points to \\(1/3\\) of the way from the tip of \\(\\mathsf{v}\\) to the tip of \\(\\mathsf{w}\\) as depicted below. Write \\(\\mathsf{u}\\) as a linear combination of \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) (hint: think about \\(\\mathsf{w} - \\mathsf{v}\\)) If \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) are solutions to \\(A x = \\mathsf{b}\\) then show that \\(\\mathsf{u}\\) is also a solution to \\(A \\mathsf{x} = \\mathsf{b}\\). 2.7 A Balanced Diet An athlete wants to consume a daily diet of 200 grams of carbohydrates, 60 grams of fats and 160 grams of proteins. Here are some of their favorite foods. Table 2.1: Food Carb/Fat/Protein (grams) food carbs fats proteins almonds 3 8 5 avocado 15 31 4 beans 20 1 8 bread 12 1 2 cheese 1 5 3 chicken 0 13 50 egg 1 5 6 milk 12 8 8 zucchini 6 0 2 Answer the following questions, using RStudio for your calculations. Each response must use two or more of the following terms: linear combination, span, linearly dependent, linearly independent. Explain why they cannot achieve their daily goal by eating only almonds, milk and zucchini. Explain why they cannot achieve their daily goal by eating only almonds, beans and cheese. Find a valid one-day diet consisting of almonds, chicken, and zucchini. 2.8 Missing Column The matrices below are supposed to be \\(3 \\times 3\\) but in each case the third column was accdentally deleted. In each case, add a third column, that has no 0s in it and is different from either the first or second column, so that the columns of \\(A\\) are linearly dependent and so that the columns of \\(B\\) are linearly independent. Briefly describe your strategy. \\[ A=\\left[ \\begin{matrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{matrix}\\right] \\qquad\\qquad B=\\left[ \\begin{matrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{matrix}\\right] \\] 2.9 Linear System Use R to solve this problem. Do your computations in an R markdown file. Knit the file to HTML and include it with your homework. Here you can download a template for doing this problem (including the matrix typed out for you!). \\[ A =\\left[ \\begin{array}{cccccc} 12 &amp; 10 &amp; -6 &amp; 8 &amp; 4 &amp; -18 \\\\ -7 &amp; -6 &amp; 4 &amp; -5 &amp; -7 &amp; 16 \\\\ 9 &amp; 9 &amp; -9 &amp; 9 &amp; 9 &amp; -27 \\\\ -4 &amp; -3 &amp; -1 &amp; 0 &amp; -8 &amp; 9 \\\\ 8 &amp; 7 &amp; -5 &amp; 6 &amp; 1 &amp; -12 \\\\ \\end{array} \\right] \\quad b = \\begin{bmatrix} 14 \\\\ -12 \\\\ 9\\\\ -15 \\\\6 \\end{bmatrix} \\] Show that the columns of \\(A\\) are linearly dependent by finding two different dependency relations among them. You can write your answer in a form like \\(5 a1+ 4 a2 + 3 a3 + 2 a4 + a5 = 0\\), where \\(a1, a2,\\) etc are the columns of \\(A\\). Augment \\(A\\) with \\(b\\) and show that \\(A x = b\\) is consistent and has infinitely many solutions. Remove the free-variable columns from \\(A\\) to get a new, smaller matrix \\(A&#39;\\). Show that \\(A&#39; x = b\\) has a unique solution and say what that solution is. "],["problem-set-3.html", "Section 3 Problem Set 3 3.1 Properties of Linear Transformations 3.2 Partial Information about a Linear Transformation 3.3 House Renovations 3.4 Matrix of a Nonlinear Transformation? 3.5 A Proof 3.6 Inner and Outer Products 3.7 Archaeological Seriation 3.8 Rental Car 3.9 Adjacency Matrix", " Section 3 Problem Set 3 Due: Tuesday February 9 by noon CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 1.8, 1.9, 2.1, 2.2. 3.1 Properties of Linear Transformations Here are the row reductions pf 4 matrices into reduced row echelon form. \\[ \\begin{array}{ll} A \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 5 &amp; -3 &amp; 0\\\\ 0 &amp; 1 &amp; -2 &amp; 8 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\qquad &amp; B \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\\\ \\\\ C \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} &amp; D \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{bmatrix} \\end{array} \\] In each case, if \\(T_M\\) is the linear transformation given by the matrix product \\(T_M(x) = M x\\), where \\(M\\) is the given matrix, then \\(T_M: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a transformation from domain \\(\\mathbb{R}^n\\) to codomain (aka target) \\(\\mathbb{R}^m\\). Determine the appropriate values for \\(n\\) and \\(m\\), and decide whether \\(T_M\\) is one-to-one and/or onto. Submit your answers in table form, as shown below. \\[ \\begin{array} {|c|c|c|c|c|} \\hline \\text{transformation} &amp; n &amp; m &amp; \\text{one-to-one?} &amp; \\text{onto?} \\\\ \\hline T_A &amp;\\phantom{\\Big\\vert XX}&amp;\\phantom{\\Big\\vert XX}&amp;&amp; \\\\ \\hline T_B &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline T_C &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline T_D &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline \\end{array} \\hskip5in \\] 3.2 Partial Information about a Linear Transformation We are given that \\(T: \\mathbb{R}^4 \\rightarrow \\mathbb{R}^3\\) is a linear transformation such that: \\[ T\\left(\\begin{bmatrix} 3 \\\\ ~2~ \\\\ 1 \\\\ 2 \\end{bmatrix} \\right)=\\begin{bmatrix} ~2~ \\\\ 3 \\\\ 6 \\end{bmatrix} \\qquad\\hbox{and}\\qquad T\\left(\\begin{bmatrix}~~2 \\\\ -1 \\\\ 0 \\\\ -1 \\end{bmatrix} \\right)=\\begin{bmatrix} 2 \\\\ ~0~ \\\\ 1 \\end{bmatrix}. \\] If that is all we know about \\(T\\), then do we have enough information to compute the value of \\(T\\) below? \\[T\\left(\\begin{bmatrix} 5 \\\\ 8 \\\\ ~3~ \\\\ 8 \\end{bmatrix} \\right) = \\hskip5in\\] If yes, then compute it (showing how you do so). If no, then explain why not. Hint: try to write the third input vector as a linear combination of the first two. 3.3 House Renovations Find the matrix of a linear transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) that performs the given transformation of my house. (Hint: use the base, the doorway and the peak of the roof as a guide.) Transformation #1 \\(\\qquad \\qquad\\) Transformation #2 \\(\\qquad \\qquad\\) 3.4 Matrix of a Nonlinear Transformation? This problem illustrates what happens if you try to make the matrix of a transformation that is not linear. Consider the transformation \\(T\\) defined by \\[ T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right) = \\begin{bmatrix} x_1 + x_2^2 + x_3 \\\\ 2 x_2 + x_1 x_3 + 1 \\\\ 2 x_1 + 3 x_2 + x_3 \\end{bmatrix} \\] This is not a linear transformation. Let’s see what happens if we compute its matrix anyway. Compute \\(T(\\mathbf{e}_1)\\), \\(T(\\mathbf{e}_2)\\), and \\(T(\\mathbf{e}_3)\\), and put the vectors you get in the columns of a matrix \\(A\\). Then compute the product below: \\[ \\underbrace{\\begin{bmatrix} \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\cdot \\\\ \\end{bmatrix}}_{A} \\begin{bmatrix} x_1 \\\\ x_ 2 \\\\ x_3 \\end{bmatrix} = \\] Explain how the result of this computation demonstrates that \\(T\\) is not linear. 3.5 A Proof Let \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) be a linear transformation. Suppose that \\(\\{v_1, v_2, v_3, v_4\\}\\) is a linearly independent set of vectors in \\(\\mathbb{R}^n\\) but the set of images \\(\\{T(v_1), T(v_2), T(v_3), T(v_4)\\}\\) is a linearly dependent set in \\(\\mathbb{R}^m\\). In the following steps, you will prove that \\(T\\) is not one-to-one. Write out clearly, using the definition, what it means for \\(\\{v_1, v_2, v_3, v_4\\}\\) to be linearly independent. Write out clearly, using the definition, what it means for \\(\\{T(v_1), T(v_2), T(v_3), T(v_4)\\}\\) to be linearly dependent. Use the definition of linear transformation and parts (a) and (b) above to argue that \\(T(x) = \\vec{0}\\) for some nonzero vector \\(x \\in \\mathbb{R}^n\\). Explain why this tells us that \\(T\\) is not one-to-one. 3.6 Inner and Outer Products We can also think of vectors as matrix. A column vector is an \\(n \\times 1\\) matrix and a row vector is a \\(1 \\times n\\) matrix. Compute the following products. These matrix products are called inner products (or dot products) of the vectors. \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\1 \\\\1 \\\\\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 4 &amp; -1 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 2 \\\\ 5 \\\\ 0 \\\\ -1 \\\\\\end{bmatrix} = \\hskip3in \\] b. Now compute the following products. These are called outer products. \\[ \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} \\begin{bmatrix} 1 &amp; -5 &amp; 2 &amp; 3\\end{bmatrix} = \\hskip3in \\] \\[ \\begin{bmatrix} 1 \\\\ 2 \\\\1 \\\\3 \\\\\\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1\\end{bmatrix} =\\hskip3in \\] Row reduce both of the matrices that you get in part b (this should be easy to do by hand,but you can use R if you want to). How many pivots do you get? Explain why you always get this number of pivots when you row reduce an outer product. 3.7 Archaeological Seriation The matrix \\(A\\) below is used in archaeological dating. Its rows correspond to four different grave sites \\(G_1, G_2, G_3, G_4\\) and its columns correspond to five types of pottery\\(P_1, P_2, P_3, P_4, P_5\\). There is a 1 in position \\(i\\)-\\(j\\) if pottery type \\(P_j\\) is found in grave \\(G_i\\) (and a 0 otherwise). \\[ A=\\begin{array}{c|ccccc} &amp; P_1 &amp; P_2 &amp; P_3 &amp; P_4 &amp; P_5 \\\\ \\hline G_1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ G_2 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ G_3 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ G_4 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ \\end{array} \\] Compute the matrix \\(\\mathbf{G} = A A^T\\), where \\(A^T\\) is the transpose of \\(A\\), meaning that the rows and columns have been interchanged. Give the meaning of the \\(i\\)-\\(j\\) entry of \\(\\mathbf{G}\\) (the entry in row \\(i\\) and column \\(j\\)). State clearly the meaning of this entry using complete sentences (or sentence) and explain why it has this meaning. 3.8 Rental Car Solve this problem using R and turn in a markdown file knitted to .html. A group of Macalester alumni open a rental car company specializing in renting electric cars. As a start, they have opened offices in St. Paul, Rochester, and Duluth. Through market research they find that of the cars rented in St. Paul, 85% will get returned in St. Paul, 9% will get returned in Rochester, and 6% will get returned in Duluth. Of the cars rented in Rochester, 40% will get returned in St. Paul, 50% will get returned in Rochester, and 10% in Duluth. Of the cars rented in Duluth, 35% will get returned in St. Paul, 55% in Rochester, and 60% in Duluth. This information is represented in the matrix below. StP = c(.85,.09,.06) Roch = c(.30,.60,.10) Dul = c(.35,.05,.60) M = cbind(StP,Roch,Dul) M ## StP Roch Dul ## [1,] 0.85 0.3 0.35 ## [2,] 0.09 0.6 0.05 ## [3,] 0.06 0.1 0.60 Such a matrix is called a probability matrix or a stochastic matrix because it contains numbers between 0 and 1 and each of its columns sum to 1. The owners are trying to use this data to estimate how much of their fleet will be at each location on average in the long run. Assume that initially they locate 20 cars in each city. This can be recorded by the vector v0 = c(20,20,20). Apply, M to v0, call this vector v1, and explain, using how the matrix-vector product works, why v1 represents the number of cars at each location one day later (for simplicity, we assume that each rental is for 1 day). Now apply M to v1 and call it v2. This should represent the number of cars at each location 2 days later. Also compute the square of the matrix M and call it M2. Confirm that M2 times v0 is the same as M times v1. Write a for loop that applies M over and over again to see what happens to the distribution of cars in the long-run (we will learn how to do this in class but you can also probably just google it). Does this sequence stabilize or does it keep changing after each application? If it does stabilize, how long does it take to stabilize (to within 0.1 cars at each location). Does the starting distribution matter? Try 4 different starting distributions (with a total of 60 cars) and see what the final distribution looks like in each case. For one of your 4 starting distributions, try all 60 cars at one of the locations. 3.9 Adjacency Matrix You can do this problem in R or by hand. Consider the matrix \\(A\\) defined here A = rbind(c( 0 , 1 , 0 , 1 , 1 ,0), c(1 , 0 , 1 , 1 , 0, 0 ),c( 0 , 1 , 0 , 1 , 1, 0 ), c( 1 , 1 , 1 , 0 , 1, 0 ),c( 1 , 0 , 0 , 1 , 0, 1 ), c(0, 0, 0, 0, 1, 0)) A ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0 1 0 1 1 0 ## [2,] 1 0 1 1 0 0 ## [3,] 0 1 0 1 1 0 ## [4,] 1 1 1 0 1 0 ## [5,] 1 0 0 1 0 1 ## [6,] 0 0 0 0 1 0 This matrix represents the connections in the network diagram below. There is a 1 in position \\((i,j)\\) of the matrix if there is a connection (an edge) between vertex \\(i\\) and vertex \\(j\\) and there is a 0 if there is not. Compute \\(A v\\) where \\(v\\) is the vector of all 1’s. Explain what this new vector tells us about the graph. Compute \\(A^2 = A A\\), the square of the matrix \\(A\\). Look at the \\((2,5)\\) entry of \\(A^2\\). Explain what this entry says about connections in the network. Do the same for the \\((2,3)\\) and the \\((2,6)\\) entry of \\(A^2\\). "],["important-definitions.html", "Section 4 Important Definitions 4.1 Vector Spaces 4.2 Matrices 4.3 Orthogonality 4.4 Spectral Decompostion", " Section 4 Important Definitions 4.1 Vector Spaces span A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span a vector space \\(V\\) if for every \\(\\mathsf{v} \\in V\\) there exist a set of scalars (weights) \\(c_1, c_2, \\ldots, c_n \\in \\mathbb{R}\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, \\ldots, c_n]^{\\top}\\) is a solution to \\(A x = \\mathsf{v}\\). linear independence A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\ldots, \\mathsf{v}_n\\) are linearly independent if the only way to write \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] is with \\(c_1 = c_2 = \\cdots = c_n = 0\\). Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(A x = \\mathsf{0}\\) has only the trivial solution. linear dependence Conversely, a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly dependent if there exist scalars \\(c_1, c_2,\\ldots, c_n \\in \\mathbb{R}\\) that are not all equal to 0 such that \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] This is called a dependence relation among the vectors. Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, c_2, \\ldots, c_n]^{\\top}\\) is a nontrivial solution to \\(A \\mathsf{x} = \\mathsf{0}\\). linear transformation A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation when: \\(T(\\mathsf{u} + \\mathsf{v}) = T(\\mathsf{u}) + T(\\mathsf{v})\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\) (preserves addition) \\(T(c \\mathsf{u} ) = c T(\\mathsf{u})\\) for all \\(\\mathsf{u} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\) (preserves scalar multiplication). It follows from these that also \\(T(\\mathsf{0}) = \\mathsf{0}\\). one-to-one A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a one-to-one when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at most one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). onto A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a onto when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at least one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). subspace A subset \\(S \\subseteq \\mathbb{R}^n\\) is a subspace when: \\(\\mathsf{u} + \\mathsf{v} \\in S\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in S\\) (closed under addition) \\(c \\mathsf{u} \\in S\\) for all \\(\\mathsf{u}\\in S\\) and \\(c \\in \\mathbb{R}\\) (closed under scalar multiplication) It follows from these that also \\(\\mathsf{0} \\in S\\). basis A basis of a vector space (or subspace) \\(V\\) is a set of vectors \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) in \\(V\\) such that \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span \\(V\\) \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly independent Equivalently, one can say that \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) is a basis of \\(V\\) if for every vector \\(\\mathsf{v} \\in V\\) there is a unique set of scalars \\(c_1, \\ldots, c_n\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] (The fact that there is a set of vectors comes from the span; the fact that they are unique comes from linear independence). dimension The dimension of a subspace \\(W\\) is the number of vectors in any basis of \\(W\\). This is also the fewest number of vectors required to span the subspace. 4.2 Matrices invertible The square \\(n \\times n\\) matrix \\(A\\) is invertible when there exists an \\(n \\times n\\) matrix \\(A^{-1}\\) such that \\(A A^{-1} = I = A^{-1} A\\). The Invertible Matrix Theorem collects over two dozen equivalent conditions, each of which guarantees that \\(A\\) is invertible. null space The null space \\(\\mbox{Nul}(A) \\subset \\mathbb{R}^n\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of solutions to the homogeneous equation \\(A \\mathsf{x} = \\mathbf{0}\\)&gt; We also write this as \\[ \\mbox{Nul}(A) = \\{ \\mathsf{x} \\in \\mathbb{R}^n : A \\mathsf{x} = \\mathbf{0} \\} \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the kernel of \\(T\\) is the null space of matrix \\(A\\). column space The column space \\(\\mbox{Col}(A) \\subset \\mathbb{R}^m\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of all linear combinations of the columns of \\(A\\). For \\(A = \\begin{bmatrix} \\mathsf{a}_1 &amp; \\mathsf{a}_2 &amp; \\cdots &amp; \\mathsf{a}_n \\end{bmatrix}\\), we have \\[ \\mbox{Col}(A) = \\mbox{span} ( \\mathsf{a}_1, \\mathsf{a}_2, \\ldots , \\mathsf{a}_n ) \\] We also write this as \\[ \\mbox{Col}(A) = \\{ \\mathsf{b} \\in \\mathbb{R}^m : \\mathsf{b} = A \\mathsf{x} \\mbox{ for some } \\mathsf{x} \\in \\mathbb{R}^n \\}. \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the range (also called the image) of \\(T\\) is the column space of matrix \\(A\\). rank The rank of the \\(m \\times n\\) matrix \\(A\\) is the dimension of the column space of \\(A\\). This is also the number of pivot columns of the matrix. eigenvalue and eigenvector For a square \\(n \\times n\\) matrix \\(A\\), the scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue for \\(A\\) when there exists a nonzero vector \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(A \\mathsf{x} = \\lambda \\mathsf{x}\\). The nonzero vector \\(\\mathsf{x}\\) is the eigenvector for eigenvalue \\(\\lambda\\). The collection of all of these eigenvalues and eigenvectors is called the eigensystem of A. diagonalization A square \\(n \\times n\\) matrix is diagonalizable when \\(A = P D P^{-1}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an invertible matrix. In this case, the eigenvalues of \\(A\\) are the diagonal entries of \\(D\\) and their corresponding eigenvectors are the columns of \\(P\\). dominant eigenvalue The eigenvalue \\(\\lambda\\) of the square matrix \\(A\\) is the dominant eigenvalue when \\(| \\lambda | &gt; | \\mu |\\) where \\(\\mu\\) is any other eigenvalue of \\(A\\). The dominant eigenvalue determines the long-term behavior of \\(A^t\\) as \\(t \\rightarrow \\infty\\). 4.3 Orthogonality length The length of a vector \\(\\mathsf{v}\\) is \\[ \\| \\mathsf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}. \\] distance and angle The distance between vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) is \\[ \\mbox{dist}(\\mathsf{u},\\mathsf{v}) = \\| \\mathsf{u} - \\mathsf{v} \\|. \\] The angle \\(\\theta\\) between these vectors is determined by \\[ \\cos \\theta = \\frac{\\mathsf{u} \\cdot \\mathsf{v}}{ \\| \\mathsf{u} \\| \\, \\| \\mathsf{v} \\|}. \\] orthogonal The vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are orthogonal when \\(\\mathsf{u} \\cdot \\mathsf{v} = 0\\). This means that either one of them is the zero vector, or they are perpendicular to one another. orthogonal complement If \\(W \\subset \\mathbb{R}^n\\) is a subspace, then its orthogonal complement \\(W^{\\perp}\\) is the set of all vectors in \\(\\mathsf{R}^n\\) that are orthogonal to \\(W\\). We also write \\[ W^{\\perp} = \\{ \\mathsf{v} \\in \\mathbb{R}^n : \\mathsf{v} \\cdot \\mathsf{w} \\mbox{ for all } \\mathsf{w} \\in W \\}. \\] orthonormal set A collection of vectors \\(\\mathsf{u}_1, \\mathsf{u}_2, \\ldots, \\mathsf{u}_k\\) are an orthonormal set when every vector has length 1 and the vectors are pairwise orthogonal. orthogonal matrix orthogonal matrix A square \\(n \\times n\\) matrix \\(P\\) is an orthogonal matrix when its columns are an orthonormal set. As a result, we have \\(P^{-1} = P^{\\top}\\). projection and residual The orthogonal projection of vector \\(\\mathsf{y}\\) into a subspace \\(W\\) is the unique vector \\(\\hat{\\mathsf{y}} \\in W\\) such that \\(\\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}} \\in W^{\\perp}\\). The vector \\(\\mathsf{z}\\) is called the residual vector for the projection. 4.4 Spectral Decompostion orthogonal diagonalization Every symmetric \\(n \\times n\\) matrix is orthogonally diagonalizable, meaning that we have \\(A = P D P^{\\top}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix. The diagonal entries of \\(D\\) are the eigenvalues of \\(A\\) and the columns of \\(P\\) are the corresponding orthonormal eigenvectors. Furthermore, the eigenvalues of \\(A\\) are nonnegative. spectral decomposition A symmetric matrix \\(A\\) can be written as a linear combination of rank 1 matrices derived from the orthonormal eigensystem of \\(A\\). In particular, we have \\[ A = \\lambda_1 \\mathsf{u}_1 \\mathsf{u}_1^{\\top} + \\lambda_2 \\mathsf{u}_2 \\mathsf{u}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{u}_n \\mathsf{u}_n^{\\top}. \\] This linear combination of rank 1 vectors is called the spectral decomposition of \\(A\\). singular value decomposition (SVD) Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be factored into its singular value decomposition \\(U \\Sigma V^{\\top}\\) where \\(U\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\Sigma\\) is a matrix whose nonzero entries are the positive numbers \\(\\sigma_1, \\ldots , \\sigma_r\\), which appear in decreasing order on the diagonal, and \\(V\\) is an \\(n \\times n\\) orthogonal matrix. The nonzero entries of \\(\\Sigma\\) are called the singular values of \\(A\\). The columns of \\(U\\) are the left singular vectors and the rows of \\(V^{\\top}\\) are the right singular vectors. SVD spectral decomposition Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be written as a linear combination of rank 1 matrices derived from the singular value decomposition of \\(A\\). In particular, we have \\[ A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\sigma_r \\mathsf{u}_r \\mathsf{v}_r^{\\top}. \\] This linear combination of rank 1 vectors is called the (SVD) spectral decomposition of \\(A\\). "],["week-1-learning-goals.html", "Section 5 Week 1 Learning Goals 5.1 Solving Linear Equations 5.2 RStudio 5.3 Vocabulary 5.4 Conceptual Thinking", " Section 5 Week 1 Learning Goals Here are the knowledge and skills you should master by the end of this first, shorter week. 5.1 Solving Linear Equations I should be able to do the following tasks: Identify linear systems from nonlinear systems Create a linear system to solve a variety of applied scenarios Convert between a linear system and an augmented matrix Row reduce an augmented matrix into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Use REF to determine whether a linear system is consistent or inconsistent Use REF to determine whether a consistent system has a unique solution or an infinite number of solutions Use RREF to find explicit equations for the solution set of a consistent system 5.2 RStudio I should be able to do the following tasks: Log in to Macalester’s RStudio server Upload R Markdown files to RStudio Knit R Markdown to produce HTML Use RStudio to create vectors and matrices Use the rref command from pracma to solve a linear system 5.3 Vocabulary I should know and be able to use and explain the following terms: elementary row operation (and be able to state them) augmented matrix REF and RREF pivot position basic variable (pivot variable) free variable consistent system and inconsistent system 5.4 Conceptual Thinking I should understand and be able to perform the following conceptual tasks: Model 2-dimensional linear systems as the intersections of lines Model 3-dimensional linear systems as the intersections of planes "],["week-2-learning-goals.html", "Section 6 Week 2 Learning Goals 6.1 Solution Sets, Span and Linear Independence 6.2 Vocabulary 6.3 Conceptual Thinking", " Section 6 Week 2 Learning Goals Here are the knowledge and skills you should master by the end the second week. 6.1 Solution Sets, Span and Linear Independence I should be able to do the following tasks: Go back and forth between (i) systems of equations, (ii) vector equations, and (iii) the matrix equation \\(Ax = b\\). Compute and understand the matrix-vector product \\(A x\\) both as a linear combination of the columns of A and as the dot product of \\(x\\) with the rows of \\(A\\). Write the solution set to \\(Ax=b\\) as a parametric vector equation. Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) 6.2 Vocabulary I should know and be able to use and explain the following terms or properties. \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) homogeneous and nonhomogeneous equations parametric vector equations linear independence and linear dependence 6.3 Conceptual Thinking I should understand and be able to explain the following concepts: Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b \\in \\mathbb{R}^m\\), the system \\(A x = b\\) has at least one solution Each \\(b \\in \\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. Understand the relation between homogeneous solutions and nonhomogeneous solutions. Linear independence Span More than \\(n\\) vectors in \\(\\mathbb{R}^n\\) must be linearly dependent. "],["week-3-learning-goals.html", "Section 7 Week 3 Learning Goals 7.1 Linear Transformations and Matrix Inverses 7.2 Vocabulary 7.3 Conceptual Thinking", " Section 7 Week 3 Learning Goals Here are the knowledge and skills you should master by the end the third week. 7.1 Linear Transformations and Matrix Inverses I should be able to do the following tasks: Determine whether a mapping from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) is a linear transformation. Use the RREF of the corresponding matrix to determine whether \\(T(\\mathsf{x})\\) is one-to-one and/or onto. Describe 2D linear transformations as a mixture of geometric operations, including expansion, contraction, reflection, rotation, shearing and dimension reduction. Perform a 2D translation using 3D homogeneous coordinates. Multiply an \\(m \\times n\\) matrix with an \\(n \\times p\\) matrix to get an \\(m \\times p\\) matrix. Determine whether a \\(2 \\times 2\\) matrix is invertible. Find the inverse of a \\(2 \\times 2\\) matrix by hand. Use RStudio to check for invertiblity and to find the inverse of an \\(n \\times n\\) square matrix. Explain the connection between Gaussian Elimination, elementary matrices, and the matrix inverse. 7.2 Vocabulary I should know and be able to use and explain the following terms or properties. linear transformation: \\(T(a \\mathsf{u} + b \\mathsf{v}) = a T(\\mathsf{u}) + b T(\\mathsf{v})\\) domain, codomain (aka target) and range (aka image) \\(T\\) maps vector \\(\\mathsf{x}\\) to its image \\(T(\\mathsf{x})\\) one-to-one onto standard matrix for a linear transformation homogeneous coordinates transpose of a matix invertible matrix elementary matrices 7.3 Conceptual Thinking I should understand and be able to explain the following concepts: A linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) corresponds to multiplication by an \\(m \\times n\\) matrix \\(A\\). \\(T(\\mathsf{x})=\\mathsf{A} \\mathsf{x}\\) is a one-to-one linear transformations if and only \\(\\mathsf{A}\\) has linearly independent columns \\(T(\\mathsf{x})=\\mathsf{A} \\mathsf{x}\\) is an onto linear transformations if and only if the columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^m\\). The Invertible Matrix Theorem (Section 2.3, Theorem 8, page 112) is one of the highlights of the course! It gives 12 different conditions that all equivalent! You should think deeply about why everything comes together like this for square matrices. "],["week-4-learning-goals.html", "Section 8 Week 4 Learning Goals 8.1 Vector Spaces and the Determinant 8.2 Vocabulary 8.3 Conceptual Thinking", " Section 8 Week 4 Learning Goals Here are the knowledge and skills you should master by the end of the fourth week. 8.1 Vector Spaces and the Determinant I should be able to do the following tasks: Prove/disprove that a subset of a vector space is a subspace. Prove/disprove that a set of vectors is linearly dependent. Prove/disprove that a set of vectors span a vector space (or a subspace). Find the kernel and image of \\(T(\\mathsf{x}) = Ax\\). Determine whether a set of vectors is a basis. Find a basis for \\(\\mathrm{Nul}(A)\\) and a basis for \\(\\mathrm{Col}(A)\\). Find the change-of-coordinate matrix \\(P_{\\mathcal{B}}\\) from basis \\({\\mathcal{B}}\\) to the standard basis \\(\\mathcal{S}\\). Use matrix inverses (and RStudio) to find the change-of-coordinate matrix \\(P_{\\mathcal{B}}^{-1}\\) from basis \\({\\mathcal{S}}\\) to the standard basis \\(\\mathcal{B}\\). Find the coordinate vector with respect to a given basis. Find the dimension of a vector space (or subspace) by finding or verifying a basis. Find the determinant of a \\(2 \\times 2\\) matrix by hand. Find the determinant of a \\(3 \\times 3\\) matrix by using row operations/cofactor expansion/permutation method. Use RStudio to calculate the determinant of a square matrix. Use \\(\\det(A)\\) to decide whether the square matrix \\(A\\) is invertible. 8.2 Vocabulary I should know and be able to use and explain the following terms or properties. every one of these Important Definitions subspace null space and column space of a matrix kernel and image of a linear transformation basis coordinate vector with respect to a basis change-of-coordinates matrix the coordinate vector with respect to a basis the dimension of a vector space (or a subspace) determinant 8.3 Conceptual Thinking I should understand and be able to explain the following concepts: A vector space consists of a collection of vectors and all of their linear combinations. A subspace is a subset of a vector space that is also a vector space by itself (closed under linear combinations). The solutions to \\(A \\mathsf{x} = \\mathbb{0}\\) form a subspace. The span of the columns of \\(A\\) form a subspace. How the kernel and image of \\(T(\\mathsf{x}) = Ax\\) correspond to the nullspace and columnspace of \\(A\\). Every basis of a given vector space (or subspace) contains the same number of vectors. Why every vector in a vector space has a unique representation as a linear combination of a given basis \\({\\mathcal{B}}\\). How dimension relates to span and linear independence. Interpret \\(\\det(A)\\) as a measure the expansion/contraction of “volumes” in \\(\\mathbb{R}^n\\) under the linear transformation \\(T(\\mathsf{x})=A\\mathsf{x}\\). "],["week-5-6-learning-goals.html", "Section 9 Week 5-6 Learning Goals 9.1 Eigensystems 9.2 Vocabulary 9.3 Conceptual Thinking", " Section 9 Week 5-6 Learning Goals Here are the knowledge and skills you should master by the end of the fifth and sixth weeks. 9.1 Eigensystems I should be able to do the following tasks: Check whether a given vector \\(\\mathsf{v}\\) is an eigenvector for square matrix \\(A\\). Find the eigenvalues of a matrix \\(2 \\times 2\\) matrix by hand, using the characteristic equation Find the eigenvalues of a triangular matix by inspection. Given the eigenvalues of matrix \\(A\\), find the eigenvectors by solving \\((A \\lambda I) = \\mathbf{0}\\). Find the eigenvalues and (“human readable”) eigenvectors of an \\(n \\times n\\) matrix \\(A\\) using eigen(A) on RStudio. Determine whether a matrix is diagonalizable. Factor a diagonalizable \\(n \\times n\\) matrix as \\(A = PDP^{-1}\\) where \\(D\\) is a diagonal matrix of eigenvalues and \\(P\\) is the matrix whose columns are the corresponding eignvectors. Use RStudio to find complex eigenvalues and (“human readable”) eigenvectors of a square matrix. Factor a \\(2 \\times 2\\) scaling-rotation matrix as \\(A = P C P^{-1}\\) where \\(C\\) is a scaling-rotation matrix \\(\\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix}\\) and \\(P = [ \\mathsf{w}, \\mathsf{u}]\\) where \\(\\mathsf{v} = \\mathsf{u} + i \\mathsf{w}\\) is the eigenvector for \\(\\lambda = a + b i\\). Use RStudio to find the Gould Index of a network Use RStudio to create a 2D plot of pairs of eigenvalues of a square matrix Use the dominant eigenvalue and dominant eigenvector to determine the long-term behavior of a dynamical system Create a trajectory of a \\(2 \\times 2\\) dynamical system (either using RStudio or by using a given vector field plot) and then relate the trajectory to the eigenvectors and eigenvalues Interpret the constants in the \\(2 \\times 2\\) matrix for two interacting populations (competition, predator-prey, mutualism, etc) Use RStudio to investigate the animal population modeled with a Leslie matrix. 9.2 Vocabulary I should know and be able to use and explain the following terms or properties. eigenvalue, eigenvector and eigenspace characteristic equation diagonalizable matrix similar matrices algebraic multiplicity of an eigenvalue geometric multiplicity of an eigenvalue scaling-rotation matrix Gould Index discrete dynamical system trajectory dominant eigenvalue and dominant eigenvector population model Leslie matrix 9.3 Conceptual Thinking I should understand and be able to explain the following concepts: An eigenspace of \\(A\\) is a subspace that is fixed under the linear transformation \\(T(\\mathsf{x}) = A \\mathsf{x}\\). An eigenvalue \\(\\lambda\\) with \\(1 &lt;| \\lambda |\\) corresponds to expansion. An eigenvalue \\(\\lambda\\) with \\(0 &lt; | \\lambda | &lt; 1\\) corresponds to contraction. A complex eigenvalue corresponds to a rotation in a 2D subspace. The eigenspace for \\(\\lambda\\) is the subspace \\(\\mathrm{Nul}(A - \\lambda I)\\). A matrix is not diagonalizable when it has complex eigenvalues. A matrix is not diagonalizable when it has an eigenvalue whose algebraic mutiplicity is strictly larger than its geometrix multiplicity. The Gould Index measures the centrality of a vertex in the network. The eigenvalues of a matrix “encode some of the patterns” found in the matrix. Larger magnitude eigenvalues indicate more important patterns. The long-term behavior of a dynamical system is determined by its dominent eigenvalue and eigenvector. Any population model predicts one of: long term growth, extinction, convergence to a stable population, or convergence to a stable orbit of populations. "],["week-7-8-learning-goals.html", "Section 10 Week 7-8 Learning Goals 10.1 Orthogonality and SVD 10.2 Vocabulary 10.3 Conceptual Thinking", " Section 10 Week 7-8 Learning Goals Here are the knowledge and skills you should master by the end of the seventh and eighth weeks. 10.1 Orthogonality and SVD I should be able to do the following tasks: Find the length of a vector Find the distance between two vectors Normalize a vector Find the cosine of the angle between two vectors Find the orthogonal projection of one vector onto another Find the orthogonal projection of one vector onto a subspace (using an orthogonal basis) Find the orthogonal complement of a subspace Use the Gram-Schmidt process to create an orthonormal basis (starting from a given basis) Find the least squares approximation for an inconsistent system Formulate a curve fitting problem as an inconsistent linear system \\(A \\mathsf{x} = \\mathsf{b}\\) Orthogonally diagonalize a symmetric matrix as \\(A=PDP^{\\top}\\). Find the spectral decomposition \\(A = \\lambda_1 \\mathsf{v}_1 \\mathsf{v}_1^{\\top} + \\lambda_2 \\mathsf{v}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{v}_n \\mathsf{v}_n^{\\top}\\) of a symmetric matrix \\(A\\) Use an orthogonal diagonalization to find the best rank \\(k\\) approximation of symmetric matrix \\(A\\) Find the singular value decomposition \\(A = U \\Sigma V^{\\top}\\) of a rectangular matrix \\(A\\) Use the singular value decomposition \\(A = U \\Sigma V^{\\top}\\) to find orthonormal bases for \\(\\mbox{Row}(A), \\mbox{Nul}(A), \\mbox{Col}(A), \\mbox{Nul}(A^{\\top})\\) Find the SVD spectral decomposition \\(A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{u}_r \\mathsf{v}_r^{\\top}\\) of a rank \\(r\\) matrix \\(A\\) Use SVD to find the best rank \\(k\\) approximation of a matrix \\(A\\) 10.2 Vocabulary I should know and be able to use and explain the following terms or properties. dot product of two vectors \\(\\mathsf{v} \\cdot \\mathsf{w} = \\mathsf{v}^{\\top} \\mathsf{w}\\) (aka scalar product, inner product) length (magnitude) of a vector angle between vectors normalize unit vector orthogonal vectors orthogonal complement of a subspace orthogonal projection orthogonal basis orthonormal basis Gram-Schmidt process normal equations for a least squares approximation least squares solution residual vector symmetric matrix orthogonally diagonalizable outer product of two vectors \\(\\mathsf{v} \\, \\mathsf{w}^{\\top}\\) spectral decomposition of a symmetric matrix singular value decomposition singular value, left singular vector, right singular vector SVD spectral decomposition of a rectangular matrix image compression 10.3 Conceptual Thinking I should understand and be able to explain the following concepts: The dot product gives an algebraic encoding of the geometry (lengths and angles) of \\(\\mathbb{R}^n\\) If two vectors are orthogonal, then they are perpendicular, or one of them is the zero vector An orthogonal projection is a linear transformation The row space of a matrix is orthogonal to its nullspace The inverse of orthogonal matrix \\(A\\) is the transpose \\(A^{\\top}\\) Cosine similarity is a useful way to compare vectors, especially in high-dimensional vector spaces. The residual vector measures the quality of fit of a least squares solution The outer product \\(\\mathsf{v}\\, \\mathsf{w}^{\\top}\\) is a square matrix with rank 1 Singular value decomposition is a generalization of orthogonal diagonalization The singular values of \\(A\\) are the square roots of the eigenvalues of \\(A^{\\top}A\\). The spectral decomposition of a matrix allows us to approximate a matrix with a linear combination of rank 1 matrices. The relative magnitudes of the eigenvectors/singular values indicate the quality of the spectral decomposition approximation. "],["linear-systems-in-r.html", "Section 11 Linear Systems in R 11.1 Getting started with R 11.2 Building Vectors and Matrices 11.3 Solving a Linear System 11.4 Solving another Linear System 11.5 Appendix: Dimensionless Vectors in R", " Section 11 Linear Systems in R 11.1 Getting started with R To use RStudio, you have two choices: Use the cloud version by logging in to Rstudio.macalester.edu. This is the easiest way to use RStudio and works great for our course. You can also download the free desktop version of RStudio. If you plan to go on to take more MSCS classes, especially in statistics and data science, you may want to use the desktop version. Download the desktop version following the instructions here: rstudio.com/products. Now, let’s learn how to use R to solve systems of linear equations! Download this Rmd file. First, we will create vectors and matrices Then we will see how to create an augmented matrix and then apply Gaussian Elimination to obtain is reduced row echelon form. Gaussian elimination is performed by the rref() command. However, this command is not loaded into R by default. So we have have to tell RStudio to use the practical math package, which is known as pracma. So we need to run the following command once at the beginning of our session. require(pracma) 11.2 Building Vectors and Matrices A vector in R is a list of data. The simplest way to create a vector is to use the c() command. The letter ‘c’ is short for ‘combine these values into a vector.’ For example, we can make a vector v for the numbers 1,2,3 as follows: v=c(1,2,3) v ## [1] 1 2 3 Note that we had to ask R to display the value of v. This is because the assignment of v doesn’t echo the value to the console. But can see the value of v in the Environment tab in the upper right panel of RStudio. For example, run this command and then check to see that the value of v gets updated in the environment. v=c(1,2,3,4,5,6) It is interesting to note that c() returns a dimensionless vector. So you can treat a vector c() as either a row or a column when you construct a matrix. For example, suppose that we want to make the matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] We could create this matrix by binding three row vectors: A = rbind(c(1,1,1), c(2,4,8), c(3,9,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 or we could bind three column vectors: A = cbind(c(1,2,3), c(1,4,9), c(1,8,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 11.3 Solving a Linear System Suppose that we want to solve the linear system \\[\\begin{aligned} x + y + z &amp;= 7 \\\\ 2x + 4y + 8z &amp;= 6 \\\\ 3x +9y+27z &amp;=12 \\end{aligned}\\] which has coefficient matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] and target (column) vector \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix}. \\] This is the same matrix A we defined above. Let’s define a vector b and use cbind() to create an augmented matrix which we will name Ab. (We could have just made the full augmented matrix from the start, but using cbind to add a column to a matrix is a skill we will use later in the course!) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 b = c(4,6,12) Ab = cbind(A,b) Ab ## b ## [1,] 1 1 1 4 ## [2,] 2 4 8 6 ## [3,] 3 9 27 12 Now we use the rref() command to apply Gaussian Elimination to produce the reduced row echelon form. (And remember: we had to load this function into R by using the require(pracma) command above.) rref(Ab) ## b ## [1,] 1 0 0 7 ## [2,] 0 1 0 -4 ## [3,] 0 0 1 1 We conclude that this is a consistent system no free variables. The unique solution is \\[\\begin{align} x&amp;=7\\\\ y&amp;=-4\\\\ z&amp;=1 \\end{align}\\] We can verify that our answer works by multiplying \\(A\\) by one of the solutions above. Matrix multiplication uses the funny operation %*%. A %*% c(7,-4,1) ## [,1] ## [1,] 4 ## [2,] 6 ## [3,] 12 #A %*% c(1,2,3,0,0) Which matches our target \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix} \\] just as we had hoped. 11.4 Solving another Linear System Now let’s find the solution set for the linear system \\[ \\begin{array}{rrrrrcr} x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ 2x_1 &amp; +x_2 &amp; +2x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; 4 \\\\ -x_1 &amp; +x_2 &amp; +x_3 &amp; &amp; &amp; = &amp; 10 \\\\ x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ \\end{array} \\] which corresponds to augmented matrix \\[ \\left[ \\begin{array}{rrrrr|r} 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ 2 &amp; +1 &amp; +2 &amp; -1 &amp; -1 &amp; 4 \\\\ -1 &amp; +1 &amp; +1 &amp; &amp; &amp; 10 \\\\ 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ \\end{array} \\right] \\] This time, let’s just construct the augmented matrix direclty. Then we define the coefficient matrix \\(A\\). Here we use cbind to combine the vectors into the columns of a matrix named \\(A\\). You can use rbind if you want to combine the vectors into the rows of a matrix. Ab = cbind(c(1,2,-1,1),c(0,1,1,0),c(-1,2,1,-1),c(-1,1,0,-1),c(-1,5,0,-1),c(-2,10,4,-2)) Ab ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 -1 -1 -1 -2 ## [2,] 2 1 2 1 5 10 ## [3,] -1 1 1 0 0 4 ## [4,] 1 0 -1 -1 -1 -2 And now let’s row reduce to get RREF. rref(Ab) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 1 1 ## [2,] 0 1 0 -1 -1 2 ## [3,] 0 0 1 1 2 3 ## [4,] 0 0 0 0 0 0 So the set of solutions in parametric form is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ 1 \\\\ -2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] and this is a “plane” in \\(\\mathbb{R}^5\\). It is in \\(\\mathbb{R}^5\\) because these vectors have 5 coordinates. It is a plane because it is spanned by two vectors that are not on the same line. 11.5 Appendix: Dimensionless Vectors in R Let’s revisit the vector constructed by cbind. Above we called this a “dimensionless” vector because it can be used as a column vector or a row vector. In general, R will do its best to make sense of a dimensionless vector. In other words, it will promote c() to make an expression valid. For example, let \\(A\\) be an \\(n \\times n\\) matrix, and let \\(b\\) be a vector. The expression \\(Av\\) is only defined when \\(v\\) is a \\(n \\times 1\\) column vector and that \\(wA\\) is only defined when \\(w\\) is a \\(1 \\times n\\) ** row vector**. But let’s look at what happens when we use a dimensionless vector instead. A = cbind(c(1,1,1),c(-1,0,1), c(0,1,-1)) A ## [,1] [,2] [,3] ## [1,] 1 -1 0 ## [2,] 1 0 1 ## [3,] 1 1 -1 b = c(2,5,11) b ## [1] 2 5 11 # A times b A %*% b ## [,1] ## [1,] -3 ## [2,] 13 ## [3,] -4 # b times A b %*% A ## [,1] [,2] [,3] ## [1,] 18 9 -6 Both of these multiplications worked! So R treated b as a column vector for the multiplicationA %*% b. And then R treated b as a row vector for the multiplication b %b% A. So how do you make a true column vector or a true row vector? The answer is to use cbind and rbind! Here are some examples: # dimensionless b = c(1,2,3,4) b ## [1] 1 2 3 4 # column vector b.col = cbind(b) b.col ## b ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 # row vector b.row = rbind(b) b.row ## [,1] [,2] [,3] [,4] ## b 1 2 3 4 "],["linear-dependence.html", "Section 12 Linear Dependence 12.1 Example 1: a 7x9 integer matrix 12.2 A 5 x 6 Numerical Matrix 12.3 Random Matrices", " Section 12 Linear Dependence In this activity, we will explore linear dependence and independence in the context of solving nonhomogeneous \\(A x = b\\) and homogeneous equations \\(A x = 0\\). Download this Rmd file. Remember that we will use the pracma package to get the rref function, so we first load it in: require(&quot;pracma&quot;) 12.1 Example 1: a 7x9 integer matrix Here is a 7 x 9 coeefficient matrix that we will use. These commands define it and echo it back. A = cbind( c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24)) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 And here is a vector b that we hope to use in solving A x = b. b = c(382, 51, -321, -314, -86, -170, -153) b ## [1] 382 51 -321 -314 -86 -170 -153 You can augment A with b, and call it Ab, using cbind: Ab = cbind(A,b) Ab ## b ## [1,] 3 5 3 4 0 -4 5 0 37 382 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 51 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 -321 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 -314 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 -86 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 -170 ## [7,] 1 4 5 -3 8 3 -1 0 -24 -153 And row reduce using rref rref(Ab) ## b ## [1,] 1 0 0 0 5 0 0 0 -2 8 ## [2,] 0 1 0 0 -2 0 0 0 2 10 ## [3,] 0 0 1 0 1 0 0 0 -3 -19 ## [4,] 0 0 0 1 -2 0 0 0 3 21 ## [5,] 0 0 0 0 0 1 0 0 0 6 ## [6,] 0 0 0 0 0 0 1 0 6 61 ## [7,] 0 0 0 0 0 0 0 1 0 8 12.1.1 Solution to the nonhomogeneous equations Ax = b Write out the solution to Ax=b in parametric form using the following formatting. You just need to fill in the correct values of the vectors: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 10 \\\\ -19 \\\\ 21 \\\\ 0 \\\\ 6 \\\\ 61 \\\\ 8 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} -5 \\\\ 2 \\\\ -1 \\\\ 2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} 2 \\\\ -2 \\\\ 3 \\\\ -3 \\\\ 0 \\\\ 0 \\\\ -6 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] Describe this solution space (by fixing up this sentence, which is incorrect right now): the set of solutions to A x= b is a plane in \\(\\mathbb{R}^9\\). 12.1.2 Solution to the nonhomogeneous equations Ax = 0 Now, describe the set of solutions to the homogeneous equations A x = 0. Again, you can just edit this: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} p1 \\\\ p2 \\\\ p3 \\\\ p4 \\\\ p5 \\\\ p6 \\\\ p7 \\\\ p8 \\\\ p9 \\end{bmatrix} + s \\begin{bmatrix} u1 \\\\ u2 \\\\ u3 \\\\ u4 \\\\ u5 \\\\ u6 \\\\ u7 \\\\ u8 \\\\ u9 \\end{bmatrix} + t \\begin{bmatrix} v1 \\\\ v2 \\\\ v3 \\\\ v4 \\\\ v5 \\\\ v6 \\\\ v7 \\\\ v8 \\\\ v9 \\end{bmatrix} \\] And describe, in words, the geometric relationship between the solutions to Ax=b and Ax=0. your answer here 12.1.3 Linearly dependent columns The columns of the matrix A are linearly dependent. You can see that in rref(A). rref(A) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Discuss in your group how you see it. Then write out a dependence relation among the columns by filling in numbers for the weights in this equation \\[ 0 = c_1 \\vec{a}_1 + c_2 \\vec{a}_2 + c_3 \\vec{a}_3 + c_4 \\vec{a}_4 + c_5 \\vec{a}_5 + c_6 \\vec{a}_6 + c_7 \\vec{a}_7 + c_8 \\vec{a}_8 + c_9 \\vec{a}_9. \\] Challenge: give a dependency relation that none of the other groups in the class have. This is telling us that there is some redundancy in the matrix A. Remove columns from A to get a new matrix M whose columns are linearly independent. You can do this by removing the appropriate columns from the code below: M = cbind( # you need to edit this matrix c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24) ) M ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 rref(M) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Your matrix should now be square (7x7) with linearly independent columns. R has a build in solve command, solve, that works for matrices of this form (i.e., square with linearly independent columns). You can try it here. First you need to un-comment-out the solve command. I have it commented out right now, because it does not work with the matrix M (above) until you remove its redundancies. # solve(M,b) Now, you should get a unique solution to the equation M x = b, since M has no free variables, and it should be one of the solutions to the original question A x = b. Which solution is it? That is, which of the many solutions to A x = b are you getting here (forw which values of the paramters?). Compare this with trying to use solve on the original equation A x = b with linearly dependent columns. The solve command in the next bit of code is commented out. Delete the comment command and try executing it. # solve(A,b) 12.2 A 5 x 6 Numerical Matrix So far, all of the matrices we’ve worked with in this class have integer values. This is only so that the calulations are nice to do by hand. All of our theory works over the real numbers. Here we will look at a real matrix with numerical values, something you might find when dealing with real-world data. B = cbind( c(0.717, -0.274, 0.365, 0.482, -0.362), c(0.587, -0.545, 0.5, -0.407, -0.597), c(-0.441, 0.886, 0.784, -0.831, -0.594), c(0.923, -0.466, 0.222, 0.867, 0.493), c(-0.42, -0.745, -0.02, -0.44, 0.209), c(0.621, 0.049, -0.134, -0.844, -0.31) ) B ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.717 0.587 -0.441 0.923 -0.420 0.621 ## [2,] -0.274 -0.545 0.886 -0.466 -0.745 0.049 ## [3,] 0.365 0.500 0.784 0.222 -0.020 -0.134 ## [4,] 0.482 -0.407 -0.831 0.867 -0.440 -0.844 ## [5,] -0.362 -0.597 -0.594 0.493 0.209 -0.310 and here is a vector d in \\(\\mathbb{R}^5\\). d = c(5.886, -4.001, 3.701, -6.621, -2.199) d ## [1] 5.886 -4.001 3.701 -6.621 -2.199 Try answering some of these questions: Are the columns of B linearly independent? Do the columns of B span \\(\\mathbb{R}^5\\)? Give the parametric solution to B x = d. What is the geometric form of this solution (e.g., a plane in \\(\\mathbb{R}^4\\))? Remove redundancies from the columns of B to get a new matrix B2 and use solve to solve the equation B2 x = d. Which of the parametric solutions to you get. 12.3 Random Matrices The following code generates a random 5 x 5 matrix. Every time you enter it, it will give you a new matrix. Use this to try to figure out how likely it is that a random square matrix has linearly dependent columns. R1 = matrix(runif(5*5), nrow = 5, ncol = 5) R1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.08784755 0.24494421 0.4519965 0.3964101 0.2327500 ## [2,] 0.86481169 0.02881095 0.4344042 0.3391761 0.1649525 ## [3,] 0.89307214 0.01800276 0.9969319 0.0672609 0.6339543 ## [4,] 0.32112319 0.41681665 0.9684359 0.1670865 0.3145215 ## [5,] 0.70062471 0.70121538 0.3994277 0.3055481 0.9318550 Try the same using the following code that generates a random 5 x 6 matrix. R2 = matrix(runif(5*6), nrow = 5, ncol = 6) R2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.7165905 0.2424612 0.5341701 0.6974342 0.7428897 0.3351236 ## [2,] 0.8538038 0.3266823 0.9407348 0.6189267 0.1019713 0.4619085 ## [3,] 0.8547894 0.2521487 0.7808665 0.1315257 0.2020849 0.1657695 ## [4,] 0.1031529 0.3928513 0.9778026 0.3846046 0.5680019 0.7870296 ## [5,] 0.9101264 0.9979231 0.7968456 0.9411147 0.9230778 0.9682635 Try the same using the following code that generates a random 5 x 4 matrix. R3 = matrix(runif(5*4), nrow = 5, ncol = 4) R3 ## [,1] [,2] [,3] [,4] ## [1,] 0.04194778 0.3481152 0.065667652 0.6156648 ## [2,] 0.93254707 0.7419040 0.381683986 0.2001260 ## [3,] 0.96473528 0.0322025 0.004214655 0.1994013 ## [4,] 0.85326158 0.2116494 0.199448487 0.3132822 ## [5,] 0.44255018 0.9092221 0.977583583 0.2090047 rref(R3) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## [5,] 0 0 0 0 In each of these cases, how likely is it that the columns of the matrix spans all of \\(\\mathbb{R}^4\\)? "],["matrix-multiplication.html", "Section 13 Matrix Multiplication", " Section 13 Matrix Multiplication Download this Rmd file from GitHub Here we will practice multiplying matrices in R. First, let’s define a few matrices.I’m using a trick here. By putting the assignment in parentheses, it assigns the matrix and displays it.s (A = cbind(c(1,2,3),c(4,5,6),c(1,1,-1))) ## [,1] [,2] [,3] ## [1,] 1 4 1 ## [2,] 2 5 1 ## [3,] 3 6 -1 (B = cbind(c(1,-1,1),c(1,1,1),c(0,2,1))) ## [,1] [,2] [,3] ## [1,] 1 1 0 ## [2,] -1 1 2 ## [3,] 1 1 1 (C = cbind(c(2,1,1),c(1,0,1),c(1,-3,1),c(3,2,1))) ## [,1] [,2] [,3] [,4] ## [1,] 2 1 1 3 ## [2,] 1 0 -3 2 ## [3,] 1 1 1 1 We multiply using the %*% command. As seen here: A %*% B ## [,1] [,2] [,3] ## [1,] -2 6 9 ## [2,] -2 8 11 ## [3,] -4 8 11 Note that B %*% A ## [,1] [,2] [,3] ## [1,] 3 9 2 ## [2,] 7 13 -2 ## [3,] 6 15 1 What do these last two multiplications say about the matrix product AB and BA? This is a very important property (or, perhaps, lack of property) of matrix multiplication. Try multiplying BC and CB. What happens? And why? The transpose of a matrix is computed by t(A). Compute the transpose of the matrices A, B, C and be sure that you all understand what it does. The command diag(n) gives the n x n identity matrix. This is denoted \\(I_n\\). For example, here is \\(I_3\\). diag(3) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Compute \\(I_2\\), \\(I_4\\), and \\(I_5\\) and be sure you all agree on what the identity matrix is. Multiply the matrices A and B by the appropriately-sized identity matrix. Multiply both ways, A I and I A, and agree upon what multiplying by the identity does. Multiply C by an identity matrix I C and C I. You might need a different size on the left and on the right. Our topic for Thursday (tomorrow) is the inverse of a matrix. You compute the inverse of the matrix A with solve(A). Try this solve(B) ## [,1] [,2] [,3] ## [1,] -0.5 -0.5 1 ## [2,] 1.5 0.5 -1 ## [3,] -1.0 0.0 1 B %*% solve(B) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Multiply A by its inverse and look closely at the answer you get. solve(A) ## [,1] [,2] [,3] ## [1,] -1.8333333 1.6666667 -0.1666667 ## [2,] 0.8333333 -0.6666667 0.1666667 ## [3,] -0.5000000 1.0000000 -0.5000000 A %*% solve(A) ## [,1] [,2] [,3] ## [1,] 1.000000e+00 0.000000e+00 0.000000e+00 ## [2,] 3.330669e-16 1.000000e+00 -1.110223e-16 ## [3,] 1.110223e-16 -2.220446e-16 1.000000e+00 Some matrices do not have inverses. Try computing the inverse of the following matrices. We will discuss this tomorrow! (M1 = cbind(c(3,5),c(-2,1))) ## [,1] [,2] ## [1,] 3 -2 ## [2,] 5 1 (M2 = cbind(c(4,3),c(5,4))) ## [,1] [,2] ## [1,] 4 5 ## [2,] 3 4 (M3 = cbind(c(4,2),c(10,5))) ## [,1] [,2] ## [1,] 4 10 ## [2,] 2 5 Enter the matrix A in problem 3.7 in the homework. Then compute the matrix G which is A times its transpose. Discuss its meaning. "],["linear-transformations-of-a-house.html", "Section 14 Linear Transformations of a House 14.1 Rotations 14.2 Expansion and contraction 14.3 Reflection 14.4 Shear Transformations 14.5 Dimension Reduction 14.6 Your Turn", " Section 14 Linear Transformations of a House Download this Rmd file from GitHub Here is a plot of my house. You will need to run this chunk of code each time you re-start R to get the house back in memory. house = cbind(c(0,0), c(0,3/4), c(1/2,3/4), c(1/2,0), c(1,0), c(1,1), c(5/4,1), c(0,2), c(-5/4,1), c(-1,1), c(-1,0), c(0,0)); plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) Here we explore linear transformations on the plane by looking at their effect on my house. We give a series of examples of 2D linear transformations. After each example, it’s your turn to play with variations from the same family of transformations. 14.1 Rotations Suppose that we wish to rotate my house by pi/3 radians. As we’ve seen, a 2D rotation matrix by \\(t\\) radians, counter-clockwise, is given by \\[A=\\displaystyle{ \\begin{bmatrix} \\cos(t) &amp; -\\sin(t) \\\\ \\sin(t) &amp; \\cos(t) \\end{bmatrix}}.\\] Here is the code to display this transformation. Observe that I apply the matrix A to the house, call it house2 and plot both the original house and the new house in the same plot. # define the matrix A. This is the only part you should need to edit. t = pi/3 A = cbind(c(cos(t),sin(t)),c(-sin(t),cos(t))) A # display the matrix A ## [,1] [,2] ## [1,] 0.5000000 -0.8660254 ## [2,] 0.8660254 0.5000000 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) Try changing the angle above to achieve different rotations. How can you rotate it clockwise? 14.2 Expansion and contraction Next, we scale the house by 2 in the \\(x\\)-direction and by 3 in the \\(y\\)-direction. # define the matrix A. This is the only part you should need to edit. A = cbind(c(2,0),c(0,3)) A # display the matrix A ## [,1] [,2] ## [1,] 2 0 ## [2,] 0 3 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) Your turn: try some different scale factors. What if you use negative scale factors. 14.3 Reflection Now we reflect over the line y = x. # define the matrix A. This is the only part you should need to edit. A = cbind(c(0,1),c(1,0)) A # display the matrix A ## [,1] [,2] ## [1,] 0 1 ## [2,] 1 0 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) Your turn: try the reflections (1) over the x-axis; (2) over the y-axis; and (3) through the origin, i.e., sending (x,y) to (-x,-y). 14.4 Shear Transformations A shear transformation is of the form \\[ A=\\displaystyle{ \\begin{bmatrix} a &amp; b \\\\ 0 &amp; c \\end{bmatrix}} \\quad \\mbox{and} \\quad A=\\displaystyle{ \\begin{bmatrix} a &amp; 0 \\\\ b &amp; c \\end{bmatrix}} \\] For example: # define the matrix A. This is the only part you should need to edit. A = cbind(c(1,0),c(1,1)) A # display the matrix A ## [,1] [,2] ## [1,] 1 1 ## [2,] 0 1 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) You try: try to get the house to slant in the other direction. 14.5 Dimension Reduction Here we perform the transformation that sends \\(\\mathsf{e}_1\\) to \\((-1,1/2)\\) and \\(\\mathsf{e}_2\\) to \\((2,-1)\\). Notice that they are the same line and the transformation projects the house onto this line. # define the matrix A. This is the only part you should need to edit. A = cbind(c(-1,1/2),c(2,-1)) A # display the matrix A ## [,1] [,2] ## [1,] -1.0 2 ## [2,] 0.5 -1 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 14.6 Your Turn See if you can do the transformations in problem 3.3. # define the matrix A. This is the only part you should need to edit. A = cbind(c(1,0),c(0,1)) A # display the matrix A ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 #----------------- this code applies the transformation and plots # create a plot that we will add more layers to house2 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # add grid lines polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) "],["quiz-1-review.html", "Section 15 Quiz 1 Review 15.1 Overview 15.2 Practice Problems 15.3 Solutions to Practice Problems", " Section 15 Quiz 1 Review 15.1 Overview Our first quiz covers sections 1.1 - 1.8 and part of 1.9 in Lay’s book. This corresponds to Problem Sets 1 and 2. In 1.8 and 1.9, it will cover the definition of a linear transformation as well as one-to-one and onto, but it will not cover the matrix of a linear transformation. 15.1.1 Vocabulary and Concepts You should understand these concepts and be able to read and use these terms correctly: elementary row operations REF and RREF pivot position linear combination span linear independence homogeneous and nonhomogeneous equations Understand the geometric relationship between the solutions to \\(Ax = 0\\) and \\(Ax=b\\) Understand Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b\\) in \\(\\mathbb{R}^m\\), \\(A x = b\\) has a solution Each \\(b\\) in \\(\\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. The columns of \\(A\\) are linearly independent if and only if \\(Ax=0\\) only has the trivial solution Understand Theorem 8 in Section 1.7: if you have more than \\(n\\) vectors in \\(\\mathbb{R}^n\\) they must be linearly dependent. linear transformation one-to-one and onto Understand Theorem 12 in Section 1.9 which states that \\(T(x)=Ax\\) is onto if and only if the column of \\(A\\) span \\(\\mathbb{R}^m\\) \\(T(x)=Ax\\) is one-to-one if and only if the columns of \\(A\\) are linearly independent. 15.1.2 Skills You should be able to perform these linear algebra tasks. Identify linear systems from nonlinear systems Make the augmented matrix from a set of equations Row reduce a system of equations into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Write the solution set to \\(Ax=b\\) as a parametric vector equation. Convert back and forth between systems of equations, vector equations, and matrix equations. Compute the matrix-vector product \\(Ax\\) Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) Manipulate matrix vector products using: \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) Determine whether a linear transformation is one-to-one and/or onto 15.2 Practice Problems 15.2.1 I have performed some row operations below for you on a matrix \\(A\\). Write out the complete set of solutions to \\(A \\mathsf{x} = {\\bf 0}\\). \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1 \\\\ 1&amp; 2&amp; 1&amp; 1&amp; 0&amp; -2 \\\\ 2&amp; 4&amp; -2&amp; 6&amp; 1&amp; 2 \\\\ 1&amp; 2&amp; 0&amp; 2&amp; -1&amp; -3 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 1&amp; -1&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 1&amp; 2\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ \\end{bmatrix} \\] 15.2.2 I have performed some row operations below for you on a matrix \\(B\\). \\[ B= \\begin{bmatrix} 1&amp; 1&amp; 0 \\\\ 0&amp; 1&amp; 1 \\\\ 2&amp; 1&amp; 2 \\\\ 1&amp; -1&amp; 1 \\\\ 2&amp; 3&amp; 1 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 0&amp; 0 \\\\ 0&amp; 1&amp; 0 \\\\ 0&amp; 0&amp; 1 \\\\ 0&amp;0&amp;0 \\\\ 0&amp;0&amp;0 \\\\ \\end{bmatrix} \\] a. Describe the solutions to the equation \\(B \\mathsf{x} = {\\bf 0}\\). Fill in the boxes: the transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{\\square}\\) to \\(\\mathbb{R}^{\\square}\\). 15.2.3 I want to know if it is possible to write \\(\\mathsf{w}\\) as a linear combination of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) below. Write down, but do not solve, a matrix equation that would solve this problem. Your answer should be of the form \\(A \\mathsf{x} = \\mathsf{b}\\), where I can clearly see what \\(A, \\mathsf{x}\\), and \\(\\mathsf{b}\\) are. I should also be able to tell how many unknowns there are. \\[ \\mathsf{v}_1 = \\left[ \\begin{matrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_2 = \\left[ \\begin{matrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_3 = \\left[ \\begin{matrix} 1 \\\\ 1 \\\\ 0 \\\\ -2 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{w} = \\left[ \\begin{matrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\\\ \\end{matrix}\\right] . \\] 15.2.4 Describe all vectors that are not in the span of the columns of the matrix \\(A\\) below: \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 4 \\\\ -3&amp; -5&amp; -11\\\\ 1&amp; 1&amp; 3 \\\\ \\end{bmatrix} \\] 15.2.5 The matrix below is \\(3 \\times 3\\) but the third column is missing. Add a nonzero third column so that the columns of \\(A\\) are linearly dependent and add a 3rd column so that the columns of \\(A\\) are linearly independent. Briefly describe your strategy. \\[ \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\qquad\\qquad \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\] 15.2.6 In each case below, find the matrix of the linear transformation that is described, if you believe that the matrix exists. Otherwise, demonstrate that the transformation is not linear. The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix}\\right) = \\begin{bmatrix} x_1 + x_2 \\\\ 2 x_1 \\\\ -x_2 \\\\\\end{bmatrix}. \\] The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)= \\begin{bmatrix} x_1 + x_2 + x_3 \\\\ x_1 x_2 \\\\ -x_2 + 2 x_3 \\end{bmatrix}. \\] The transformation \\(L: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) The transformation \\(R: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) 15.2.7 Write the following systems of equations in vector and matrix form. \\[ \\begin{array} {ccccccccccc} 5 x_1 &amp;+&amp; 3 x_2 &amp;+&amp; x_3 &amp;+&amp; 11 x_4 &amp;-&amp; x_5 &amp;=&amp; 10 \\\\ 4 x_1 &amp;+&amp; x_2 &amp;+&amp; 3 x_3 &amp;+&amp; 2 x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 11 \\\\ - x_1 &amp;+&amp; 3 x_2 &amp;-&amp; 2 x_3 &amp;+&amp; x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 12 \\\\ \\end{array} \\] 15.2.8 Let \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) be the vectors in the columns of the matrix \\(A\\) below. \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 3 &amp; 1 \\\\ 2 &amp; 0 &amp; 2 &amp; 3 \\\\ 1 &amp; 1 &amp; 3 &amp; 1 \\\\ -1 &amp; 0 &amp; -1 &amp; 0 \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right] \\] a. Are the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) linear independent or dependent? If they are linearly dependent, please give a dependence relation among them. b. Describe the span of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) inside of \\(\\mathbb{R}^4\\)? 15.2.9 Find a solution to \\(A \\mathsf{x}=0\\) that no one else in the class has. \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 2 &amp; 0 &amp; 4 &amp; 1 &amp; 4 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 1 &amp; 0 &amp; 2 &amp; 1 &amp; 3 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\] 15.3 Solutions to Practice Problems 15.3.1 The parametric vector form of the solution is \\[\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\\\ 1 \\\\0 \\\\ 0 \\end{bmatrix} u \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\-2 \\\\ 1 \\end{bmatrix}\\] 15.3.2 There is one solution: \\(\\mathsf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\). The transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{3}\\) to \\(\\mathbb{R}^{5}\\). 15.3.3 \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 0 &amp; 1 \\\\ 3 &amp; 1 &amp; 0 \\\\ 4 &amp; 0 &amp; -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\end{bmatrix} \\] 15.3.4 We want to find all target vectors \\(\\mathsf{b}\\) such that \\(A \\mathsf{x} = \\mathsf{b}\\) is inconsistent. So we want the augmented matrix \\(\\begin{bmatrix} A \\,| \\, b \\end{bmatrix}\\) to have a pivot in the last column. \\[ \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ -3&amp; -5&amp; -11 &amp; b_2\\\\ 1&amp; 1&amp; 3 &amp; b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; -1&amp; -1 &amp; -b_1+b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; 0&amp; 0 &amp; 2b_1+b_2+b_3 \\\\ \\end{array} \\right] \\] So the set of target vectors that are not in the span of the columns of \\(A\\) are the vectors \\[ \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\qquad \\mbox{where} \\qquad 2b_1 + b_2 + b_3 \\neq 0. \\] 15.3.5 This is on PS2. 15.3.6 This is a linear transformation with \\[A = \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}.\\] This is not a linear transformation because \\[ 2 \\, T \\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 2 \\\\ 2 \\end{bmatrix} \\quad \\mbox{while} \\quad T \\left( \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 6 \\\\ 4 \\\\ 2 \\end{bmatrix}. \\] \\(A= \\begin{bmatrix} 1/2 &amp; 1/2 \\\\ 1/2 &amp; 1/2 \\end{bmatrix}\\) \\(A= \\begin{bmatrix} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{bmatrix}\\) 15.3.7 Vector Form: \\[ x_1 \\begin{bmatrix} 5 \\\\ 4 \\\\ -1 \\end{bmatrix} + x_2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 3 \\end{bmatrix} + x_3 \\begin{bmatrix} 1 \\\\ 3 \\\\ -2 \\end{bmatrix} + x_4 \\begin{bmatrix} 11 \\\\ 2 \\\\ 1 \\end{bmatrix} + x_5 \\begin{bmatrix} -1 \\\\ 6 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] Matrix Form: \\[ \\begin{bmatrix} 5 &amp; 3 &amp; 1 &amp; 11 &amp; -1 \\\\ 4 &amp; 1 &amp; 3 &amp; 2 &amp; 6 \\\\ -1 &amp; 3 &amp; -2&amp; 1 &amp; 6 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] 15.3.8 \\(-\\mathsf{v}_1 - 2\\mathsf{v}_2 + \\mathsf{v}_3 + 0 \\mathsf{v}_4 = 0\\). \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4)\\) looks like a copy of \\(\\mathbb{R}^3\\) sitting inside \\(\\mathbb{R}^4\\). In other words, is 3-dimensional subset of \\(\\mathbb{R}^4\\). 15.3.9 The general solution is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] My solution is \\[77,083,679 \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - 72,159,215 \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] "]]
