[["index.html", "MATH 236: Linear Algebra Preface", " MATH 236: Linear Algebra Preface This is the class handbook for Math 236 Linear Algebra at Macalester College. The content here was made by Andrew Beveridge and Tom Halverson and other faculty in the Department of Mathematics, Statistics and Computer Science at Macalester College. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["problem-set-1.html", "Section 1 Problem Set 1 1.1 Characterize the Solution Set 1.2 Find the General Solution 1.3 Elementary row operations are reversible 1.4 Designer Parabolas 1.5 Traffic Flow", " Section 1 Problem Set 1 Due: Tuesday May 25 by midnight CST. Upload your solutions to problems 1–4 by writing them out by hand, scanning them to pdf using a scanning software such as AdobeScan, assembling them into a single PDF, and uploading it to Moodle. Problem 1.5 is to be done using RStudio. To solve it, create an Rmarkdown file, knit it to .html, and upload the .html on Moodle along with the PDF for questions 1-4. There will be time to work on this problem in class on May 23-24. 1.1 Characterize the Solution Set The following augmented matrices are in row echelon form. Decide whether the set of solutions is a point, line, plane, or the empty set in 3-space. Briefly justify your answer. \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 4 \\\\ 0 &amp; 1 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; -1 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 0 &amp; 1 &amp; 0 &amp; 6 \\\\ 0 &amp; 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.2 Find the General Solution Each of the following matrices is the reduced row echelon form of the augmented matrix of a system of linear equations. Give the general solution to each system. \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 3 &amp; 0 &amp; -2 &amp; 5\\\\ 0 &amp; 0 &amp; 1 &amp; 4 &amp; -2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; 4 &amp; 0 &amp; 3 &amp; 6\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -2&amp; -8 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 3 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 4 &amp; 0 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7 &amp; 6\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.3 Elementary row operations are reversible In each case below, an elementary row operation turns the matrix \\(A\\) into the matrix \\(B\\). For each of them, Describe the row operation that turns \\(A\\) into \\(B\\), and Describe the row operation that turns \\(B\\) into \\(A\\). Give your answers in the form: “scale \\(R_2\\) by 3” or “swap \\(R_1\\) and \\(R_4\\)” or “replace \\(R_3\\) with \\(R_3 + \\frac{1}{5} R_1\\).” \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 0 &amp; 7 &amp; 0 &amp; -4 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 4 &amp; 1 &amp; -2 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] 1.4 Designer Parabolas In each part below, set up and solve a linear system of equations to find all possible parabolas of the form \\[ f(x) = a + b x + c x^2 \\] that satisfy the given conditions. For full credit, please solve these by hand, doing all row reductions that bring the system of equations to Reduced Row Echelon Form. On future assignments, you can solve problems like this using either RStudio or WolframAlpha. You are welcome (and, in fact, encouraged) to check your answers using software. \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(2,4)\\). \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(3,10)\\). \\(f(x)\\) passes through the two points: \\((1,3)\\) and \\((3,11)\\). 1.5 Traffic Flow Below you find a section of one-way streets in downtown St Paul, where the arrows indicate traffic direction. The traffic control center has installed electronic sensors that count the numbers of vehicles passing through the 6 streets that lead into and out of this area. Assume that the total flow that enters each intersection equals the the total flow that leaves each intersection (we will ignore parking and staying). Create a system of linear equations to find the possible flow values for the inner streets \\(x_1, x_2, x_3, x_4\\). Using RStudio, enter the augmented matrix of this system, and solve it using the rref command. Type out the general solution to this system of equations. Your answer to part b should be an infinite solution set. Give two distinct solutions that are realistic in terms of traffic flow. Is it possible to close down the street labeled by \\(x_2\\) for road construction? That is, is it possible to have \\(x_2 = 0\\) and to meet the other conditions? "],["problem-set-2.html", "Section 2 Problem Set 2 2.1 Parametric Vector Form 2.2 RREF for a linear system 2.3 RREF for a set of vectors 2.4 A square matrix 2.5 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) 2.6 A Balanced Diet 2.7 Missing Column 2.8 Linear System", " Section 2 Problem Set 2 Due: Tuesday June 1 by 11:59PM CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. In problems where you use RStudio for row reduction and are not asked to turn in an R markdown file, you can write something like this: The Problem Set covers sections 1.3, 1.4, 1.5, and 1.7. 2.1 Parametric Vector Form Here is the augmented matrix for a system of linear equations \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\), and its RREF. Give the general solution to this system in parametric vector form and describe the geometry of the solution. For example, you answer should be something like: “it is a plane in \\(\\mathbb{R}^3\\)” or “it is a line in \\(\\mathbb{R}^7\\)” or “it is a point in \\(\\mathbb{R}^4\\).” \\[ \\left[ \\begin{array}{ccccc|c} 1 &amp; 1 &amp; -1 &amp; -1 &amp; 2 &amp; 1 \\\\ 1 &amp; 0 &amp; -2 &amp; 1 &amp; 1 &amp; 3 \\\\ -2 &amp; 1 &amp; 5 &amp; 1 &amp; -6 &amp; 2 \\\\ -3 &amp; 0 &amp; 6 &amp; 2 &amp; -8 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; -3 &amp; 6 \\\\ 1 &amp; 0 &amp; -2 &amp; -1 &amp; 3 &amp; -1 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; -2 &amp; 0 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] 2.2 RREF for a linear system Here is the reduced row echelon form of a matrix \\(\\mathsf{A}\\) (you are not given the matrix \\(\\mathsf{A}\\)). \\[ \\mathsf{A} \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 0 &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; -5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] Give the general solution to this system in parametric vector form and describe the geometry of the solution. For example, you answer should be something like: “it is a plane in \\(\\mathbb{R}^3\\)” or “it is a line in \\(\\mathbb{R}^7\\)” or “it is a point in \\(\\mathbb{R}^4\\).” Suppose that we also know that \\(\\mathsf{A}\\begin{bmatrix} 4 \\\\ 1 \\\\ -3 \\\\ 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 22 \\\\ -13 \\\\ 7 \\\\ \\end{bmatrix}\\). Then give the general solution to \\(\\mathsf{A} \\mathsf{x}= \\begin{bmatrix} 22 \\\\ -13\\\\ 7 \\\\ \\end{bmatrix}\\) in parametric form. 2.3 RREF for a set of vectors Suppose that we have five vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4,\\mathsf{v}_5\\) in \\(\\mathbb{R}^4\\) and that the matrix \\(A\\) containing those vectors row reduces as follows \\[ A = \\left[ \\begin{array}{ccc} \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\\\ \\mathsf{v}_1 &amp; \\mathsf{v}_2 &amp; \\mathsf{v}_3 &amp;\\mathsf{v}_4 &amp;\\mathsf{v}_5 \\\\ \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\end{array} \\right] \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; -3 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 4 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}. \\] Do the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\) span \\(\\mathbb{R}^4\\)? Justify your answer. Is the vector \\(\\mathsf{v}_3\\) in \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2)\\)? Justify your answer. Pick any \\(\\mathsf{b}\\) in \\(\\mathrm{span}(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5)\\). Is there always a unique way to write \\(\\mathsf{b}\\) as a linear combination of \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\)? Justify your answer. 2.4 A square matrix Suppose that \\(A\\) is a \\(5\\times 5\\) matrix and \\(\\mathsf{b}\\) is a vector in \\(\\mathbb{R}^5\\) with the property that \\(A\\mathsf{x}=\\mathsf{b}\\) has a unique solution. Explain why the columns of \\(A\\) must span \\(\\mathbb{R}^5\\). Use the reduced row echelon form of \\(A\\) in your explanation. 2.5 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) Suppose that \\(\\mathsf{x}_1\\) and \\(\\mathsf{x}_2\\) are solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) (where \\(\\mathsf{b} \\not= \\mathsf{0}\\)). Decide if any of the following are also solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\). \\(\\mathsf{x}_1+ \\mathsf{x}_2\\) \\(\\mathsf{x}_1 - \\mathsf{x}_2\\) \\(\\frac{1}{2} ( \\mathsf{x}_1 + \\mathsf{x}_2)\\) \\(\\frac{5}{2} \\mathsf{x}_1 - \\frac{3}{2} \\mathsf{x}_2\\). Under what conditions on \\(c\\) and \\(d\\) is \\(\\mathsf{x} = c \\mathsf{x}_1 + d \\mathsf{x}_2\\) a solution to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\)? Justify your answer. Let \\(\\mathsf{u}\\) be the vector that points to \\(1/3\\) of the way from the tip of \\(\\mathsf{v}\\) to the tip of \\(\\mathsf{w}\\) as depicted below. Write \\(\\mathsf{u}\\) as a linear combination of \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) (hint: think about \\(\\mathsf{w} - \\mathsf{v}\\)) If \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) are solutions to \\(A x = \\mathsf{b}\\) then show that \\(\\mathsf{u}\\) is also a solution to \\(A \\mathsf{x} = \\mathsf{b}\\). 2.6 A Balanced Diet An athlete wants to consume a daily diet of 200 grams of carbohydrates, 60 grams of fats and 160 grams of proteins. Here are some of their favorite foods. Table 2.1: Food Carb/Fat/Protein (grams) food carbs fats proteins almonds 3 8 5 avocado 15 31 4 beans 20 1 8 bread 12 1 2 cheese 1 5 3 chicken 0 13 50 egg 1 5 6 milk 12 8 8 zucchini 6 0 2 Answer the following questions, using RStudio for your calculations. Each response must use two or more of the following terms: linear combination, span, linearly dependent, linearly independent. Explain why they cannot achieve their daily goal by eating only almonds, milk and zucchini. Explain why they cannot achieve their daily goal by eating only almonds, beans and cheese. Find a valid one-day diet consisting of almonds, chicken, and zucchini. 2.7 Missing Column The matrices below are supposed to be \\(3 \\times 3\\) but in each case the third column was accdentally deleted. In each case, add a third column, that has no 0s in it and is different from either the first or second column, so that the columns of \\(A\\) are linearly dependent and so that the columns of \\(B\\) are linearly independent. Briefly describe your strategy. \\[ A=\\left[ \\begin{matrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{matrix}\\right] \\qquad\\qquad B=\\left[ \\begin{matrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{matrix}\\right] \\] 2.8 Linear System Use R to solve this problem. Do your computations in an R markdown file. Knit the file to HTML and include it with your homework. Here you can download a template for doing this problem (including the matrix typed out for you!). \\[ A =\\left[ \\begin{array}{cccccc} 12 &amp; 10 &amp; -6 &amp; 8 &amp; 4 &amp; -18 \\\\ -7 &amp; -6 &amp; 4 &amp; -5 &amp; -7 &amp; 16 \\\\ 9 &amp; 9 &amp; -9 &amp; 9 &amp; 9 &amp; -27 \\\\ -4 &amp; -3 &amp; -1 &amp; 0 &amp; -8 &amp; 9 \\\\ 8 &amp; 7 &amp; -5 &amp; 6 &amp; 1 &amp; -12 \\\\ \\end{array} \\right] \\quad b = \\begin{bmatrix} 14 \\\\ -12 \\\\ 9\\\\ -15 \\\\6 \\end{bmatrix} \\] Show that the columns of \\(A\\) are linearly dependent by finding two different dependency relations among them. You can write your answer in a form like \\(5 a1+ 4 a2 + 3 a3 + 2 a4 + a5 = 0\\), where \\(a1, a2,\\) etc are the columns of \\(A\\). Augment \\(A\\) with \\(b\\) and show that \\(A x = b\\) is consistent and has infinitely many solutions. Remove the free-variable columns from \\(A\\) to get a new, smaller matrix \\(A&#39;\\). Show that \\(A&#39; x = b\\) has a unique solution and say what that solution is. "],["important-definitions.html", "Section 3 Important Definitions 3.1 Vector Spaces 3.2 Matrices 3.3 Orthogonality 3.4 Spectral Decompostion", " Section 3 Important Definitions 3.1 Vector Spaces span A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span a vector space \\(V\\) if for every \\(\\mathsf{v} \\in V\\) there exist a set of scalars (weights) \\(c_1, c_2, \\ldots, c_n \\in \\mathbb{R}\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, \\ldots, c_n]^{\\top}\\) is a solution to \\(A x = \\mathsf{v}\\). linear independence A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\ldots, \\mathsf{v}_n\\) are linearly independent if the only way to write \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] is with \\(c_1 = c_2 = \\cdots = c_n = 0\\). Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(A x = \\mathsf{0}\\) has only the trivial solution. linear dependence Conversely, a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly dependent if there exist scalars \\(c_1, c_2,\\ldots, c_n \\in \\mathbb{R}\\) that are not all equal to 0 such that \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] This is called a dependence relation among the vectors. Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, c_2, \\ldots, c_n]^{\\top}\\) is a nontrivial solution to \\(A \\mathsf{x} = \\mathsf{0}\\). linear transformation A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation when: \\(T(\\mathsf{u} + \\mathsf{v}) = T(\\mathsf{u}) + T(\\mathsf{v})\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\) (preserves addition) \\(T(c \\mathsf{u} ) = c T(\\mathsf{u})\\) for all \\(\\mathsf{u} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\) (preserves scalar multiplication). It follows from these that also \\(T(\\mathsf{0}) = \\mathsf{0}\\). one-to-one A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a one-to-one when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at most one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). onto A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a onto when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at least one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). subspace A subset \\(S \\subseteq \\mathbb{R}^n\\) is a subspace when: \\(\\mathsf{u} + \\mathsf{v} \\in S\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in S\\) (closed under addition) \\(c \\mathsf{u} \\in S\\) for all \\(\\mathsf{u}\\in S\\) and \\(c \\in \\mathbb{R}\\) (closed under scalar multiplication) It follows from these that also \\(\\mathsf{0} \\in S\\). basis A basis of a vector space (or subspace) \\(V\\) is a set of vectors \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) in \\(V\\) such that \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span \\(V\\) \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly independent Equivalently, one can say that \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) is a basis of \\(V\\) if for every vector \\(\\mathsf{v} \\in V\\) there is a unique set of scalars \\(c_1, \\ldots, c_n\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] (The fact that there is a set of vectors comes from the span; the fact that they are unique comes from linear independence). dimension The dimension of a subspace \\(W\\) is the number of vectors in any basis of \\(W\\). This is also the fewest number of vectors required to span the subspace. 3.2 Matrices invertible The square \\(n \\times n\\) matrix \\(A\\) is invertible when there exists an \\(n \\times n\\) matrix \\(A^{-1}\\) such that \\(A A^{-1} = I = A^{-1} A\\). The Invertible Matrix Theorem collects over two dozen equivalent conditions, each of which guarantees that \\(A\\) is invertible. null space The null space \\(\\mbox{Nul}(A) \\subset \\mathbb{R}^n\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of solutions to the homogeneous equation \\(A \\mathsf{x} = \\mathbf{0}\\)&gt; We also write this as \\[ \\mbox{Nul}(A) = \\{ \\mathsf{x} \\in \\mathbb{R}^n : A \\mathsf{x} = \\mathbf{0} \\} \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the kernel of \\(T\\) is the null space of matrix \\(A\\). column space The column space \\(\\mbox{Col}(A) \\subset \\mathbb{R}^m\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of all linear combinations of the columns of \\(A\\). For \\(A = \\begin{bmatrix} \\mathsf{a}_1 &amp; \\mathsf{a}_2 &amp; \\cdots &amp; \\mathsf{a}_n \\end{bmatrix}\\), we have \\[ \\mbox{Col}(A) = \\mbox{span} ( \\mathsf{a}_1, \\mathsf{a}_2, \\ldots , \\mathsf{a}_n ) \\] We also write this as \\[ \\mbox{Col}(A) = \\{ \\mathsf{b} \\in \\mathbb{R}^m : \\mathsf{b} = A \\mathsf{x} \\mbox{ for some } \\mathsf{x} \\in \\mathbb{R}^n \\}. \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the range (also called the image) of \\(T\\) is the column space of matrix \\(A\\). rank The rank of the \\(m \\times n\\) matrix \\(A\\) is the dimension of the column space of \\(A\\). This is also the number of pivot columns of the matrix. eigenvalue and eigenvector For a square \\(n \\times n\\) matrix \\(A\\), the scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue for \\(A\\) when there exists a nonzero vector \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(A \\mathsf{x} = \\lambda \\mathsf{x}\\). The nonzero vector \\(\\mathsf{x}\\) is the eigenvector for eigenvalue \\(\\lambda\\). The collection of all of these eigenvalues and eigenvectors is called the eigensystem of A. diagonalization A square \\(n \\times n\\) matrix is diagonalizable when \\(A = P D P^{-1}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an invertible matrix. In this case, the eigenvalues of \\(A\\) are the diagonal entries of \\(D\\) and their corresponding eigenvectors are the columns of \\(P\\). dominant eigenvalue The eigenvalue \\(\\lambda\\) of the square matrix \\(A\\) is the dominant eigenvalue when \\(| \\lambda | &gt; | \\mu |\\) where \\(\\mu\\) is any other eigenvalue of \\(A\\). The dominant eigenvalue determines the long-term behavior of \\(A^t\\) as \\(t \\rightarrow \\infty\\). 3.3 Orthogonality length The length of a vector \\(\\mathsf{v}\\) is \\[ \\| \\mathsf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}. \\] distance and angle The distance between vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) is \\[ \\mbox{dist}(\\mathsf{u},\\mathsf{v}) = \\| \\mathsf{u} - \\mathsf{v} \\|. \\] The angle \\(\\theta\\) between these vectors is determined by \\[ \\cos \\theta = \\frac{\\mathsf{u} \\cdot \\mathsf{v}}{ \\| \\mathsf{u} \\| \\, \\| \\mathsf{v} \\|}. \\] orthogonal The vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are orthogonal when \\(\\mathsf{u} \\cdot \\mathsf{v} = 0\\). This means that either one of them is the zero vector, or they are perpendicular to one another. orthogonal complement If \\(W \\subset \\mathbb{R}^n\\) is a subspace, then its orthogonal complement \\(W^{\\perp}\\) is the set of all vectors in \\(\\mathsf{R}^n\\) that are orthogonal to \\(W\\). We also write \\[ W^{\\perp} = \\{ \\mathsf{v} \\in \\mathbb{R}^n : \\mathsf{v} \\cdot \\mathsf{w} \\mbox{ for all } \\mathsf{w} \\in W \\}. \\] orthonormal set A collection of vectors \\(\\mathsf{u}_1, \\mathsf{u}_2, \\ldots, \\mathsf{u}_k\\) are an orthonormal set when every vector has length 1 and the vectors are pairwise orthogonal. orthogonal matrix orthogonal matrix A square \\(n \\times n\\) matrix \\(P\\) is an orthogonal matrix when its columns are an orthonormal set. As a result, we have \\(P^{-1} = P^{\\top}\\). projection and residual The orthogonal projection of vector \\(\\mathsf{y}\\) into a subspace \\(W\\) is the unique vector \\(\\hat{\\mathsf{y}} \\in W\\) such that \\(\\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}} \\in W^{\\perp}\\). The vector \\(\\mathsf{z}\\) is called the residual vector for the projection. 3.4 Spectral Decompostion orthogonal diagonalization Every symmetric \\(n \\times n\\) matrix is orthogonally diagonalizable, meaning that we have \\(A = P D P^{\\top}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix. The diagonal entries of \\(D\\) are the eigenvalues of \\(A\\) and the columns of \\(P\\) are the corresponding orthonormal eigenvectors. Furthermore, the eigenvalues of \\(A\\) are nonnegative. spectral decomposition A symmetric matrix \\(A\\) can be written as a linear combination of rank 1 matrices derived from the orthonormal eigensystem of \\(A\\). In particular, we have \\[ A = \\lambda_1 \\mathsf{u}_1 \\mathsf{u}_1^{\\top} + \\lambda_2 \\mathsf{u}_2 \\mathsf{u}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{u}_n \\mathsf{u}_n^{\\top}. \\] This linear combination of rank 1 vectors is called the spectral decomposition of \\(A\\). singular value decomposition (SVD) Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be factored into its singular value decomposition \\(U \\Sigma V^{\\top}\\) where \\(U\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\Sigma\\) is a matrix whose nonzero entries are the positive numbers \\(\\sigma_1, \\ldots , \\sigma_r\\), which appear in decreasing order on the diagonal, and \\(V\\) is an \\(n \\times n\\) orthogonal matrix. The nonzero entries of \\(\\Sigma\\) are called the singular values of \\(A\\). The columns of \\(U\\) are the left singular vectors and the rows of \\(V^{\\top}\\) are the right singular vectors. SVD spectral decomposition Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be written as a linear combination of rank 1 matrices derived from the singular value decomposition of \\(A\\). In particular, we have \\[ A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\sigma_r \\mathsf{u}_r \\mathsf{v}_r^{\\top}. \\] This linear combination of rank 1 vectors is called the (SVD) spectral decomposition of \\(A\\). "],["week-1-learning-goals.html", "Section 4 Week 1 Learning Goals 4.1 Solving Linear Equations 4.2 RStudio 4.3 Vocabulary 4.4 Conceptual Thinking", " Section 4 Week 1 Learning Goals Here are the knowledge and skills you should master by the end of this first, shorter week. 4.1 Solving Linear Equations I should be able to do the following tasks: Identify linear systems from nonlinear systems Create a linear system to solve a variety of applied scenarios Convert between a linear system and an augmented matrix Row reduce an augmented matrix into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Use REF to determine whether a linear system is consistent or inconsistent Use REF to determine whether a consistent system has a unique solution or an infinite number of solutions Use RREF to find explicit equations for the solution set of a consistent system 4.2 RStudio I should be able to do the following tasks: Log in to Macalester’s RStudio server Upload R Markdown files to RStudio Knit R Markdown to produce HTML Use RStudio to create vectors and matrices Use the rref command from pracma to solve a linear system 4.3 Vocabulary I should know and be able to use and explain the following terms: elementary row operation (and be able to state them) augmented matrix REF and RREF pivot position basic variable (pivot variable) free variable consistent system and inconsistent system 4.4 Conceptual Thinking I should understand and be able to perform the following conceptual tasks: Model 2-dimensional linear systems as the intersections of lines Model 3-dimensional linear systems as the intersections of planes "],["week-2-learning-goals.html", "Section 5 Week 2 Learning Goals 5.1 Solution Sets, Span and Linear Independence 5.2 Vocabulary 5.3 Conceptual Thinking", " Section 5 Week 2 Learning Goals Here are the knowledge and skills you should master by the end the second week. 5.1 Solution Sets, Span and Linear Independence I should be able to do the following tasks: Go back and forth between (i) systems of equations, (ii) vector equations, and (iii) the matrix equation \\(Ax = b\\). Compute and understand the matrix-vector product \\(A x\\) both as a linear combination of the columns of A and as the dot product of \\(x\\) with the rows of \\(A\\). Write the solution set to \\(Ax=b\\) as a parametric vector equation. Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) 5.2 Vocabulary I should know and be able to use and explain the following terms or properties. \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) homogeneous and nonhomogeneous equations parametric vector equations linear independence and linear dependence 5.3 Conceptual Thinking I should understand and be able to explain the following concepts: Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b \\in \\mathbb{R}^m\\), the system \\(A x = b\\) has at least one solution Each \\(b \\in \\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. Understand the relation between homogeneous solutions and nonhomogeneous solutions. Linear independence Span More than \\(n\\) vectors in \\(\\mathbb{R}^n\\) must be linearly dependent. "],["class-examples.html", "Section 6 Class Examples 6.1 Day 5: Tuesday May 25", " Section 6 Class Examples 6.1 Day 5: Tuesday May 25 Give the parametric solution to \\(A x = b\\) if \\[ A=\\left[ \\begin{array}{ccccc} 1 &amp; 1 &amp; 1 &amp; -1 \\\\ 1 &amp; 0 &amp; 3 &amp; -1 \\\\ -1 &amp; 1 &amp; -5 &amp; 1 \\\\ 1 &amp; 2 &amp; -1 &amp; 1 \\\\ \\end{array} \\right] \\qquad \\hbox{and}\\qquad b = \\begin{bmatrix} 2 \\\\ 4 \\\\ -6 \\\\ 4 \\end{bmatrix}. \\hskip.5in \\] A = cbind(c(1,1,-1,1),c(1,0,1,2),c(1,3,-5,-1),c(-1,-1,1,1)) b = c(2,4,-6,4) Ab = cbind(A,b) A ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 -1 ## [2,] 1 0 3 -1 ## [3,] -1 1 -5 1 ## [4,] 1 2 -1 1 Ab ## b ## [1,] 1 1 1 -1 2 ## [2,] 1 0 3 -1 4 ## [3,] -1 1 -5 1 -6 ## [4,] 1 2 -1 1 4 rref(Ab) ## b ## [1,] 1 0 3 0 6 ## [2,] 0 1 -2 0 -2 ## [3,] 0 0 0 1 2 ## [4,] 0 0 0 0 0 Give the solution this \\(Ax=b\\) problem in parametric form: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ -2 \\\\ 0 \\\\ 2 \\end{bmatrix} + s \\begin{bmatrix} -3 \\\\ 2 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Let’s make two different solutions. One with s = 0 and one with s = 1. They are x1 = c(6,-2,0,2) x2 = c(3,0,1,2) And we check to see that they are solutions. Observe that we use the unfortunate notation %*% to multply a matrix times a vector, but also that we get b in both cases: A %*% x1 ## [,1] ## [1,] 2 ## [2,] 4 ## [3,] -6 ## [4,] 4 A %*% x2 ## [,1] ## [1,] 2 ## [2,] 4 ## [3,] -6 ## [4,] 4 Notice that we get b for both. Now we look at the difference. It is a solution to the homogeneous equations A x = 0 as we see here: xh = x1 - x2 A %*% xh ## [,1] ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 Are any of these vectors solutions to Ax=b? A %*% (x1 + x2) ## [,1] ## [1,] 4 ## [2,] 8 ## [3,] -12 ## [4,] 8 No! (this is 2b) A %*% (1/2*x1 + 1/2* x2) ## [,1] ## [1,] 2 ## [2,] 4 ## [3,] -6 ## [4,] 4 Yes! This is b A %*% (x1 + 2021 * xh) ## [,1] ## [1,] 2 ## [2,] 4 ## [3,] -6 ## [4,] 4 Yes! This is b. "],["linear-systems-in-r.html", "Section 7 Linear Systems in R 7.1 Getting started with R 7.2 Building Vectors and Matrices 7.3 Solving a Linear System 7.4 Solving another Linear System 7.5 Appendix: Dimensionless Vectors in R", " Section 7 Linear Systems in R 7.1 Getting started with R To use RStudio, you have two choices: Use the cloud version by logging in to Rstudio.macalester.edu. This is the easiest way to use RStudio and works great for our course. You can also download the free desktop version of RStudio. If you plan to go on to take more MSCS classes, especially in statistics and data science, you may want to use the desktop version. Download the desktop version following the instructions here: rstudio.com/products. Now, let’s learn how to use R to solve systems of linear equations! Download this Rmd file. First, we will create vectors and matrices Then we will see how to create an augmented matrix and then apply Gaussian Elimination to obtain is reduced row echelon form. Gaussian elimination is performed by the rref() command. However, this command is not loaded into R by default. So we have have to tell RStudio to use the practical math package, which is known as pracma. So we need to run the following command once at the beginning of our session. require(pracma) 7.2 Building Vectors and Matrices A vector in R is a list of data. The simplest way to create a vector is to use the c() command. The letter ‘c’ is short for ‘combine these values into a vector.’ For example, we can make a vector v for the numbers 1,2,3 as follows: v=c(1,2,3) v ## [1] 1 2 3 Note that we had to ask R to display the value of v. This is because the assignment of v doesn’t echo the value to the console. But can see the value of v in the Environment tab in the upper right panel of RStudio. For example, run this command and then check to see that the value of v gets updated in the environment. v=c(1,2,3,4,5,6) It is interesting to note that c() returns a dimensionless vector. So you can treat a vector c() as either a row or a column when you construct a matrix. For example, suppose that we want to make the matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] We could create this matrix by binding three row vectors: A = rbind(c(1,1,1), c(2,4,8), c(3,9,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 or we could bind three column vectors: A = cbind(c(1,2,3), c(1,4,9), c(1,8,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 7.3 Solving a Linear System Suppose that we want to solve the linear system \\[\\begin{aligned} x + y + z &amp;= 7 \\\\ 2x + 4y + 8z &amp;= 6 \\\\ 3x +9y+27z &amp;=12 \\end{aligned}\\] which has coefficient matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] and target (column) vector \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix}. \\] This is the same matrix A we defined above. Let’s define a vector b and use cbind() to create an augmented matrix which we will name Ab. (We could have just made the full augmented matrix from the start, but using cbind to add a column to a matrix is a skill we will use later in the course!) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 b = c(4,6,12) Ab = cbind(A,b) Ab ## b ## [1,] 1 1 1 4 ## [2,] 2 4 8 6 ## [3,] 3 9 27 12 Now we use the rref() command to apply Gaussian Elimination to produce the reduced row echelon form. (And remember: we had to load this function into R by using the require(pracma) command above.) rref(Ab) ## b ## [1,] 1 0 0 7 ## [2,] 0 1 0 -4 ## [3,] 0 0 1 1 We conclude that this is a consistent system no free variables. The unique solution is \\[\\begin{align} x&amp;=7\\\\ y&amp;=-4\\\\ z&amp;=1 \\end{align}\\] We can verify that our answer works by multiplying \\(A\\) by one of the solutions above. Matrix multiplication uses the funny operation %*%. A %*% c(7,-4,1) ## [,1] ## [1,] 4 ## [2,] 6 ## [3,] 12 #A %*% c(1,2,3,0,0) Which matches our target \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix} \\] just as we had hoped. 7.4 Solving another Linear System Now let’s find the solution set for the linear system \\[ \\begin{array}{rrrrrcr} x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ 2x_1 &amp; +x_2 &amp; +2x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; 4 \\\\ -x_1 &amp; +x_2 &amp; +x_3 &amp; &amp; &amp; = &amp; 10 \\\\ x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ \\end{array} \\] which corresponds to augmented matrix \\[ \\left[ \\begin{array}{rrrrr|r} 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ 2 &amp; +1 &amp; +2 &amp; -1 &amp; -1 &amp; 4 \\\\ -1 &amp; +1 &amp; +1 &amp; &amp; &amp; 10 \\\\ 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ \\end{array} \\right] \\] This time, let’s just construct the augmented matrix direclty. Then we define the coefficient matrix \\(A\\). Here we use cbind to combine the vectors into the columns of a matrix named \\(A\\). You can use rbind if you want to combine the vectors into the rows of a matrix. Ab = cbind(c(1,2,-1,1),c(0,1,1,0),c(-1,2,1,-1),c(-1,1,0,-1),c(-1,5,0,-1),c(-2,10,4,-2)) Ab ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 -1 -1 -1 -2 ## [2,] 2 1 2 1 5 10 ## [3,] -1 1 1 0 0 4 ## [4,] 1 0 -1 -1 -1 -2 And now let’s row reduce to get RREF. rref(Ab) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 1 1 ## [2,] 0 1 0 -1 -1 2 ## [3,] 0 0 1 1 2 3 ## [4,] 0 0 0 0 0 0 So the set of solutions in parametric form is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ 1 \\\\ -2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] and this is a “plane” in \\(\\mathbb{R}^5\\). It is in \\(\\mathbb{R}^5\\) because these vectors have 5 coordinates. It is a plane because it is spanned by two vectors that are not on the same line. 7.5 Appendix: Dimensionless Vectors in R Let’s revisit the vector constructed by cbind. Above we called this a “dimensionless” vector because it can be used as a column vector or a row vector. In general, R will do its best to make sense of a dimensionless vector. In other words, it will promote c() to make an expression valid. For example, let \\(A\\) be an \\(n \\times n\\) matrix, and let \\(b\\) be a vector. The expression \\(Av\\) is only defined when \\(v\\) is a \\(n \\times 1\\) column vector and that \\(wA\\) is only defined when \\(w\\) is a \\(1 \\times n\\) ** row vector**. But let’s look at what happens when we use a dimensionless vector instead. A = cbind(c(1,1,1),c(-1,0,1), c(0,1,-1)) A ## [,1] [,2] [,3] ## [1,] 1 -1 0 ## [2,] 1 0 1 ## [3,] 1 1 -1 b = c(2,5,11) b ## [1] 2 5 11 # A times b A %*% b ## [,1] ## [1,] -3 ## [2,] 13 ## [3,] -4 # b times A b %*% A ## [,1] [,2] [,3] ## [1,] 18 9 -6 Both of these multiplications worked! So R treated b as a column vector for the multiplicationA %*% b. And then R treated b as a row vector for the multiplication b %b% A. So how do you make a true column vector or a true row vector? The answer is to use cbind and rbind! Here are some examples: # dimensionless b = c(1,2,3,4) b ## [1] 1 2 3 4 # column vector b.col = cbind(b) b.col ## b ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 # row vector b.row = rbind(b) b.row ## [,1] [,2] [,3] [,4] ## b 1 2 3 4 "],["linear-dependence.html", "Section 8 Linear Dependence 8.1 Example 1: a 7x9 integer matrix 8.2 A 5 x 6 Numerical Matrix 8.3 Random Matrices", " Section 8 Linear Dependence In this activity, we will explore linear dependence and independence in the context of solving nonhomogeneous \\(A x = b\\) and homogeneous equations \\(A x = 0\\). Download this Rmd file. Remember that we will use the pracma package to get the rref function, so we first load it in: require(&quot;pracma&quot;) 8.1 Example 1: a 7x9 integer matrix Here is a 7 x 9 coeefficient matrix that we will use. These commands define it and echo it back. A = cbind( c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24)) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 And here is a vector b that we hope to use in solving A x = b. b = c(382, 51, -321, -314, -86, -170, -153) b ## [1] 382 51 -321 -314 -86 -170 -153 You can augment A with b, and call it Ab, using cbind: Ab = cbind(A,b) Ab ## b ## [1,] 3 5 3 4 0 -4 5 0 37 382 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 51 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 -321 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 -314 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 -86 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 -170 ## [7,] 1 4 5 -3 8 3 -1 0 -24 -153 And row reduce using rref rref(Ab) ## b ## [1,] 1 0 0 0 5 0 0 0 -2 8 ## [2,] 0 1 0 0 -2 0 0 0 2 10 ## [3,] 0 0 1 0 1 0 0 0 -3 -19 ## [4,] 0 0 0 1 -2 0 0 0 3 21 ## [5,] 0 0 0 0 0 1 0 0 0 6 ## [6,] 0 0 0 0 0 0 1 0 6 61 ## [7,] 0 0 0 0 0 0 0 1 0 8 8.1.1 Solution to the nonhomogeneous equations Ax = b Write out the solution to Ax=b in parametric form using the following formatting. You just need to fill in the correct values of the vectors: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 10 \\\\ -19 \\\\ 21 \\\\ 0 \\\\ 6 \\\\ 61 \\\\ 8 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} -5 \\\\ 2 \\\\ -1 \\\\ 2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} 2 \\\\ -2 \\\\ 3 \\\\ -3 \\\\ 0 \\\\ 0 \\\\ -6 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] Describe this solution space (by fixing up this sentence, which is incorrect right now): the set of solutions to A x= b is a plane in \\(\\mathbb{R}^9\\). 8.1.2 Solution to the nonhomogeneous equations Ax = 0 Now, describe the set of solutions to the homogeneous equations A x = 0. Again, you can just edit this: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7 \\\\ x_8 \\\\ x_9 \\end{bmatrix} = \\begin{bmatrix} p1 \\\\ p2 \\\\ p3 \\\\ p4 \\\\ p5 \\\\ p6 \\\\ p7 \\\\ p8 \\\\ p9 \\end{bmatrix} + s \\begin{bmatrix} u1 \\\\ u2 \\\\ u3 \\\\ u4 \\\\ u5 \\\\ u6 \\\\ u7 \\\\ u8 \\\\ u9 \\end{bmatrix} + t \\begin{bmatrix} v1 \\\\ v2 \\\\ v3 \\\\ v4 \\\\ v5 \\\\ v6 \\\\ v7 \\\\ v8 \\\\ v9 \\end{bmatrix} \\] And describe, in words, the geometric relationship between the solutions to Ax=b and Ax=0. your answer here 8.1.3 Linearly dependent columns The columns of the matrix A are linearly dependent. You can see that in rref(A). rref(A) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Discuss in your group how you see it. Then write out a dependence relation among the columns by filling in numbers for the weights in this equation \\[ 0 = c_1 \\vec{a}_1 + c_2 \\vec{a}_2 + c_3 \\vec{a}_3 + c_4 \\vec{a}_4 + c_5 \\vec{a}_5 + c_6 \\vec{a}_6 + c_7 \\vec{a}_7 + c_8 \\vec{a}_8 + c_9 \\vec{a}_9. \\] Challenge: give a dependency relation that none of the other groups in the class have. This is telling us that there is some redundancy in the matrix A. Remove columns from A to get a new matrix M whose columns are linearly independent. You can do this by removing the appropriate columns from the code below: M = cbind( # you need to edit this matrix c(3, 0, 0, 1, -2, -4, 1), c(5, -5, 0, 3, 3, 1, 4), c(3, 5, -1, 1, -3, -3, 5), c(4, -1, -2, 0, -1, 2, -3), c(0, 17, 3, 0, -17, -29, 8), c(-4, -1, -5, -2, -1, -4, 3), c(5, 3, -4, -5, -2, -3, -1), c(0, 5, -3, -2, -1, -5, 0), c(37, -10, -27, -29, 4, 7, -24) ) M ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 3 5 3 4 0 -4 5 0 37 ## [2,] 0 -5 5 -1 17 -1 3 5 -10 ## [3,] 0 0 -1 -2 3 -5 -4 -3 -27 ## [4,] 1 3 1 0 0 -2 -5 -2 -29 ## [5,] -2 3 -3 -1 -17 -1 -2 -1 4 ## [6,] -4 1 -3 2 -29 -4 -3 -5 7 ## [7,] 1 4 5 -3 8 3 -1 0 -24 rref(M) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 1 0 0 0 5 0 0 0 -2 ## [2,] 0 1 0 0 -2 0 0 0 2 ## [3,] 0 0 1 0 1 0 0 0 -3 ## [4,] 0 0 0 1 -2 0 0 0 3 ## [5,] 0 0 0 0 0 1 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 6 ## [7,] 0 0 0 0 0 0 0 1 0 Your matrix should now be square (7x7) with linearly independent columns. R has a build in solve command, solve, that works for matrices of this form (i.e., square with linearly independent columns). You can try it here. First you need to un-comment-out the solve command. I have it commented out right now, because it does not work with the matrix M (above) until you remove its redundancies. # solve(M,b) Now, you should get a unique solution to the equation M x = b, since M has no free variables, and it should be one of the solutions to the original question A x = b. Which solution is it? That is, which of the many solutions to A x = b are you getting here (forw which values of the paramters?). Compare this with trying to use solve on the original equation A x = b with linearly dependent columns. The solve command in the next bit of code is commented out. Delete the comment command and try executing it. # solve(A,b) 8.2 A 5 x 6 Numerical Matrix So far, all of the matrices we’ve worked with in this class have integer values. This is only so that the calulations are nice to do by hand. All of our theory works over the real numbers. Here we will look at a real matrix with numerical values, something you might find when dealing with real-world data. B = cbind( c(0.717, -0.274, 0.365, 0.482, -0.362), c(0.587, -0.545, 0.5, -0.407, -0.597), c(-0.441, 0.886, 0.784, -0.831, -0.594), c(0.923, -0.466, 0.222, 0.867, 0.493), c(-0.42, -0.745, -0.02, -0.44, 0.209), c(0.621, 0.049, -0.134, -0.844, -0.31) ) B ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.717 0.587 -0.441 0.923 -0.420 0.621 ## [2,] -0.274 -0.545 0.886 -0.466 -0.745 0.049 ## [3,] 0.365 0.500 0.784 0.222 -0.020 -0.134 ## [4,] 0.482 -0.407 -0.831 0.867 -0.440 -0.844 ## [5,] -0.362 -0.597 -0.594 0.493 0.209 -0.310 and here is a vector d in \\(\\mathbb{R}^5\\). d = c(5.886, -4.001, 3.701, -6.621, -2.199) d ## [1] 5.886 -4.001 3.701 -6.621 -2.199 Try answering some of these questions: Are the columns of B linearly independent? Do the columns of B span \\(\\mathbb{R}^5\\)? Give the parametric solution to B x = d. What is the geometric form of this solution (e.g., a plane in \\(\\mathbb{R}^4\\))? Remove redundancies from the columns of B to get a new matrix B2 and use solve to solve the equation B2 x = d. Which of the parametric solutions to you get. 8.3 Random Matrices The following code generates a random 5 x 5 matrix. Every time you enter it, it will give you a new matrix. Use this to try to figure out how likely it is that a random square matrix has linearly dependent columns. R1 = matrix(runif(5*5), nrow = 5, ncol = 5) R1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.1470883 0.1111442 0.5527609 0.1709559 0.28576717 ## [2,] 0.5041338 0.7645989 0.6686369 0.9134523 0.39697379 ## [3,] 0.4288498 0.4369059 0.8731083 0.9639793 0.54076999 ## [4,] 0.7068949 0.2027694 0.7661286 0.7426023 0.02086088 ## [5,] 0.4731627 0.5108888 0.2716027 0.7157737 0.88759220 Try the same using the following code that generates a random 5 x 6 matrix. R2 = matrix(runif(5*6), nrow = 5, ncol = 6) R2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.04041633 0.5972663 0.5529482 0.2350636 0.14473268 0.30007336 ## [2,] 0.20447961 0.2929129 0.8189568 0.1253530 0.10232492 0.14572245 ## [3,] 0.29658783 0.9525141 0.1167553 0.7490482 0.05774989 0.23479897 ## [4,] 0.97679878 0.1146261 0.5346714 0.5665212 0.14729105 0.26900706 ## [5,] 0.12784866 0.8167909 0.4995020 0.1300045 0.12065885 0.05151382 Try the same using the following code that generates a random 5 x 4 matrix. R3 = matrix(runif(5*4), nrow = 5, ncol = 4) R3 ## [,1] [,2] [,3] [,4] ## [1,] 0.69453480 0.8609982 0.2074091 0.3583982 ## [2,] 0.19085614 0.4281405 0.2345482 0.1552930 ## [3,] 0.06509063 0.9110361 0.4399915 0.3706388 ## [4,] 0.12075955 0.5641169 0.9642872 0.4009696 ## [5,] 0.95528211 0.7320608 0.9352591 0.8712182 rref(R3) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## [5,] 0 0 0 0 In each of these cases, how likely is it that the columns of the matrix spans all of \\(\\mathbb{R}^4\\)? "]]
