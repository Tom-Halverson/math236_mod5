[["index.html", "MATH 236: Linear Algebra Preface", " MATH 236: Linear Algebra Preface This is the class handbook for Math 236 Linear Algebra at Macalester College. The content here was made by Andrew Beveridge and Tom Halverson and other faculty in the Department of Mathematics, Statistics and Computer Science at Macalester College. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["problem-set-1.html", "Section 1 Problem Set 1 1.1 Characterize the Solution Set 1.2 Find the General Solution 1.3 Elementary row operations are reversible 1.4 Designer Parabolas 1.5 Traffic Flow", " Section 1 Problem Set 1 Due: Tuesday January 26 by noon CST. Week 1 and Week 8 are half weeks, so those two assignments will be split in two and called Problem Sets 1A and 1B. Upload your solutions to problems 1–4 by writing them out by hand, scanning them to pdf using a scanning software such as AdobeScan, assembling them into a single PDF, and uploading it to Moodle. Problem 1.5 is to be done using RStudio. To solve it, create an Rmarkdown file, knit it to .html, and upload the .html on Moodle along with the PDF for questions 1-4. 1.1 Characterize the Solution Set The following augmented matrices are in row echelon form. Decide whether the set of solutions is a point, line, plane, or the empty set in 3-space. Briefly justify your answer. \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 4 \\\\ 0 &amp; 1 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; -1 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 0 &amp; 1 &amp; 0 &amp; 6 \\\\ 0 &amp; 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.2 Find the General Solution Each of the following matrices is the reduced row echelon form of the augmented matrix of a system of linear equations. Give the general solution to each system. \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 3 &amp; 0 &amp; -2 &amp; 5\\\\ 0 &amp; 0 &amp; 1 &amp; 4 &amp; -2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; 4 &amp; 0 &amp; 3 &amp; 6\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -2&amp; -8 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 3 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 4 &amp; 0 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7 &amp; 6\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.3 Elementary row operations are reversible In each case below, an elementary row operation turns the matrix \\(A\\) into the matrix \\(B\\). For each of them, Describe the row operation that turns \\(A\\) into \\(B\\), and Describe the row operation that turns \\(B\\) into \\(A\\). Give your answers in the form: “scale \\(R_2\\) by 3” or “swap \\(R_1\\) and \\(R_4\\)” or “replace \\(R_3\\) with \\(R_3 + \\frac{1}{5} R_1\\).” \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 0 &amp; 7 &amp; 0 &amp; -4 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 4 &amp; 1 &amp; -2 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] 1.4 Designer Parabolas In each part below, set up and solve a linear system of equations to find all possible parabolas of the form \\[ f(x) = a + b x + c x^2 \\] that satisfy the given conditions. For full credit, please solve these by hand, doing all row reductions that bring the system of equations to Reduced Row Echelon Form. On future assignments, you can solve problems like this using either RStudio or WolframAlpha. You are welcome (and, in fact, encouraged) to check your answers using software. \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(2,4)\\). \\(f(x)\\) passes through the three points: \\((1,3), (3,11),(3,10)\\). \\(f(x)\\) passes through the two points: \\((1,3)\\) and \\((3,11)\\). 1.5 Traffic Flow Below you find a section of one-way streets in downtown St Paul, where the arrows indicate traffic direction. The traffic control center has installed electronic sensors that count the numbers of vehicles passing through the 6 streets that lead into and out of this area. Assume that the total flow that enters each intersection equals the the total flow that leaves each intersection (we will ignore parking and staying). Create a system of linear equations to find the possible flow values for the inner streets \\(x_1, x_2, x_3, x_4\\). Using RStudio, enter the augmented matrix of this system, and solve it using the rref command. Type out the general solution to this system of equations. Your answer to part b should be an infinite solution set. Give two distinct solutions that are realistic in terms of traffic flow. Is it possible to close down the street labeled by \\(x_2\\) for road construction? That is, is it possible to have \\(x_2 = 0\\) and to meet the other conditions? "],["important-definitions.html", "Section 2 Important Definitions 2.1 Vector Spaces 2.2 Matrices 2.3 Orthogonality 2.4 Spectral Decompostion", " Section 2 Important Definitions 2.1 Vector Spaces span A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span a vector space \\(V\\) if for every \\(\\mathsf{v} \\in V\\) there exist a set of scalars (weights) \\(c_1, c_2, \\ldots, c_n \\in \\mathbb{R}\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, \\ldots, c_n]^{\\top}\\) is a solution to \\(A x = \\mathsf{v}\\). linear independence A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\ldots, \\mathsf{v}_n\\) are linearly independent if the only way to write \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] is with \\(c_1 = c_2 = \\cdots = c_n = 0\\). Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(A x = \\mathsf{0}\\) has only the trivial solution. linear dependence Conversely, a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly dependent if there exist scalars \\(c_1, c_2,\\ldots, c_n \\in \\mathbb{R}\\) that are not all equal to 0 such that \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] This is called a dependence relation among the vectors. Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, c_2, \\ldots, c_n]^{\\top}\\) is a nontrivial solution to \\(A \\mathsf{x} = \\mathsf{0}\\). linear transformation A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation when: \\(T(\\mathsf{u} + \\mathsf{v}) = T(\\mathsf{u}) + T(\\mathsf{v})\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\) (preserves addition) \\(T(c \\mathsf{u} ) = c T(\\mathsf{u})\\) for all \\(\\mathsf{u} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\) (preserves scalar multiplication). It follows from these that also \\(T(\\mathsf{0}) = \\mathsf{0}\\). one-to-one A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a one-to-one when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at most one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). onto A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a onto when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at least one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). subspace A subset \\(S \\subseteq \\mathbb{R}^n\\) is a subspace when: \\(\\mathsf{u} + \\mathsf{v} \\in S\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in S\\) (closed under addition) \\(c \\mathsf{u} \\in S\\) for all \\(\\mathsf{u}\\in S\\) and \\(c \\in \\mathbb{R}\\) (closed under scalar multiplication) It follows from these that also \\(\\mathsf{0} \\in S\\). basis A basis of a vector space (or subspace) \\(V\\) is a set of vectors \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) in \\(V\\) such that \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span \\(V\\) \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly independent Equivalently, one can say that \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) is a basis of \\(V\\) if for every vector \\(\\mathsf{v} \\in V\\) there is a unique set of scalars \\(c_1, \\ldots, c_n\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] (The fact that there is a set of vectors comes from the span; the fact that they are unique comes from linear independence). dimension The dimension of a subspace \\(W\\) is the number of vectors in any basis of \\(W\\). This is also the fewest number of vectors required to span the subspace. 2.2 Matrices invertible The square \\(n \\times n\\) matrix \\(A\\) is invertible when there exists an \\(n \\times n\\) matrix \\(A^{-1}\\) such that \\(A A^{-1} = I = A^{-1} A\\). The Invertible Matrix Theorem collects over two dozen equivalent conditions, each of which guarantees that \\(A\\) is invertible. null space The null space \\(\\mbox{Nul}(A) \\subset \\mathbb{R}^n\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of solutions to the homogeneous equation \\(A \\mathsf{x} = \\mathbf{0}\\)&gt; We also write this as \\[ \\mbox{Nul}(A) = \\{ \\mathsf{x} \\in \\mathbb{R}^n : A \\mathsf{x} = \\mathbf{0} \\} \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the kernel of \\(T\\) is the null space of matrix \\(A\\). column space The column space \\(\\mbox{Col}(A) \\subset \\mathbb{R}^m\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of all linear combinations of the columns of \\(A\\). For \\(A = \\begin{bmatrix} \\mathsf{a}_1 &amp; \\mathsf{a}_2 &amp; \\cdots &amp; \\mathsf{a}_n \\end{bmatrix}\\), we have \\[ \\mbox{Col}(A) = \\mbox{span} ( \\mathsf{a}_1, \\mathsf{a}_2, \\ldots , \\mathsf{a}_n ) \\] We also write this as \\[ \\mbox{Col}(A) = \\{ \\mathsf{b} \\in \\mathbb{R}^m : \\mathsf{b} = A \\mathsf{x} \\mbox{ for some } \\mathsf{x} \\in \\mathbb{R}^n \\}. \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the range (also called the image) of \\(T\\) is the column space of matrix \\(A\\). rank The rank of the \\(m \\times n\\) matrix \\(A\\) is the dimension of the column space of \\(A\\). This is also the number of pivot columns of the matrix. eigenvalue and eigenvector For a square \\(n \\times n\\) matrix \\(A\\), the scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue for \\(A\\) when there exists a nonzero vector \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(A \\mathsf{x} = \\lambda \\mathsf{x}\\). The nonzero vector \\(\\mathsf{x}\\) is the eigenvector for eigenvalue \\(\\lambda\\). The collection of all of these eigenvalues and eigenvectors is called the eigensystem of A. diagonalization A square \\(n \\times n\\) matrix is diagonalizable when \\(A = P D P^{-1}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an invertible matrix. In this case, the eigenvalues of \\(A\\) are the diagonal entries of \\(D\\) and their corresponding eigenvectors are the columns of \\(P\\). dominant eigenvalue The eigenvalue \\(\\lambda\\) of the square matrix \\(A\\) is the dominant eigenvalue when \\(| \\lambda | &gt; | \\mu |\\) where \\(\\mu\\) is any other eigenvalue of \\(A\\). The dominant eigenvalue determines the long-term behavior of \\(A^t\\) as \\(t \\rightarrow \\infty\\). 2.3 Orthogonality length The length of a vector \\(\\mathsf{v}\\) is \\[ \\| \\mathsf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}. \\] distance and angle The distance between vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) is \\[ \\mbox{dist}(\\mathsf{u},\\mathsf{v}) = \\| \\mathsf{u} - \\mathsf{v} \\|. \\] The angle \\(\\theta\\) between these vectors is determined by \\[ \\cos \\theta = \\frac{\\mathsf{u} \\cdot \\mathsf{v}}{ \\| \\mathsf{u} \\| \\, \\| \\mathsf{v} \\|}. \\] orthogonal The vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are orthogonal when \\(\\mathsf{u} \\cdot \\mathsf{v} = 0\\). This means that either one of them is the zero vector, or they are perpendicular to one another. orthogonal complement If \\(W \\subset \\mathbb{R}^n\\) is a subspace, then its orthogonal complement \\(W^{\\perp}\\) is the set of all vectors in \\(\\mathsf{R}^n\\) that are orthogonal to \\(W\\). We also write \\[ W^{\\perp} = \\{ \\mathsf{v} \\in \\mathbb{R}^n : \\mathsf{v} \\cdot \\mathsf{w} \\mbox{ for all } \\mathsf{w} \\in W \\}. \\] orthonormal set A collection of vectors \\(\\mathsf{u}_1, \\mathsf{u}_2, \\ldots, \\mathsf{u}_k\\) are an orthonormal set when every vector has length 1 and the vectors are pairwise orthogonal. orthogonal matrix orthogonal matrix A square \\(n \\times n\\) matrix \\(P\\) is an orthogonal matrix when its columns are an orthonormal set. As a result, we have \\(P^{-1} = P^{\\top}\\). projection and residual The orthogonal projection of vector \\(\\mathsf{y}\\) into a subspace \\(W\\) is the unique vector \\(\\hat{\\mathsf{y}} \\in W\\) such that \\(\\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}} \\in W^{\\perp}\\). The vector \\(\\mathsf{z}\\) is called the residual vector for the projection. 2.4 Spectral Decompostion orthogonal diagonalization Every symmetric \\(n \\times n\\) matrix is orthogonally diagonalizable, meaning that we have \\(A = P D P^{\\top}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix. The diagonal entries of \\(D\\) are the eigenvalues of \\(A\\) and the columns of \\(P\\) are the corresponding orthonormal eigenvectors. Furthermore, the eigenvalues of \\(A\\) are nonnegative. spectral decomposition A symmetric matrix \\(A\\) can be written as a linear combination of rank 1 matrices derived from the orthonormal eigensystem of \\(A\\). In particular, we have \\[ A = \\lambda_1 \\mathsf{u}_1 \\mathsf{u}_1^{\\top} + \\lambda_2 \\mathsf{u}_2 \\mathsf{u}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{u}_n \\mathsf{u}_n^{\\top}. \\] This linear combination of rank 1 vectors is called the spectral decomposition of \\(A\\). singular value decomposition (SVD) Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be factored into its singular value decomposition \\(U \\Sigma V^{\\top}\\) where \\(U\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\Sigma\\) is a matrix whose nonzero entries are the positive numbers \\(\\sigma_1, \\ldots , \\sigma_r\\), which appear in decreasing order on the diagonal, and \\(V\\) is an \\(n \\times n\\) orthogonal matrix. The nonzero entries of \\(\\Sigma\\) are called the singular values of \\(A\\). The columns of \\(U\\) are the left singular vectors and the rows of \\(V^{\\top}\\) are the right singular vectors. SVD spectral decomposition Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be written as a linear combination of rank 1 matrices derived from the singular value decomposition of \\(A\\). In particular, we have \\[ A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\sigma_r \\mathsf{u}_r \\mathsf{v}_r^{\\top}. \\] This linear combination of rank 1 vectors is called the (SVD) spectral decomposition of \\(A\\). "],["week-1-learning-goals.html", "Section 3 Week 1 Learning Goals 3.1 Solving Linear Equations 3.2 RStudio 3.3 Vocabulary 3.4 Conceptual Thinking", " Section 3 Week 1 Learning Goals Here are the knowledge and skills you should master by the end of this first, shorter week. 3.1 Solving Linear Equations I should be able to do the following tasks: Identify linear systems from nonlinear systems Create a linear system to solve a variety of applied scenarios Convert between a linear system and an augmented matrix Row reduce an augmented matrix into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Use REF to determine whether a linear system is consistent or inconsistent Use REF to determine whether a consistent system has a unique solution or an infinite number of solutions Use RREF to find explicit equations for the solution set of a consistent system 3.2 RStudio I should be able to do the following tasks: Log in to Macalester’s RStudio server Upload R Markdown files to RStudio Knit R Markdown to produce HTML Use RStudio to create vectors and matrices Use the rref command from pracma to solve a linear system 3.3 Vocabulary I should know and be able to use and explain the following terms: elementary row operation (and be able to state them) augmented matrix REF and RREF pivot position basic variable (pivot variable) free variable consistent system and inconsistent system 3.4 Conceptual Thinking I should understand and be able to perform the following conceptual tasks: Model 2-dimensional linear systems as the intersections of lines Model 3-dimensional linear systems as the intersections of planes "],["week-2-learning-goals.html", "Section 4 Week 2 Learning Goals 4.1 Solution Sets, Span and Linear Independence 4.2 Vocabulary 4.3 Conceptual Thinking", " Section 4 Week 2 Learning Goals Here are the knowledge and skills you should master by the end the second week. 4.1 Solution Sets, Span and Linear Independence I should be able to do the following tasks: Go back and forth between (i) systems of equations, (ii) vector equations, and (iii) the matrix equation \\(Ax = b\\). Compute and understand the matrix-vector product \\(A x\\) both as a linear combination of the columns of A and as the dot product of \\(x\\) with the rows of \\(A\\). Write the solution set to \\(Ax=b\\) as a parametric vector equation. Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) 4.2 Vocabulary I should know and be able to use and explain the following terms or properties. \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) homogeneous and nonhomogeneous equations parametric vector equations linear independence and linear dependence 4.3 Conceptual Thinking I should understand and be able to explain the following concepts: Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b \\in \\mathbb{R}^m\\), the system \\(A x = b\\) has at least one solution Each \\(b \\in \\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. Understand the relation between homogeneous solutions and nonhomogeneous solutions. Linear independence Span More than \\(n\\) vectors in \\(\\mathbb{R}^n\\) must be linearly dependent. "],["linear-systems-in-r.html", "Section 5 Linear Systems in R 5.1 Getting started with R 5.2 Building Vectors and Matrices 5.3 Solving a Linear System 5.4 Solving another Linear System 5.5 Appendix: Dimensionless Vectors in R", " Section 5 Linear Systems in R 5.1 Getting started with R To use RStudio, you have two choices: Use the cloud version by logging in to Rstudio.macalester.edu. This is the easiest way to use RStudio and works great for our course. You can also download the free desktop version of RStudio. If you plan to go on to take more MSCS classes, especially in statistics and data science, you may want to use the desktop version. Download the desktop version following the instructions here: rstudio.com/products. Now, let’s learn how to use R to solve systems of linear equations! Download this Rmd file. First, we will create vectors and matrices Then we will see how to create an augmented matrix and then apply Gaussian Elimination to obtain is reduced row echelon form. Gaussian elimination is performed by the rref() command. However, this command is not loaded into R by default. So we have have to tell RStudio to use the practical math package, which is known as pracma. So we need to run the following command once at the beginning of our session. require(pracma) 5.2 Building Vectors and Matrices A vector in R is a list of data. The simplest way to create a vector is to use the c() command. The letter ‘c’ is short for ‘combine these values into a vector.’ For example, we can make a vector v for the numbers 1,2,3 as follows: v=c(1,2,3) v ## [1] 1 2 3 Note that we had to ask R to display the value of v. This is because the assignment of v doesn’t echo the value to the console. But can see the value of v in the Environment tab in the upper right panel of RStudio. For example, run this command and then check to see that the value of v gets updated in the environment. v=c(1,2,3,4,5,6) It is interesting to note that c() returns a dimensionless vector. So you can treat a vector c() as either a row or a column when you construct a matrix. For example, suppose that we want to make the matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] We could create this matrix by binding three row vectors: A = rbind(c(1,1,1), c(2,4,8), c(3,9,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 or we could bind three column vectors: A = cbind(c(1,2,3), c(1,4,9), c(1,8,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 5.3 Solving a Linear System Suppose that we want to solve the linear system \\[\\begin{aligned} x + y + z &amp;= 7 \\\\ 2x + 4y + 8z &amp;= 6 \\\\ 3x +9y+27z &amp;=12 \\end{aligned}\\] which has coefficient matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] and target (column) vector \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix}. \\] This is the same matrix A we defined above. Let’s define a vector b and use cbind() to create an augmented matrix which we will name Ab. (We could have just made the full augmented matrix from the start, but using cbind to add a column to a matrix is a skill we will use later in the course!) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 b = c(4,6,12) Ab = cbind(A,b) Ab ## b ## [1,] 1 1 1 4 ## [2,] 2 4 8 6 ## [3,] 3 9 27 12 Now we use the rref() command to apply Gaussian Elimination to produce the reduced row echelon form. (And remember: we had to load this function into R by using the require(pracma) command above.) rref(Ab) ## b ## [1,] 1 0 0 7 ## [2,] 0 1 0 -4 ## [3,] 0 0 1 1 We conclude that this is a consistent system no free variables. The unique solution is \\[\\begin{align} x&amp;=7\\\\ y&amp;=-4\\\\ z&amp;=1 \\end{align}\\] We can verify that our answer works by multiplying \\(A\\) by one of the solutions above. Matrix multiplication uses the funny operation %*%. A %*% c(7,-4,1) ## [,1] ## [1,] 4 ## [2,] 6 ## [3,] 12 #A %*% c(1,2,3,0,0) Which matches our target \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix} \\] just as we had hoped. 5.4 Solving another Linear System Now let’s find the solution set for the linear system \\[ \\begin{array}{rrrrrcr} x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ 2x_1 &amp; +x_2 &amp; +2x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; 4 \\\\ -x_1 &amp; +x_2 &amp; +x_3 &amp; &amp; &amp; = &amp; 10 \\\\ x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ \\end{array} \\] which corresponds to augmented matrix \\[ \\left[ \\begin{array}{rrrrr|r} 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ 2 &amp; +1 &amp; +2 &amp; -1 &amp; -1 &amp; 4 \\\\ -1 &amp; +1 &amp; +1 &amp; &amp; &amp; 10 \\\\ 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ \\end{array} \\right] \\] This time, let’s just construct the augmented matrix direclty. Then we define the coefficient matrix \\(A\\). Here we use cbind to combine the vectors into the columns of a matrix named \\(A\\). You can use rbind if you want to combine the vectors into the rows of a matrix. Ab = cbind(c(1,2,-1,1),c(0,1,1,0),c(-1,2,1,-1),c(-1,1,0,-1),c(-1,5,0,-1),c(-2,10,4,-2)) Ab ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 -1 -1 -1 -2 ## [2,] 2 1 2 1 5 10 ## [3,] -1 1 1 0 0 4 ## [4,] 1 0 -1 -1 -1 -2 And now let’s row reduce to get RREF. rref(Ab) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 1 1 ## [2,] 0 1 0 -1 -1 2 ## [3,] 0 0 1 1 2 3 ## [4,] 0 0 0 0 0 0 So the set of solutions in parametric form is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ 1 \\\\ -2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] and this is a “plane” in \\(\\mathbb{R}^5\\). It is in \\(\\mathbb{R}^5\\) because these vectors have 5 coordinates. It is a plane because it is spanned by two vectors that are not on the same line. 5.5 Appendix: Dimensionless Vectors in R Let’s revisit the vector constructed by cbind. Above we called this a “dimensionless” vector because it can be used as a column vector or a row vector. In general, R will do its best to make sense of a dimensionless vector. In other words, it will promote c() to make an expression valid. For example, let \\(A\\) be an \\(n \\times n\\) matrix, and let \\(b\\) be a vector. The expression \\(Av\\) is only defined when \\(v\\) is a \\(n \\times 1\\) column vector and that \\(wA\\) is only defined when \\(w\\) is a \\(1 \\times n\\) ** row vector**. But let’s look at what happens when we use a dimensionless vector instead. A = cbind(c(1,1,1),c(-1,0,1), c(0,1,-1)) A ## [,1] [,2] [,3] ## [1,] 1 -1 0 ## [2,] 1 0 1 ## [3,] 1 1 -1 b = c(2,5,11) b ## [1] 2 5 11 # A times b A %*% b ## [,1] ## [1,] -3 ## [2,] 13 ## [3,] -4 # b times A b %*% A ## [,1] [,2] [,3] ## [1,] 18 9 -6 Both of these multiplications worked! So R treated b as a column vector for the multiplicationA %*% b. And then R treated b as a row vector for the multiplication b %b% A. So how do you make a true column vector or a true row vector? The answer is to use cbind and rbind! Here are some examples: # dimensionless b = c(1,2,3,4) b ## [1] 1 2 3 4 # column vector b.col = cbind(b) b.col ## b ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 # row vector b.row = rbind(b) b.row ## [,1] [,2] [,3] [,4] ## b 1 2 3 4 "]]
