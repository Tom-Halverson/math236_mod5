<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 2 Important Definitions | MATH 236: Linear Algebra</title>
  <meta name="description" content="This is the class activity manual for MATH 236 at Macalester College." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 2 Important Definitions | MATH 236: Linear Algebra" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the class activity manual for MATH 236 at Macalester College." />
  <meta name="github-repo" content="Tom-Halverson/math236_s21" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 2 Important Definitions | MATH 236: Linear Algebra" />
  
  <meta name="twitter:description" content="This is the class activity manual for MATH 236 at Macalester College." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="problem-set-1.html"/>
<link rel="next" href="week-1-learning-goals.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; } /* Alert */
code span.an { color: #008000; } /* Annotation */
code span.at { } /* Attribute */
code span.bu { } /* BuiltIn */
code span.cf { color: #0000ff; } /* ControlFlow */
code span.ch { color: #008080; } /* Char */
code span.cn { } /* Constant */
code span.co { color: #008000; } /* Comment */
code span.cv { color: #008000; } /* CommentVar */
code span.do { color: #008000; } /* Documentation */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.im { } /* Import */
code span.in { color: #008000; } /* Information */
code span.kw { color: #0000ff; } /* Keyword */
code span.op { } /* Operator */
code span.ot { color: #ff4000; } /* Other */
code span.pp { color: #ff4000; } /* Preprocessor */
code span.sc { color: #008080; } /* SpecialChar */
code span.ss { color: #008080; } /* SpecialString */
code span.st { color: #008080; } /* String */
code span.va { } /* Variable */
code span.vs { color: #008080; } /* VerbatimString */
code span.wa { color: #008000; font-weight: bold; } /* Warning */
</style>

<link rel="stylesheet" href="m236.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href = "./">MATH 236: Linear Algebra</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Problem Sets</b></span></li>
<li class="chapter" data-level="1" data-path="problem-set-1.html"><a href="problem-set-1.html"><i class="fa fa-check"></i><b>1</b> Problem Set 1</a><ul>
<li class="chapter" data-level="1.1" data-path="problem-set-1.html"><a href="problem-set-1.html#characterize-the-solution-set"><i class="fa fa-check"></i><b>1.1</b> Characterize the Solution Set</a></li>
<li class="chapter" data-level="1.2" data-path="problem-set-1.html"><a href="problem-set-1.html#find-the-general-solution"><i class="fa fa-check"></i><b>1.2</b> Find the General Solution</a></li>
<li class="chapter" data-level="1.3" data-path="problem-set-1.html"><a href="problem-set-1.html#elementary-row-operations-are-reversible"><i class="fa fa-check"></i><b>1.3</b> Elementary row operations are reversible</a></li>
<li class="chapter" data-level="1.4" data-path="problem-set-1.html"><a href="problem-set-1.html#designer-parabolas"><i class="fa fa-check"></i><b>1.4</b> Designer Parabolas</a></li>
<li class="chapter" data-level="1.5" data-path="problem-set-1.html"><a href="problem-set-1.html#traffic-flow"><i class="fa fa-check"></i><b>1.5</b> Traffic Flow</a></li>
</ul></li>
<li class="part"><span><b>II Concepts of Linear Algebra</b></span></li>
<li class="chapter" data-level="2" data-path="important-definitions.html"><a href="important-definitions.html"><i class="fa fa-check"></i><b>2</b> Important Definitions</a><ul>
<li class="chapter" data-level="2.1" data-path="important-definitions.html"><a href="important-definitions.html#vector-spaces"><i class="fa fa-check"></i><b>2.1</b> Vector Spaces</a></li>
<li class="chapter" data-level="2.2" data-path="important-definitions.html"><a href="important-definitions.html#matrices"><i class="fa fa-check"></i><b>2.2</b> Matrices</a></li>
<li class="chapter" data-level="2.3" data-path="important-definitions.html"><a href="important-definitions.html#orthogonality"><i class="fa fa-check"></i><b>2.3</b> Orthogonality</a></li>
<li class="chapter" data-level="2.4" data-path="important-definitions.html"><a href="important-definitions.html#spectral-decompostion"><i class="fa fa-check"></i><b>2.4</b> Spectral Decompostion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-1-learning-goals.html"><a href="week-1-learning-goals.html"><i class="fa fa-check"></i><b>3</b> Week 1 Learning Goals</a><ul>
<li class="chapter" data-level="3.1" data-path="week-1-learning-goals.html"><a href="week-1-learning-goals.html#solving-linear-equations"><i class="fa fa-check"></i><b>3.1</b> Solving Linear Equations</a></li>
<li class="chapter" data-level="3.2" data-path="week-1-learning-goals.html"><a href="week-1-learning-goals.html#rstudio"><i class="fa fa-check"></i><b>3.2</b> RStudio</a></li>
<li class="chapter" data-level="3.3" data-path="week-1-learning-goals.html"><a href="week-1-learning-goals.html#vocabulary"><i class="fa fa-check"></i><b>3.3</b> Vocabulary</a></li>
<li class="chapter" data-level="3.4" data-path="week-1-learning-goals.html"><a href="week-1-learning-goals.html#conceptual-thinking"><i class="fa fa-check"></i><b>3.4</b> Conceptual Thinking</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-learning-goals.html"><a href="week-2-learning-goals.html"><i class="fa fa-check"></i><b>4</b> Week 2 Learning Goals</a><ul>
<li class="chapter" data-level="4.1" data-path="week-2-learning-goals.html"><a href="week-2-learning-goals.html#solution-sets-span-and-linear-independence"><i class="fa fa-check"></i><b>4.1</b> Solution Sets, Span and Linear Independence</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-learning-goals.html"><a href="week-2-learning-goals.html#vocabulary-1"><i class="fa fa-check"></i><b>4.2</b> Vocabulary</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-learning-goals.html"><a href="week-2-learning-goals.html#conceptual-thinking-1"><i class="fa fa-check"></i><b>4.3</b> Conceptual Thinking</a></li>
</ul></li>
<li class="part"><span><b>III Linear Algebra in R</b></span></li>
<li class="chapter" data-level="5" data-path="linear-systems-in-r.html"><a href="linear-systems-in-r.html"><i class="fa fa-check"></i><b>5</b> Linear Systems in R</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-systems-in-r.html"><a href="linear-systems-in-r.html#getting-started-with-r"><i class="fa fa-check"></i><b>5.1</b> Getting started with R</a></li>
<li class="chapter" data-level="5.2" data-path="linear-systems-in-r.html"><a href="linear-systems-in-r.html#building-vectors-and-matrices"><i class="fa fa-check"></i><b>5.2</b> Building Vectors and Matrices</a></li>
<li class="chapter" data-level="5.3" data-path="linear-systems-in-r.html"><a href="linear-systems-in-r.html#solving-a-linear-system"><i class="fa fa-check"></i><b>5.3</b> Solving a Linear System</a></li>
<li class="chapter" data-level="5.4" data-path="linear-systems-in-r.html"><a href="linear-systems-in-r.html#solving-another-linear-system"><i class="fa fa-check"></i><b>5.4</b> Solving another Linear System</a></li>
<li class="chapter" data-level="5.5" data-path="linear-systems-in-r.html"><a href="linear-systems-in-r.html#appendix-dimensionless-vectors-in-r"><i class="fa fa-check"></i><b>5.5</b> Appendix: Dimensionless Vectors in R</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH 236: Linear Algebra</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="important-definitions" class="section level1">
<h1><span class="header-section-number">Section 2</span> Important Definitions</h1>
<div id="vector-spaces" class="section level2">
<h2><span class="header-section-number">2.1</span> Vector Spaces</h2>
<dl>
<dt>span</dt>
<dd><p>A set of vectors <span class="math inline">\(\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n\)</span> <strong>span</strong> a vector space <span class="math inline">\(V\)</span> if for every <span class="math inline">\(\mathsf{v} \in V\)</span> there exist a set of scalars (weights) <span class="math inline">\(c_1, c_2, \ldots, c_n \in \mathbb{R}\)</span> such that
<span class="math display">\[
\mathsf{v} = c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + \cdots + c_n \mathsf{v}_n.
\]</span>
<em>Connection to Matrices:</em> If <span class="math inline">\(A = [\mathsf{v}_1 \mathsf{v}_2 \cdots \mathsf{v}_n]\)</span> is the matrix with these vectors in the columns, then this is the same as saying that <span class="math inline">\(\mathsf{x} = [c_1, \ldots, c_n]^{\top}\)</span> is a solution to <span class="math inline">\(A x = \mathsf{v}\)</span>.</p>
</dd>
<dt>linear independence</dt>
<dd><p>A set of vectors <span class="math inline">\(\mathsf{v}_1, \mathsf{v}_2,\ldots, \mathsf{v}_n\)</span> are <strong>linearly independent</strong> if the only way to write
<span class="math display">\[
\mathsf{0} = c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + \cdots + c_n \mathsf{v}_n
\]</span>
is with <span class="math inline">\(c_1 = c_2 = \cdots = c_n = 0\)</span>.
<br/>
<em>Connection to Matrices:</em> If <span class="math inline">\(A = [\mathsf{v}_1 \mathsf{v}_2 \cdots \mathsf{v}_n]\)</span> is the matrix with these vectors in the columns, then this is the same as saying that <span class="math inline">\(A x = \mathsf{0}\)</span> has only the trivial solution.</p>
</dd>
<dt>linear dependence</dt>
<dd><p>Conversely, a set of vectors <span class="math inline">\(\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n\)</span> are <strong>linearly dependent</strong> if there exist scalars <span class="math inline">\(c_1, c_2,\ldots, c_n \in \mathbb{R}\)</span> that are <strong>not all equal to 0</strong> such that
<span class="math display">\[
\mathsf{0} = c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + \cdots + c_n \mathsf{v}_n
\]</span>
This is called a <strong>dependence relation</strong> among the vectors.
<br/>
<em>Connection to Matrices:</em> If <span class="math inline">\(A = [\mathsf{v}_1 \mathsf{v}_2 \cdots \mathsf{v}_n]\)</span> is the matrix with these vectors in the columns, then this is the same as saying that <span class="math inline">\(\mathsf{x} = [c_1, c_2, \ldots, c_n]^{\top}\)</span> is a nontrivial solution to <span class="math inline">\(A \mathsf{x} = \mathsf{0}\)</span>.</p>
</dd>
<dt>linear transformation</dt>
<dd><p>A function <span class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}^m\)</span> is a <strong>linear transformation</strong> when:</p>
<ul>
<li><span class="math inline">\(T(\mathsf{u} + \mathsf{v}) = T(\mathsf{u}) + T(\mathsf{v})\)</span> for all <span class="math inline">\(\mathsf{u}, \mathsf{v} \in \mathbb{R}^n\)</span> (preserves addition)</li>
<li><span class="math inline">\(T(c \mathsf{u} ) = c T(\mathsf{u})\)</span> for all <span class="math inline">\(\mathsf{u} \in \mathbb{R}^n\)</span> and <span class="math inline">\(c \in \mathbb{R}\)</span> (preserves scalar multiplication).
It follows from these that also <span class="math inline">\(T(\mathsf{0}) = \mathsf{0}\)</span>.</li>
</ul>
</dd>
<dt>one-to-one</dt>
<dd><p>A function <span class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}^m\)</span> is a <strong>one-to-one</strong> when:</p>
</dd>
</dl>
<center>
for all <span class="math inline">\(\mathsf{y} \in \mathbb{R}^m\)</span> there is <strong>at most</strong> one <span class="math inline">\(\mathsf{x} \in \mathbb{R}^n\)</span> such that <span class="math inline">\(T(\mathsf{x}) = \mathsf{y}\)</span>.
</center>
<dl>
<dt>onto</dt>
<dd><p>A function <span class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}^m\)</span> is a <strong>onto</strong> when:</p>
</dd>
</dl>
<center>
for all <span class="math inline">\(\mathsf{y} \in \mathbb{R}^m\)</span> there is <strong>at least</strong> one <span class="math inline">\(\mathsf{x} \in \mathbb{R}^n\)</span> such that <span class="math inline">\(T(\mathsf{x}) = \mathsf{y}\)</span>.
</center>
<dl>
<dt>subspace</dt>
<dd><p>A subset <span class="math inline">\(S \subseteq \mathbb{R}^n\)</span> is a <strong>subspace</strong> when:</p>
<ul>
<li><span class="math inline">\(\mathsf{u} + \mathsf{v} \in S\)</span> for all <span class="math inline">\(\mathsf{u}, \mathsf{v} \in S\)</span> (closed under addition)</li>
<li><span class="math inline">\(c \mathsf{u} \in S\)</span> for all <span class="math inline">\(\mathsf{u}\in S\)</span> and <span class="math inline">\(c \in \mathbb{R}\)</span> (closed under scalar multiplication)
It follows from these that also <span class="math inline">\(\mathsf{0} \in S\)</span>.</li>
</ul>
</dd>
<dt>basis</dt>
<dd><p>A <strong>basis</strong> of a vector space (or subspace) <span class="math inline">\(V\)</span> is a set of vectors <span class="math inline">\(\mathcal{B} = \{\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n\}\)</span> in <span class="math inline">\(V\)</span> such that</p>
<ul>
<li><span class="math inline">\(\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n\)</span> span <span class="math inline">\(V\)</span></li>
<li><span class="math inline">\(\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n\)</span> are linearly independent
Equivalently, one can say that <span class="math inline">\(\mathcal{B} = \{\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n\}\)</span> is a basis of <span class="math inline">\(V\)</span> if for every vector <span class="math inline">\(\mathsf{v} \in V\)</span> there is a <strong>unique</strong> set of scalars <span class="math inline">\(c_1, \ldots, c_n\)</span> such that
<span class="math display">\[
\mathsf{v} = c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + \cdots + c_n \mathsf{v}_n.
\]</span>
(The fact that there is a set of vectors comes from the span; the fact that they are unique comes from linear independence).</li>
</ul>
</dd>
<dt>dimension</dt>
<dd><p>The <strong>dimension</strong> of a subspace <span class="math inline">\(W\)</span> is the number of vectors in any basis of <span class="math inline">\(W\)</span>. This is also the fewest number of vectors required to span the subspace.</p>
</dd>
</dl>
</div>
<div id="matrices" class="section level2">
<h2><span class="header-section-number">2.2</span> Matrices</h2>
<dl>
<dt>invertible</dt>
<dd><p>The square <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is <strong>invertible</strong> when there exists an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A^{-1}\)</span> such that <span class="math inline">\(A A^{-1} = I = A^{-1} A\)</span>. The Invertible Matrix Theorem collects over two dozen equivalent conditions, each of which guarantees that <span class="math inline">\(A\)</span> is invertible.</p>
</dd>
<dt>null space</dt>
<dd><p>The <strong>null space</strong> <span class="math inline">\(\mbox{Nul}(A) \subset \mathbb{R}^n\)</span> of the <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> is the set of solutions to the homogeneous equation <span class="math inline">\(A \mathsf{x} = \mathbf{0}\)</span>&gt; We also write this as
<span class="math display">\[
\mbox{Nul}(A) = \{ \mathsf{x} \in \mathbb{R}^n : A \mathsf{x} = \mathbf{0} \}
\]</span>
<em>Connection to Linear Transformations</em>: If <span class="math inline">\(T(\mathsf{x}) = A \mathsf{x}\)</span>, then the kernel of <span class="math inline">\(T\)</span> is the null space of matrix <span class="math inline">\(A\)</span>.</p>
</dd>
<dt>column space</dt>
<dd><p>The <strong>column space</strong> <span class="math inline">\(\mbox{Col}(A) \subset \mathbb{R}^m\)</span> of the <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> is the set of all linear combinations of the columns of <span class="math inline">\(A\)</span>.
For <span class="math inline">\(A = \begin{bmatrix} \mathsf{a}_1 &amp; \mathsf{a}_2 &amp; \cdots &amp; \mathsf{a}_n \end{bmatrix}\)</span>, we have
<span class="math display">\[
\mbox{Col}(A) = \mbox{span} ( \mathsf{a}_1, \mathsf{a}_2, \ldots , \mathsf{a}_n )
\]</span>
We also write this as
<span class="math display">\[
\mbox{Col}(A) = \{ \mathsf{b} \in \mathbb{R}^m : \mathsf{b} = A \mathsf{x} \mbox{ for some } \mathsf{x} \in \mathbb{R}^n \}.
\]</span>
<em>Connection to Linear Transformations</em>: If <span class="math inline">\(T(\mathsf{x}) = A \mathsf{x}\)</span>, then the range (also called the image) of <span class="math inline">\(T\)</span> is the column space of matrix <span class="math inline">\(A\)</span>.</p>
</dd>
<dt>rank</dt>
<dd><p>The <strong>rank</strong> of the <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> is the dimension of the column space of <span class="math inline">\(A\)</span>. This is also the number of pivot columns of the matrix.</p>
</dd>
<dt>eigenvalue and eigenvector</dt>
<dd><p>For a square <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span>, the scalar <span class="math inline">\(\lambda \in \mathbb{R}\)</span> is an <strong>eigenvalue</strong> for <span class="math inline">\(A\)</span> when there exists a nonzero vector <span class="math inline">\(\mathsf{x} \in \mathbb{R}^n\)</span> such that <span class="math inline">\(A \mathsf{x} = \lambda \mathsf{x}\)</span>. The nonzero vector <span class="math inline">\(\mathsf{x}\)</span> is the <strong>eigenvector</strong> for eigenvalue <span class="math inline">\(\lambda\)</span>. The collection of all of these eigenvalues and eigenvectors is called the <strong>eigensystem</strong> of A.</p>
</dd>
<dt>diagonalization</dt>
<dd><p>A square <span class="math inline">\(n \times n\)</span> matrix is <strong>diagonalizable</strong> when <span class="math inline">\(A = P D P^{-1}\)</span> where <span class="math inline">\(D\)</span> is a diagonal matrix and <span class="math inline">\(P\)</span> is an invertible matrix. In this case, the eigenvalues of <span class="math inline">\(A\)</span> are the diagonal entries of <span class="math inline">\(D\)</span> and their corresponding eigenvectors are the columns of <span class="math inline">\(P\)</span>.</p>
</dd>
<dt>dominant eigenvalue</dt>
<dd><p>The eigenvalue <span class="math inline">\(\lambda\)</span> of the square matrix <span class="math inline">\(A\)</span> is the <strong>dominant eigenvalue</strong> when <span class="math inline">\(| \lambda | &gt; | \mu |\)</span> where <span class="math inline">\(\mu\)</span> is any other eigenvalue of <span class="math inline">\(A\)</span>. The dominant eigenvalue determines the long-term behavior of <span class="math inline">\(A^t\)</span> as <span class="math inline">\(t \rightarrow \infty\)</span>.</p>
</dd>
</dl>
</div>
<div id="orthogonality" class="section level2">
<h2><span class="header-section-number">2.3</span> Orthogonality</h2>
<dl>
<dt>length</dt>
<dd><p>The <strong>length</strong> of a vector <span class="math inline">\(\mathsf{v}\)</span> is
<span class="math display">\[
\| \mathsf{v} \| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
\]</span></p>
</dd>
<dt>distance and angle</dt>
<dd><p>The <strong>distance</strong> between vectors <span class="math inline">\(\mathsf{u}\)</span> and <span class="math inline">\(\mathsf{v}\)</span> is
<span class="math display">\[
\mbox{dist}(\mathsf{u},\mathsf{v}) = \| \mathsf{u} - \mathsf{v} \|.
\]</span>
The <strong>angle</strong> <span class="math inline">\(\theta\)</span> between these vectors is determined by
<span class="math display">\[
\cos \theta = \frac{\mathsf{u} \cdot \mathsf{v}}{ \| \mathsf{u} \| \, \| \mathsf{v} \|}.
\]</span></p>
</dd>
<dt>orthogonal</dt>
<dd><p>The vectors <span class="math inline">\(\mathsf{u}\)</span> and <span class="math inline">\(\mathsf{v}\)</span> are <strong>orthogonal</strong> when <span class="math inline">\(\mathsf{u} \cdot \mathsf{v} = 0\)</span>. This means that either one of them is the zero vector, or they are perpendicular to one another.</p>
</dd>
<dt>orthogonal complement</dt>
<dd><p>If <span class="math inline">\(W \subset \mathbb{R}^n\)</span> is a subspace, then its <strong>orthogonal complement</strong> <span class="math inline">\(W^{\perp}\)</span> is the set of all vectors in <span class="math inline">\(\mathsf{R}^n\)</span> that are orthogonal to <span class="math inline">\(W\)</span>. We also write
<span class="math display">\[
W^{\perp} = \{ \mathsf{v} \in \mathbb{R}^n : \mathsf{v} \cdot \mathsf{w} \mbox{ for all } \mathsf{w} \in W \}.
\]</span></p>
</dd>
<dt>orthonormal set</dt>
<dd><p>A collection of vectors <span class="math inline">\(\mathsf{u}_1, \mathsf{u}_2, \ldots, \mathsf{u}_k\)</span> are an <strong>orthonormal set</strong> when every vector has length 1 and the vectors are pairwise orthogonal.
orthogonal matrix</p>
</dd>
<dt>orthogonal matrix</dt>
<dd><p>A square <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(P\)</span> is an <strong>orthogonal matrix</strong> when its columns are an orthonormal set. As a result, we have <span class="math inline">\(P^{-1} = P^{\top}\)</span>.</p>
</dd>
<dt>projection and residual</dt>
<dd><p>The <strong>orthogonal projection</strong> of vector <span class="math inline">\(\mathsf{y}\)</span> into a subspace <span class="math inline">\(W\)</span> is the unique vector <span class="math inline">\(\hat{\mathsf{y}} \in W\)</span> such that <span class="math inline">\(\mathsf{z} = \mathsf{y} - \hat{\mathsf{y}} \in W^{\perp}\)</span>. The vector <span class="math inline">\(\mathsf{z}\)</span> is called the <strong>residual vector</strong> for the projection.</p>
</dd>
</dl>
</div>
<div id="spectral-decompostion" class="section level2">
<h2><span class="header-section-number">2.4</span> Spectral Decompostion</h2>
<dl>
<dt>orthogonal diagonalization</dt>
<dd><p>Every symmetric <span class="math inline">\(n \times n\)</span> matrix is <strong>orthogonally diagonalizable</strong>, meaning that we have <span class="math inline">\(A = P D P^{\top}\)</span> where <span class="math inline">\(D\)</span> is a diagonal matrix and <span class="math inline">\(P\)</span> is an orthogonal matrix. The diagonal entries of <span class="math inline">\(D\)</span> are the eigenvalues of <span class="math inline">\(A\)</span> and the columns of <span class="math inline">\(P\)</span> are the corresponding orthonormal eigenvectors. Furthermore, the eigenvalues of <span class="math inline">\(A\)</span> are nonnegative.</p>
</dd>
<dt>spectral decomposition</dt>
<dd><p>A symmetric matrix <span class="math inline">\(A\)</span> can be written as a linear combination of rank 1 matrices derived from the orthonormal eigensystem of <span class="math inline">\(A\)</span>. In particular, we have
<span class="math display">\[
A = \lambda_1 \mathsf{u}_1 \mathsf{u}_1^{\top} + \lambda_2 \mathsf{u}_2 \mathsf{u}_2^{\top} + \cdots + \lambda_n \mathsf{u}_n \mathsf{u}_n^{\top}. 
\]</span>
This linear combination of rank 1 vectors is called the <strong>spectral decomposition</strong> of <span class="math inline">\(A\)</span>.</p>
</dd>
<dt>singular value decomposition (SVD)</dt>
<dd><p>Any <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> of rank <span class="math inline">\(r\)</span> can be factored into its <strong>singular value decomposition</strong> <span class="math inline">\(U \Sigma V^{\top}\)</span> where</p>
</dd>
</dl>
<ul>
<li><span class="math inline">\(U\)</span> is an <span class="math inline">\(m \times m\)</span> orthogonal matrix,</li>
<li><span class="math inline">\(\Sigma\)</span> is a matrix whose nonzero entries are the positive numbers <span class="math inline">\(\sigma_1, \ldots , \sigma_r\)</span>, which appear in decreasing order on the diagonal, and</li>
<li><span class="math inline">\(V\)</span> is an <span class="math inline">\(n \times n\)</span> orthogonal matrix.
The nonzero entries of <span class="math inline">\(\Sigma\)</span> are called the singular values of <span class="math inline">\(A\)</span>. The columns of <span class="math inline">\(U\)</span> are the left singular vectors and the rows of <span class="math inline">\(V^{\top}\)</span> are the right singular vectors.</li>
</ul>
<dl>
<dt>SVD spectral decomposition</dt>
<dd><p>Any <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> of rank <span class="math inline">\(r\)</span> can be written as a linear combination of rank 1 matrices derived from the singular value decomposition of <span class="math inline">\(A\)</span>. In particular, we have
<span class="math display">\[
A = \sigma_1 \mathsf{u}_1 \mathsf{v}_1^{\top} + \sigma_2 \mathsf{u}_2 \mathsf{v}_2^{\top} + \cdots + \sigma_r \mathsf{u}_r \mathsf{v}_r^{\top}. 
\]</span>
This linear combination of rank 1 vectors is called the <strong>(SVD) spectral decomposition</strong> of <span class="math inline">\(A\)</span>.</p>
</dd>
</dl>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="problem-set-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-1-learning-goals.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
