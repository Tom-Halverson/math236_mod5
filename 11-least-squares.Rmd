# Least Squares Approximation

[Download this Rmd file](https://github.com/Tom-Halverson/math236_s21/blob/main/11-least-squares.Rmd)

```{r,warning=FALSE}
require(pracma)
```

## Introduction

Let's start with a summary of Least Squares Approximation.

**The Why**: Given a matrix $A$ and a vector $\mathsf{b}$ that is not in $W = \mathrm{Col}(A)$, we want to find the "best approximate solution" $\hat{\mathsf{b}} \in W$. In other words, we want to pick the best possible $\hat{\mathsf{b}} \approx \mathsf{b}$ that lies in the column space of $A$.

**The What**: The answer is to use projections.

* This "best approximation" is the projection $\hat{\mathsf{b}} = \mbox{proj}_W \mathsf{b}$. 
* The residual vector vector $\mathsf{r} = \mathsf{b} - \hat{\mathbf{b}}$  is in $W^{\perp}$.
* The length $\| \mathsf{r} \|$ of the residual vector measures the closeness the approximation.
* The approximate solution to our original problem is the vector $\hat{\mathsf{x}}$ such that 
$A \hat{\mathsf{x}} = \hat{\mathsf{b}}$.

![](images/least-squares-pic.png){width=70%}

**The How**: A clever way to solve this is to use the *normal equations*. The best choice for $\hat{\mathsf{x}}$ satisfies
$$
A^{\top} A \hat{\mathsf{x}} = A^{\top} \mathsf{b}. 
$$
\newcommand{\A}{{\mathsf{A}}}
\newcommand{\b}{{\mathsf{b}}}

## Example 1

Here is an example that we did by hand in class. We now see how to do it in R. 

Find the least-squares solution to $\A x = \b$ if
\begin{equation}
\A = 
\begin{bmatrix}
1 & 1 \\
1 & 2 \\
0 & -1 \\
\end{bmatrix}
\quad \mbox{and} \quad
\b = 
\begin{bmatrix} 1 \\ 1 \\ 3 \end{bmatrix}.
(\#eq:inconsistent)
\end{equation}
First, for good measure, let's see if the system is inconsistent

```{r,echo=TRUE}
A = cbind(c(1,1,0),c(1,2,-1))
b = c(1,1,3)
Ab = cbind(A,b)
Ab
rref(Ab)
```

True indeed: $\b \not \in Col(\A)$.

Now we compute the normal equations to see what they look like:
```{r,echo=TRUE}
t(A) %*% A
t(A) %*% b
```
So we are going to instead solve the following **normal equations** instead of 
\@ref(eq:inconsistent):
\begin{equation}
\begin{bmatrix}
2 & 3 \\
3 & 6 \\
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
= 
\begin{bmatrix} 2 \\ 0 \end{bmatrix}.
(\#eq:normal)
\end{equation}

We can do this in the following nice, single R command
```{r,echo=TRUE}
(xhat = solve(t(A) %*% A, t(A) %*% b))
```
To compute $\hat \b$ we use
```{r,echo=TRUE}
(bhat = A %*% xhat)
```
And to get the residual, we use
```{r,echo=TRUE}
(r = b - bhat)
sqrt(t(r) %*% r)
```
We can also check that the residual is orthogonal to $Col(\A)$:
```{r,echo=TRUE}
t(A) %*% r
```


## Template

The following R code does it all. You can use this as a **template** for future problems. Just enter the matrix A and the vector b.

```{r, echo=TRUE,collapse=TRUE}
# Given: the matrix A
A = cbind(c(1,1,0),c(1,2,-1))
# Given: the target vector b
b = c(1,1,3)

#solve the normal equation
(xhat = solve(t(A) %*% A, t(A) %*% b))

# find the projection
(bhat = A %*% xhat)

# find the residual vector
(r = b - bhat)

# check that z is orthogonal to Col(A)
t(A) %*% r

# measure the distance between bhat and b
sqrt( t(r) %*% r)
```


## Fitting for a Linear Function

Here are some points that we'd like to fit to a linear function $y = a_0 + a_1 x$. 

**Note:** Here we use `y` instead of `b` because we like to write linear equations as "$y = cx + d$." So the expression "$b = a_0 + a_1 x$" looks funny to us. So we will talk about `y` and `yhat` instead of `b` and `bhat`.

```{r quadplot1,fig.width=4.5, fig.height=4.5, echo=TRUE}
x = c(1,2,3,4,5,6)
y = c(7,2,1,3,7,7)
plot(x,y,pch=19,ylim=c(0,10))
grid()
```

The linear equations that we want to fit are as follows. 
$$
\begin{bmatrix} 
1 & 1 \\  1 & 2  \\ 1 & 3  \\ 1 & 4  \\ 1 & 5  \\ 1 & 6  \\
\end{bmatrix}
\begin{bmatrix} a_0 \\ a_1  \end{bmatrix}
= 
\begin{bmatrix} 7 \\ 2 \\ 1 \\ 3 \\ 7 \\ 7 \end{bmatrix}.
$$

These equations are inconsistent, so we solve the normal equations $A^T A x = A^T y$ and find an approximate solution instead. **Pro Tip:** a clever way to create the desired matrix $A$ is to use the fact that $x^0=1$ for any number $x$.

```{r, echo=TRUE}
(A = cbind(x^0,x))
```
Let's take a look at the normal equations:
```{r, echo=TRUE}
t(A) %*% A
t(A) %*% y
```
So the normal equations to solve are below. It's surprising how, even though there are 6 variables, we only have to solve a 2x2 equation, since there are 2 unknowns.
$$
\begin{bmatrix} 
6 & 21 \\  21 & 91  \\
\end{bmatrix}
\begin{bmatrix} a_0 \\ a_1  \end{bmatrix}
= 
\begin{bmatrix} 27 \\ 103 \end{bmatrix}.
$$
```{r,echo=TRUE}
(xhat = solve(t(A) %*% A, t(A) %*% y))
```
This tells us that the desired intercept is $a_0 = 2.8$, the desired slope is $a_1 = 0.4856$, and the linear model is $y = 2.8 + 0.4856x$.

We can plot the points together with the solution using:
```{r quadplot2, fig.width=4.5, fig.height=4.5, echo=TRUE}

#plot the original set of points
plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main='the best-fit linear function')

# generate points for the fitted line and plot it
tt = seq(1,6,len=100)  
lines(tt,xhat[1]+xhat[2]*tt,col='blue')

# get yhat
yhat = A %*% xhat

# add the residuals to the plot
for (i in 1:length(x)) {
  lines(c(x[i],x[i]),c(y[i],yhat[i]), col='red')
}

#add yhat to the plot
points(x,yhat,pch=19,col='orange')

#put the original points back on the plot last so we can see them 
points(x,y,pch=19,col="black")
grid()
```

In this visualization we see the following:

* The black points: the original data points  `cbind(x,y)`. This represents the entries of the desired target vector `y`. 

* The blue curve: the fitted curve, created from the approximate solution `xhat`.  
* The orange points: the approximations `cbind(x,yhat)` of the data points `cbind(x,y)`. This represents entries of the projection `yhat`.

* The red line segments: the distances between the original data points (block dots) and their approximations (orange dots). The lengths of these red segments are the entries of the residual vector `r`.

Let's look at the residual and see that it is indeed orthogonal to the columns of $A$.

```{r,echo=TRUE}
yhat = A %*% xhat
r = y - yhat
res=cbind(y,yhat,r)
colnames(res) = c("y","yhat","r")
res
t(A) %*% r
sqrt(t(r) %*% r)
```


## Fitting for a Quadratic Function

The data we have been working with has a quadratic look to it, so let's try adding an $x^2$ term. That is, we will fit the model $y = a_0 + a_1 x + a_2 x^2$. The equations we want to solve are
In this case, the linear model that we'd like to solve is:
$$
\begin{bmatrix} 
1 & 1 & 1\\  1 & 2 & 4 \\ 1 & 3 & 9 \\ 1 & 4 & 16 \\ 1 & 5 & 25 \\ 1 & 6 & 36 \\
\end{bmatrix}
\begin{bmatrix} a_0 \\ a_1 \\ a_2 \end{bmatrix}
= 
\begin{bmatrix} 7 \\ 2 \\ 1 \\ 3 \\ 7 \\ 7 \end{bmatrix}.
$$


It is easy enough to add this to our matrix $A$.

```{r, echo=TRUE}
(A = cbind(x^0,x,x^2))
```
In this case our normal equations are 3x3

```{r, echo=TRUE}
t(A) %*% A
t(A) %*% y
```

$$
\begin{bmatrix} 
6 & 21 & 91 \\  21 & 91 & 441 \\ 91 & 441 & 2275 
\end{bmatrix}
\begin{bmatrix} a_0 \\ a_1 \\ a_2 \end{bmatrix}
= 
\begin{bmatrix} 27 \\ 103 \\ 499 \end{bmatrix}.
$$
Whose solution is computed by
```{r,echo=TRUE}
(xhat = solve(t(A) %*% A, t(A) %*% y))
```
Notice that our solution is now $y = 10.3 - 5.1393 x + 0.8036 x^2$. The linear term is now negative, but there is a positive quadratic term. Let's look at the same plo but with the addex $x^2$ term. We see that the residuals are smaller **and**, importantly, the model appears to better fit the data.

```{r quadplot3, fig.width=4.5, fig.height=4.5, echo=TRUE}

#plot the original set of points
plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main='the best-fit quadratic function')

# generate points for the fitted line and plot it
tt = seq(0,7,len=100)  
lines(tt,xhat[1]+xhat[2]*tt+xhat[3]*tt^2,col='blue')

# get yhat
yhat = A %*% xhat

# add the residuals to the plot
for (i in 1:length(x)) {
  lines(c(x[i],x[i]),c(y[i],yhat[i]), col='red')
}

#add yhat to the plot
points(x,yhat,pch=19,col='orange')

#put the original points back on the plot last so we can see them 
points(x,y,pch=19,col="black")
grid()
```

Let's look again at the residual and see that it is indeed orthogonal to the columns of $A$ and see that the residual got shorter.

```{r,echo=TRUE}
yhat = A %*% xhat
r = y - yhat
res=cbind(y,yhat,r)
colnames(res) = c("y","yhat","r")
res
t(A) %*% r
sqrt(t(r) %*% r)
```

